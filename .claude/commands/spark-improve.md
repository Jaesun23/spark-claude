---
name: spark-improve
description: Code quality enhancement and performance optimization with refactoring expertise
type: command
requires: improver-spark
---

# /spark-improve - Intelligent Code Improvement Command

**Purpose**: Improvement is the art of making good code great through "ë”°ë°•ë”°ë°• ê¾¸ì—­ê¾¸ì—­" persistence - methodically fixing every quality violation until ZERO remain, never using dangerous automated scripts that destroy memory.

## Philosophy (Natural Language Inspiration)

Code improvement requires both vision and restraint - seeing potential while respecting current functionality. We approach enhancement with:

- **Evidence-based changes**: Every improvement backed by metrics
- **Incremental evolution**: Small steps toward large improvements
- **Quality without disruption**: Better code that still works perfectly
- **Performance with purpose**: Optimize what matters most

The best improvements feel inevitable in retrospect - obvious solutions that somehow weren't obvious before.

## Behavior Protocol (Code-Based Execution)

```python
class SparkImproveCommand:
    """Intelligent improvement with systematic quality enhancement.
    
    This protocol ensures measurable improvements while the philosophy above
    guides what to change versus what to preserve. Progress with stability.
    """
    
    # Improvement dimensions - HOLISTIC ENHANCEMENT
    IMPROVEMENT_AREAS = {
        "performance": ["algorithmic", "memory", "io", "caching"],
        "maintainability": ["readability", "modularity", "documentation"],
        "reliability": ["error_handling", "edge_cases", "testing"],
        "security": ["vulnerabilities", "best_practices", "hardening"]
    }
    
    # Quality metrics - MEASURABLE PROGRESS
    IMPROVEMENT_METRICS = {
        "performance_gain": "> 20%",
        "complexity_reduction": "> 15%", 
        "test_coverage_increase": "> 10%",
        "maintainability_score": "> 0.8"
    }
    
    def improve_codebase(self, focus_area: str) -> dict:
        """Main improvement orchestration with metrics validation."""
        baseline = self.establish_baseline_metrics()
        
        improvement_plan = self.analyze_improvement_opportunities(
            focus_area, baseline
        )
        
        # Apply improvements incrementally with validation
        results = []
        for improvement in improvement_plan["prioritized_changes"]:
            result = self.apply_improvement_safely(improvement)
            
            # Measure impact
            new_metrics = self.measure_improvement_impact(baseline)
            if not self.meets_improvement_thresholds(new_metrics):
                self.revert_change(improvement)
                continue
                
            results.append(result)
            baseline = new_metrics  # New baseline for next improvement
        
        return self.generate_improvement_report(baseline, results)
    
    def balance_optimization_with_readability(self, context: dict) -> str:
        """Balance performance gains with code clarity.
        
        Embodies 'ë¯¸ë¬˜í•œ ì¡°ì ˆì´ë‚˜ ê· í˜•ì˜ ë¬˜' - sometimes slower,
        clearer code is better than fast, cryptic code.
        """
        if context["team_experience"] == "junior":
            return "readability_first"
        elif context["performance_critical"] == True:
            return "optimization_focused"
        else:
            return "balanced_improvement"
```

## ğŸ“ 2í˜¸(Claude Code) MUST FOLLOW THIS EXACT PROTOCOL

### **WHEN RECEIVING /spark-improve COMMAND:**

```python
1. INITIAL ASSESSMENT:
   # Check if multi-session state exists
   state_file = ".claude/workflows/improver_state.yaml"
   
   if exists(state_file):
      state = load_yaml(state_file)
      print(f"ğŸ“‚ ì´ì „ ê°œì„  ì‘ì—… ë°œê²¬")
      print(f"   Progress: {state['progress']['issues_fixed']}/{state['progress']['total_issues']} fixed")
      print(f"ğŸ¯ ë‹¤ìŒ í¬ì»¤ìŠ¤: {state['next_session']['focus']}")
      
      # Provide context to agent
      Task("improver-spark", f"""
         {user_request}
         
         PREVIOUS STATE EXISTS:
         - Sessions completed: {state['sessions_completed']}
         - Issues fixed: {state['progress']['issues_fixed']}
         - Improvement type: {state['improvement_type']}
         Continue from saved state.
      """)
   else:
      # New improvement
      Task("improver-spark", user_request)

2. WAIT for agent completion

3. CHECK PROJECT/.claude/workflows/current_task.json:
   REQUIRED CONDITIONS:
   - quality.violations_total == 0
   - quality.can_proceed == true  
   - state.status == "completed"

4. CHECK FOR MULTI-SESSION:
   if exists(state_file):
      state = load_yaml(state_file)
      
      if not state.get('improvement_complete', False):
         total_issues = state['progress']['total_issues']
         fixed_issues = state['progress']['issues_fixed']
         remaining = total_issues - fixed_issues
         
         print(f"""
         ğŸ“Š ê°œì„  ì§„í–‰ ìƒí™©:
         - ì´ ì´ìŠˆ: {total_issues}ê°œ
         - ìˆ˜ì • ì™„ë£Œ: {fixed_issues}ê°œ  
         - ë‚¨ì€ ì‘ì—…: {remaining}ê°œ
         ğŸ¯ ë‹¤ìŒ ì„¸ì…˜: {state['next_session']['focus']}
         
         âš ï¸ ìŠ¤í¬ë¦½íŠ¸ ìë™ ìˆ˜ì • ì ˆëŒ€ ê¸ˆì§€ - Memory V3/V5 íŒŒê´´ êµí›ˆ
         
         ê³„ì†í•˜ë ¤ë©´: /spark-improve --continue
         
         ë˜ëŠ” ìë™ìœ¼ë¡œ ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n)
         """)
         
         if user_confirms or "--auto" in request:
            # Continue automatically
            goto step 1 with "--continue" flag
         else:
            # Wait for user to resume
            return
      else:
         print("âœ… ê°œì„  ì‘ì—… ì™„ë£Œ! ëª¨ë“  í’ˆì§ˆ ìœ„ë°˜ í•´ê²°ë¨")

5. FINAL DECISION:
   âœ… ALL CONDITIONS MET â†’ Report complete improvement results
   âŒ ANY CONDITION FAILED â†’ Task("improver-spark", """
      Previous improvement incomplete or failed quality checks.
      Please complete the improvements and fix issues: {violations}
      CRITICAL: No automated scripts - fix each issue individually
      """)
```

### **Multi-Session Orchestration Protocol:**

When improver-spark creates a state file, 2í˜¸ must:

1. **Recognize Multi-Session Need**: Large improvement scope requires progressive work
2. **Monitor Progress**: Track issues fixed vs. remaining
3. **Intelligent Continuation**: 
   - Show progress and next focus area
   - Warn about script prohibition
   - Allow user to review intermediate results
4. **Safety Management**:
   ```python
   # Ensure no automated script usage
   if "sed" in agent_response or "awk" in agent_response:
      print("ğŸš« CRITICAL: Automated script detected - aborting")
      print("   Reason: Memory V3/V5 were destroyed by auto-scripts")
      
   # Maximum 10 sessions for any improvement
   if state['sessions_completed'] >= 10:
      print("âš ï¸ ê°œì„ ì´ 10ì„¸ì…˜ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë²”ìœ„ë¥¼ ì¬ì¡°ì •í•˜ì„¸ìš”.")
   
   # Progress validation
   if state['progress']['issues_fixed'] == 0 and sessions > 2:
      print("âš ï¸ 2ì„¸ì…˜ í›„ì—ë„ ì´ìŠˆê°€ ìˆ˜ì •ë˜ì§€ ì•ŠìŒ. ì ‘ê·¼ ë°©ì‹ì„ ì¬ê²€í† í•˜ì„¸ìš”.")
   ```

The improver-spark specialist will:
- **Methodically fix EVERY quality violation** (ë”°ë°•ë”°ë°• ê¾¸ì—­ê¾¸ì—­)
- **Use only manual fixes** - NO scripts, regex, or batch operations
- **Save progress state between sessions** for large-scale improvements
- **Resume intelligently** from previous state
- **Handle stubborn issues** with advanced strategies
- **Roll back problematic changes** immediately
- **Verify each fix** doesn't break functionality
