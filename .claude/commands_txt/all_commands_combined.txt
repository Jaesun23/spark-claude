================================================================================
SPARK v3.5 - ëª¨ë“  ëª…ë ¹ì–´ ì •ì˜ í†µí•© íŒŒì¼
================================================================================

ëª©ì°¨:
----------------------------------------
â€¢ multi-implement
â€¢ spark-analyze
â€¢ spark-audit
â€¢ spark-clean
â€¢ spark-design
â€¢ spark-fix
â€¢ spark-implement
â€¢ spark-launch
â€¢ spark-migrate
â€¢ spark-optimize
â€¢ spark-refactor
â€¢ spark-test

================================================================================


================================================================================
ëª…ë ¹ì–´: multi-implement
íŒŒì¼: multi-implement.md
================================================================================

# /multi-implement - SPARK Parallel Multi-Team Implementation

**Purpose**: Execute multiple implementation tasks in parallel using up to 4 teams with JSON context relay

## ğŸš¨ CRITICAL: ONE MESSAGE WITH MULTIPLE TOOL CALLS ğŸš¨

### **THE GOLDEN RULE FOR PARALLEL EXECUTION:**
```
YOU MUST USE A SINGLE MESSAGE WITH MULTIPLE TASK TOOL CALLS!
DO NOT SEND SEPARATE MESSAGES FOR EACH TASK!
```

## âš¡ IMMEDIATE EXECUTION PROTOCOL

### **AS SOON AS YOU RECEIVE /multi-implement:**

```python
# âœ… CORRECT - ALL IN ONE MESSAGE:
[Single Message]
â”œâ”€â”€ Task("implementer-spark", "Team1: {task1} Read team1_current_task.json")
â”œâ”€â”€ Task("implementer-spark", "Team2: {task2} Read team2_current_task.json")  
â”œâ”€â”€ Task("implementer-spark", "Team3: {task3} Read team3_current_task.json")
â””â”€â”€ Task("implementer-spark", "Team4: {task4} Read team4_current_task.json")
[Wait for all to complete]

# âŒ WRONG - SEQUENTIAL MESSAGES:
Message 1: Task("implementer-spark", "Team1...")
Message 2: Task("implementer-spark", "Team2...")  # NO! This waits for Team1!
Message 3: Task("implementer-spark", "Team3...")  # NO! This waits for Team2!
Message 4: Task("implementer-spark", "Team4...")  # NO! This waits for Team3!
```

### **EXECUTION STEPS:**
1. **PREPARE**: Create all team JSON files (team1_current_task.json, etc.)
2. **LAUNCH ALL AT ONCE**: Single message with 4 Task tool calls
3. **WAIT**: For all 4 teams to complete
4. **TEST ALL AT ONCE**: Single message with 4 tester Task calls
5. **REPORT**: Consolidate results

âš ï¸ **REMEMBER**: If you send Task calls in separate messages, they run SEQUENTIALLY, not in PARALLEL!

## ğŸ“ Orchestration Process

### Phase 0: Task Allocation
Analyze tasks and allocate to teams:
1. Parse task IDs from command
2. Team JSON files will be auto-generated by StateManager:
   ```python
   # StateManager automatically creates team#_current_task.json files
   # when write_team_state(team_id, task_data) is called
   # Uses hardcoded _default_state() structure with team_id added
   ```
3. Identify shared resources needing locks

### Phase 1: Parallel Implementation
Call all assigned teams SIMULTANEOUSLY (not sequentially):
```
# ALL FOUR CALLS AT ONCE - NO WAITING BETWEEN!
Task("team1-implementer-spark", "{task1}")
Task("team2-implementer-spark", "{task2}")
Task("team3-implementer-spark", "{task3}")
Task("team4-implementer-spark", "{task4}")
```

Each team implementer:
- Automatically reads their team#_current_task.json file
- Implements only their assigned feature
- Updates their JSON file with 'implementation' section
- Runs self-validation before exit (recommended)
- Respects file locks for shared resources

### Phase 1.5: Claude CODE Implementation Review
After all implementations complete, Claude CODE reviews all team results:

```python
# Claude CODE reviews each team's results
1. Read team1_current_task.json (check 'implementation' section)
2. Read team2_current_task.json (check 'implementation' section)  
3. Read team3_current_task.json (check 'implementation' section)
4. Read team4_current_task.json (check 'implementation' section)

# Decision for each team:
âœ… If team results satisfactory â†’ Proceed to Phase 2
âŒ If issues found â†’ Re-call that team's implementer
```

### Phase 2: Parallel Testing
After Claude CODE approves all implementations, call testers:
```
Task("team1-tester-spark", "Test Team 1 implementation")
Task("team2-tester-spark", "Test Team 2 implementation")
Task("team3-tester-spark", "Test Team 3 implementation")
Task("team4-tester-spark", "Test Team 4 implementation")
```

Each team tester:
- Automatically reads their team#_current_task.json
- Creates comprehensive tests (95% coverage target)
- Updates their JSON file with 'testing' section
- Runs self-validation before exit (recommended)

### Phase 2.5: Claude CODE Testing Review
After all testing complete, Claude CODE reviews all test results:

```python
# Claude CODE reviews each team's test results
1. Read team1_current_task.json (check 'testing' section)
2. Read team2_current_task.json (check 'testing' section)
3. Read team3_current_task.json (check 'testing' section)  
4. Read team4_current_task.json (check 'testing' section)

# Decision for each team:
âœ… If test coverage â‰¥95% and all tests pass â†’ Proceed to Phase 3
âŒ If issues found â†’ Re-call that team's tester
```

### Phase 3: Parallel Documentation
After Claude CODE approves all testing, call documenters:
```
Task("team1-documenter-spark", "Document Team 1 work")
Task("team2-documenter-spark", "Document Team 2 work")
Task("team3-documenter-spark", "Document Team 3 work")
Task("team4-documenter-spark", "Document Team 4 work")
```

Each team documenter:
- Automatically reads their team#_current_task.json
- Documents implementation and test results
- Updates their JSON file with 'documentation' section
- Runs self-validation before exit (recommended)

### Phase 3.5: Claude CODE Documentation Review
After all documentation complete, Claude CODE reviews all documentation results:

```python
# Claude CODE reviews each team's documentation
1. Read team1_current_task.json (check 'documentation' section)
2. Read team2_current_task.json (check 'documentation' section)
3. Read team3_current_task.json (check 'documentation' section)
4. Read team4_current_task.json (check 'documentation' section)

# Decision for each team:
âœ… If documentation complete â†’ Mark team as finished
âŒ If issues found â†’ Re-call that team's documenter
```

### Phase 4: Final Consolidation
After all teams pass all phases, Claude CODE provides final report:

```python
# Final multi-team implementation report
- Summary of all team implementations
- Consolidated quality metrics
- Integration points and dependencies
- Overall project completion status
```

## ğŸ’¡ Quality Criteria

Same as single implementation:
- MyPy strict: 0 errors
- Ruff: 0 violations
- Test coverage: â‰¥95%
- Security scan: 0 issues
- Documentation: Complete

## ğŸ”§ JSON Context Structure

Each team's JSON file (team#_current_task.json) contains:
```json
{
  "team_id": "team1",
  "task_id": "TASK-API-01",
  "status": "implementing|testing|documenting|completed",
  "task_details": {
    "description": "Implement user authentication endpoint",
    "files_to_modify": ["api/auth.py"],
    "requirements": ["JWT support", "Refresh tokens"]
  },
  "implementation": {
    "agent": "team1-implementer-spark",
    "timestamp": "ISO-8601",
    "status": "completed",
    "files_created": ["api/auth.py"],
    "files_modified": ["main.py"],
    "quality_metrics": {
      "linting_passed": true,
      "type_checking_passed": true
    }
  },
  "testing": {
    "agent": "team1-tester-spark",
    "timestamp": "ISO-8601",
    "status": "completed",
    "test_files": ["tests/test_auth.py"],
    "coverage": 96,
    "all_tests_pass": true
  },
  "documentation": {
    "agent": "team1-documenter-spark",
    "timestamp": "ISO-8601",
    "status": "completed",
    "docs_created": ["docs/auth_api.md"],
    "readme_updated": true
  },
  "locks_held": ["constants.py"],
  "self_validated": true
}
```

## ğŸ”’ File Lock Management

For shared resources (constants.py, types.py):
- Teams request locks through JSON
- Orchestrator manages lock allocation
- 30-second timeout prevents deadlocks

## ğŸš€ Usage Examples

```bash
# 2 tasks in parallel
/multi-implement "TASK-API-01" "TASK-UI-02"

# 4 tasks in parallel 
/multi-implement "TASK-API-01" "TASK-UI-02" "TASK-SEC-03" "TASK-DATA-04"

# With task descriptions
/multi-implement "Create user auth endpoint" "Build dashboard component" "Add security middleware" "Implement data pipeline"
```

## ğŸ“Š Performance Benefits

- **2 tasks**: ~1.8x faster than sequential
- **3 tasks**: ~2.5x faster than sequential
- **4 tasks**: ~3.1x faster than sequential

## âš ï¸ Limitations

- Maximum 4 teams (context window constraint)
- All teams wait for slowest to complete each phase
- Shared file modifications are serialized
- No direct agent-to-agent communication


================================================================================
ëª…ë ¹ì–´: spark-analyze
íŒŒì¼: spark-analyze.md
================================================================================

# /spark-analyze - SPARK Analysis Command

**Purpose**: Multi-dimensional code and system analysis with evidence-based investigation

## Execution Instructions

When this command is called, I will delegate the analysis to the analyzer-spark specialist:

The analyzer-spark specialist will:
- Conduct comprehensive multi-perspective analysis
- Examine code from performance, security, and quality angles
- Provide evidence-based insights with specific metrics
- Generate actionable recommendations for improvements

## Usage Examples

```bash
/spark-analyze "performance bottlenecks in the authentication system"
/spark-analyze "security vulnerabilities in API endpoints"
/spark-analyze "code complexity and maintainability metrics"
/spark-analyze "dependency relationships and potential issues"
/spark-analyze "memory usage patterns in the data processing pipeline"
```

## Analysis Capabilities

- **Performance Analysis**: Bottlenecks, memory usage, optimization opportunities
- **Security Analysis**: Vulnerabilities, attack vectors, compliance issues  
- **Code Quality**: Complexity metrics, maintainability scores, technical debt
- **Architecture**: Structure analysis, coupling patterns, design issues
- **Dependencies**: Vulnerability scanning, usage optimization, conflict detection

## SPARK Intelligence Integration

- ğŸ­ **Code Analyst Persona**: Activates analytical thinking patterns
- ğŸ“Š **Evidence-Based**: All findings backed by data and metrics
- ğŸ” **Multi-Perspective**: Examines code from multiple angles
- ğŸš€ **Optimized Token Usage**: Focused analysis without token waste


================================================================================
ëª…ë ¹ì–´: spark-audit
íŒŒì¼: spark-audit.md
================================================================================

# /spark-audit - SPARK Security & Performance Audit Pipeline

**Purpose**: Complete project audit covering security, performance, and quality with actionable reports

## Execution Instructions

When this command is called, execute the following comprehensive audit pipeline:


## Usage Examples

```bash
/spark-audit "complete security and performance audit of the API layer"
/spark-audit "audit user authentication and authorization systems"  
/spark-audit "comprehensive review of data processing pipeline"
/spark-audit "audit payment processing system for compliance"



================================================================================
ëª…ë ¹ì–´: spark-clean
íŒŒì¼: spark-clean.md
================================================================================

# /spark-clean - SPARK Project Cleanup Command  

**Purpose**: Comprehensive project cleanup and technical debt reduction with SPARK intelligence

## Execution Instructions

When this command is called, I will activate the cleaner-spark optimization specialist:

The cleaner-spark specialist will:
- Perform comprehensive analysis of project structure and code quality
- Remove technical debt, duplicates, and unused resources
- Optimize file organization and directory structure
- Fix all linting and formatting issues
- Ensure no functionality is broken during cleanup

## Usage Examples

```bash
/spark-clean "full project cleanup and optimization"
/spark-clean "remove unused dependencies and dead code"
/spark-clean "fix all linting violations and code quality issues"
/spark-clean "optimize directory structure and file organization"
/spark-clean "clean up documentation and fix broken links"
```

## Cleanup Capabilities

- **File Management**: Remove duplicates, temporary files, empty directories
- **Code Quality**: Fix linting issues, improve formatting, add missing type hints
- **Dependencies**: Remove unused packages, optimize requirements, resolve conflicts
- **Structure**: Organize directories, consolidate functionality, improve imports
- **Documentation**: Clean outdated docs, fix broken references, improve README

## SPARK Quality Assurance

All cleanup operations ensure:
- âœ… **No Functionality Broken**: All tests continue to pass
- âœ… **Quality Improvements**: Better linting and type checking scores
- âœ… **Structure Optimization**: Follows language/framework best practices  
- âœ… **Token Efficiency**: Optimized usage maintained through cleanup process


================================================================================
ëª…ë ¹ì–´: spark-design
íŒŒì¼: spark-design.md
================================================================================

# /spark-design - SPARK Design Command

**Purpose**: System design and UI/UX creation with architecture and accessibility expertise

## Execution Instructions

When this command is called, I will collaborate with the designer-spark specialist:

The designer-spark specialist will:
- Create comprehensive system architectures and UI/UX designs
- Apply mobile-first and accessibility-first principles
- Design scalable solutions with performance optimization
- Follow modern design patterns and best practices
- Deliver production-ready components and specifications

## Usage Examples

```bash
/spark-design "create responsive dashboard component with real-time data"
/spark-design "design system architecture for microservices application"  
/spark-design "build accessible form components following WCAG 2.1"
/spark-design "create mobile-first navigation component"
/spark-design "design database schema for e-commerce platform"
```

## Design Capabilities

- **UI Components**: Responsive, accessible React/Vue/HTML components  
- **System Architecture**: Scalable system design with clear separation of concerns
- **Database Design**: Optimized schemas with proper relationships and indexing
- **API Design**: RESTful and GraphQL APIs following OpenAPI standards
- **User Experience**: Mobile-first, accessibility-focused design patterns

## Design Standards

All SPARK designs ensure:
- âœ… **Responsive Design**: Mobile-first approach with all screen sizes supported
- âœ… **Accessibility**: WCAG 2.1 AA compliance with semantic HTML and ARIA
- âœ… **Performance**: Optimized loading, minimal bundle size, efficient rendering
- âœ… **Scalability**: Designs that work from prototype to enterprise scale
- âœ… **Consistency**: Follows established design systems and patterns

## SPARK Intelligence Integration

- ğŸ­ **Frontend/Architect Persona**: Activates design-focused thinking
- ğŸ¨ **Modern Patterns**: Uses current best practices and design systems
- â™¿ **Accessibility First**: Built-in WCAG compliance and inclusive design
- ğŸš€ **Optimized Token Usage**: Efficient design creation without bloat


================================================================================
ëª…ë ¹ì–´: spark-fix
íŒŒì¼: spark-fix.md
================================================================================

# /spark-fix - SPARK Troubleshooting Command

**Purpose**: Problem investigation, debugging, and root cause analysis with SPARK intelligence

## Execution Instructions

When this command is called, I will engage the troubleshooter-spark debugging specialist:

The troubleshooter-spark specialist will:
- Systematically investigate the reported issue
- Analyze error patterns, logs, and stack traces
- Apply scientific debugging methods to isolate root causes
- Develop targeted fixes with minimal side effects
- Verify solutions thoroughly to prevent regressions

## Usage Examples

```bash
/spark-fix "API endpoints returning 500 errors intermittently"
/spark-fix "memory leak in the data processing pipeline"
/spark-fix "tests failing after dependency update" 
/spark-fix "performance degradation in search functionality"
/spark-fix "authentication system not working in production"
```

## Troubleshooting Capabilities

- **Error Investigation**: Systematic analysis of logs, stack traces, and error patterns
- **Performance Debugging**: Profiling, bottleneck identification, optimization
- **Integration Issues**: API failures, database connectivity, service communication
- **Environment Problems**: Configuration, deployment, dependency conflicts
- **Code Logic Bugs**: Logic errors, edge cases, race conditions

## Debugging Process

SPARK troubleshooting follows systematic approach:
1. **Problem Reproduction**: Isolate and reproduce the issue consistently
2. **Root Cause Analysis**: Trace the issue to its underlying cause
3. **Solution Development**: Create targeted fix with minimal side effects
4. **Verification**: Test fix thoroughly and ensure no regressions
5. **Prevention**: Suggest improvements to prevent similar issues

## SPARK Intelligence Integration

- ğŸ­ **Debugger Persona**: Activates systematic problem-solving patterns
- ğŸ” **Evidence-Based**: All conclusions backed by logs, metrics, and testing
- ğŸ§ª **Hypothesis Testing**: Scientific approach to isolating issues
- ğŸš€ **Optimized Token Usage**: Focused debugging without information overload


================================================================================
ëª…ë ¹ì–´: spark-implement
íŒŒì¼: spark-implement.md
================================================================================

# /implement - SPARK Implementation Command

**Purpose**: Quality-driven implementation workflow with 2í˜¸'s intelligent orchestration and optimized token efficiency

## ğŸš€ Quality-Driven Multi-Agent Workflow

This command orchestrates a complete development pipeline with quality gates ensuring all deliverables meet SPARK standards before progressing.

### Workflow Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Implementer-Sparkâ”‚ â†â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
         â†“                â”‚ (Quality retry)
    Quality Gates â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (Passed)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tester-Spark    â”‚ â†â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
         â†“                â”‚ (Coverage retry)
    Test Validation â”€â”€â”€â”€â”€â”€â”˜
         â†“ (95%+ achieved)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Documenter-Spark â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    âœ… Completion Report
```

## ğŸ“ Claude CODE Action Protocol

### **UPON RECEIVING /implement COMMAND:**
```python
# Claude CODE's ORCHESTRATION PROTOCOL (systematic 3-phase execution)
1. Task("implementer-spark", user_request)  # CALL IMMEDIATELY
2. Wait for SubagentStop hook signal
3. Claude CODE reviews current_task.json:
   - Check `implementation` section
   - Review quality_metrics (linting, type checking)
   - Verify files_created and files_modified
   - Review next_steps and known_issues
4. DECISION:
   âœ… If satisfied â†’ Task("tester-spark", implementation_context)
   âŒ If issues found â†’ Task("implementer-spark", retry_with_feedback)

5. Wait for tester SubagentStop hook signal  
6. Claude CODE reviews current_task.json:
   - Check `testing` section
   - Verify test coverage (target: 95%+)
   - Confirm all tests passing
   - Review test quality metrics
7. DECISION:
   âœ… If satisfied â†’ Task("documenter-spark", context)
   âŒ If issues found â†’ Task("tester-spark", retry_with_feedback)

8. Wait for documenter SubagentStop hook signal
9. Claude CODE reviews current_task.json:
   - Check `documentation` section
   - Verify README updates
   - Confirm API documentation
   - Review usage examples
10. FINAL DECISION:
    âœ… All phases complete â†’ Report success to user
    âŒ Issues found â†’ Task("documenter-spark", retry_with_feedback)
```

âš¡ **Core Principle**: Claude CODE reviews JSON results at each phase and decides next agent invocation

## ğŸ“ Orchestration Process

### Phase 1: Implementation
Claude CODE will delegate to implementer-spark specialist:

1. **Task Assignment**: Request the implementer-spark specialist to implement the feature
2. **Quality Validation**: The SPARK quality gates hook automatically validates:
   - Syntax correctness (0 errors)
   - Type checking (MyPy 0 errors)
   - Linting compliance (Ruff 0 violations)
   - Security scanning (0 issues)
   - Documentation presence (docstrings required)
3. **Auto-Retry**: If quality gates fail, the specialist automatically retries (max 3 attempts)

**Quality Review Checklist:**
- âœ… Syntax validation (0 errors)
- âœ… Type checking (MyPy 0 errors)  
- âœ… Linting (Ruff 0 violations)
- âœ… Security scan (0 issues)
- âœ… Documentation (Docstrings required)

**Phase 1 â†’ Phase 2 Decision by Claude CODE:**
- Review `implementation` section in current_task.json
- Check quality_metrics in the JSON
- If satisfied â†’ Call tester-spark
- If issues found â†’ Call implementer-spark again with feedback

### Phase 2: Testing
After Claude CODE approves implementation, call tester-spark specialist:

1. **Test Development**: Request comprehensive test creation with 95%+ coverage target
2. **Test Validation**: The test runner hook automatically verifies:
   - All tests passing (0 failures)
   - Coverage â‰¥ 95% (target achievement)
   - Edge cases covered (boundary testing)
   - Integration tests exist (system testing)
3. **Coverage Retry**: If coverage is below 95%, the specialist enhances tests (max 2 attempts)

**Test Quality Review:**
- âœ… All tests passing (0 failures)
- âœ… Coverage â‰¥ 95% (target achieved)
- âœ… Edge cases covered (boundary testing)
- âœ… Integration tests exist (system testing)

**Phase 2 â†’ Phase 3 Decision by Claude CODE:**
- Review `testing` section in current_task.json
- Check test coverage (target: 95%+)
- If satisfied â†’ Call documenter-spark
- If issues found â†’ Call tester-spark again with feedback

### Phase 3: Documentation
After Claude CODE approves testing, call documenter-spark specialist:

1. **Documentation Creation**: Request comprehensive documentation including:
   - README updates with new features
   - API documentation for new endpoints
   - Usage examples and code samples
   - Inline docstrings for all functions
2. **Final Report**: Generate completion report with all deliverables

**Phase 3 Completion Criteria:**
- README.md updated
- API documentation complete (if applicable)
- Usage examples added
- All functions/classes have docstrings
- Final implementation report generated

## ğŸ’¡ Quality Standards

### Implementation Quality (Phase 1)
- **í•„ìˆ˜ í†µê³¼ í•­ëª©**: Syntax (0), MyPy (0), Ruff (0), Security (0), Docstrings (0)

### Testing Quality (Phase 2) 
- **í•„ìˆ˜ ë‹¬ì„±**: Coverage â‰¥95%, Test failures (0), Edge cases covered

### Documentation Quality (Phase 3)
- **í•„ìˆ˜ í¬í•¨**: README updates, API docs, Usage examples, Inline docstrings

## ğŸš€ Usage Examples

```bash
/implement "Create secure user authentication with JWT tokens"
/implement "Build responsive dashboard with real-time data"
/implement "Implement data validation pipeline with error handling"
```

## ğŸ“Š SPARK Efficiency

- **Token Usage**: Lazy-loading architecture (only load required agents)
- **Quality Assurance**: 8 quality gates + retry system (max 3 attempts)
- **JSON Communication**: Unified current_task.json for all phases


================================================================================
ëª…ë ¹ì–´: spark-launch
íŒŒì¼: spark-launch.md
================================================================================

# /spark-launch - SPARK Full-Stack Feature Launch Pipeline

**Purpose**: Complete feature development from design to deployment with quality assurance

## ğŸš€ 5-Phase Automatic Development Pipeline

âš¡ **í•µì‹¬ ì›ì¹™**: ìˆ˜ë™ í™•ì¸ ì ˆì°¨ ì—†ìŒ - ëª¨ë“  ë‹¨ê³„ëŠ” ì¡°ê±´ ì¶©ì¡± ì‹œ ìë™ ì§„í–‰

## ğŸš€ 5-Phase Development Pipeline

This command executes a comprehensive development workflow with multiple specialists:

### Phase 1: Design Architecture (ìë™ ì‹¤í–‰)
I will immediately engage designer-spark specialist to:
- Create system architecture and UI/UX designs
- Define technical requirements and specifications
- Establish design patterns and component structures

**Phase 1 â†’ Phase 2 ìë™ ì§„í–‰:**
- âœ… ì•„í‚¤í…ì²˜ ì„¤ê³„ ì™„ë£Œ â†’ ìë™ìœ¼ë¡œ Phase 2 ì‹œì‘
- âœ… UI/UX ë””ìì¸ ì™„ì„± â†’ ì¦‰ì‹œ êµ¬í˜„ ë‹¨ê³„ë¡œ ì „í™˜
- âœ… ê¸°ìˆ  ìŠ¤í™ ë¬¸ì„œ ì‘ì„± â†’ ëŒ€ê¸° ì—†ì´ ë‹¤ìŒ ë‹¨ê³„ ì‹¤í–‰

### Phase 2: Implementation (ìë™ ì‹¤í–‰)
The implementer-spark specialist will immediately:
- Implement the core functionality based on design
- Follow established patterns and architecture
- Ensure code quality through SPARK quality gates

**Phase 2 â†’ Phase 3 ìë™ ì§„í–‰:**
- âœ… ëª¨ë“  í’ˆì§ˆ ê²Œì´íŠ¸ í†µê³¼ (5/5) â†’ ìë™ìœ¼ë¡œ Phase 3 ì‹œì‘
- âœ… í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„ ì™„ë£Œ â†’ ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ë¡œ ì „í™˜
- âœ… Hook ê²€ì¦ í†µê³¼ â†’ ëŒ€ê¸° ì—†ì´ ë‹¤ìŒ ë‹¨ê³„ ì‹¤í–‰

### Phase 3: Comprehensive Testing (ìë™ ì‹¤í–‰)
The tester-spark specialist will immediately:
- Create unit, integration, and end-to-end tests
- Achieve 95%+ code coverage
- Validate all functionality works as designed

**Phase 3 â†’ Phase 4 ìë™ ì§„í–‰:**
- âœ… í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 95% ë‹¬ì„± â†’ ìë™ìœ¼ë¡œ Phase 4 ì‹œì‘
- âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ (0 failures) â†’ ì¦‰ì‹œ ë¬¸ì„œí™” ë‹¨ê³„ë¡œ ì „í™˜
- âœ… ê¸°ëŠ¥ ì™„ì „ì„± ê²€ì¦ ì™„ë£Œ â†’ ëŒ€ê¸° ì—†ì´ ë‹¤ìŒ ë‹¨ê³„ ì‹¤í–‰

### Phase 4: Documentation (ìë™ ì‹¤í–‰)
The documenter-spark specialist will immediately:
- Create comprehensive API documentation
- Write user guides and examples
- Update project README and architecture docs

**Phase 4 â†’ Phase 5 ìë™ ì§„í–‰:**
- âœ… API ë¬¸ì„œ ì‘ì„± ì™„ë£Œ â†’ ìë™ìœ¼ë¡œ Phase 5 ì‹œì‘
- âœ… ì‚¬ìš©ì ê°€ì´ë“œ ì™„ì„± â†’ ì¦‰ì‹œ Git í†µí•© ë‹¨ê³„ë¡œ ì „í™˜
- âœ… ëª¨ë“  docstring ì—…ë°ì´íŠ¸ â†’ ëŒ€ê¸° ì—†ì´ ë‹¤ìŒ ë‹¨ê³„ ì‹¤í–‰

### Phase 5: Git Integration (ìë™ ì‹¤í–‰)
The gitter-spark specialist will immediately:
- Review all changes and create meaningful commits
- Prepare deployment-ready code
- Generate release notes and version updates


## Usage Examples

```bash
/spark-launch "user notification system with email and SMS support"
/spark-launch "real-time chat feature with file sharing capabilities"
/spark-launch "advanced search functionality with filters and sorting"
/spark-launch "user dashboard with analytics and reporting"
/spark-launch "payment processing system with multiple gateways"



================================================================================
ëª…ë ¹ì–´: spark-migrate
íŒŒì¼: spark-migrate.md
================================================================================

# /spark-migrate - SPARK Legacy Migration & Modernization Pipeline  

**Purpose**: Comprehensive legacy system migration with risk assessment and modern implementation

## Execution Instructions

When this command is called, execute the following legacy migration pipeline:


## Usage Examples

```bash
/spark-migrate "migrate PHP legacy system to modern Node.js architecture"
/spark-migrate "modernize jQuery frontend to React with TypeScript"
/spark-migrate "move from monolithic Rails app to microservices"
/spark-migrate "migrate SQL Server database to PostgreSQL with optimization"
/spark-migrate "convert legacy REST API to GraphQL with better performance"



================================================================================
ëª…ë ¹ì–´: spark-optimize
íŒŒì¼: spark-optimize.md
================================================================================

# /spark-optimize - SPARK Performance Optimization Pipeline

**Purpose**: Comprehensive performance optimization with analysis, implementation, and validation

## Execution Instructions

When this command is called, execute the following performance optimization pipeline:


## Usage Examples

```bash
/spark-optimize "optimize API response times and database query performance"
/spark-optimize "improve frontend loading speed and bundle size optimization"
/spark-optimize "optimize memory usage and garbage collection in data processing"  
/spark-optimize "enhance search functionality performance with indexing strategies"
/spark-optimize "optimize image processing pipeline for faster throughput"



================================================================================
ëª…ë ¹ì–´: spark-refactor
íŒŒì¼: spark-refactor.md
================================================================================

# /spark-refactor - SPARK Multi-Agent Refactoring Pipeline

**Purpose**: Complete code refactoring with analysis, improvement, testing, and documentation

## Execution Instructions

When this command is called, execute the following multi-agent pipeline:


## Usage Examples

```bash
/spark-refactor "refactor authentication module for better maintainability"
/spark-refactor "modernize legacy API endpoints to follow REST standards"
/spark-refactor "optimize database queries and improve performance"
/spark-refactor "restructure component hierarchy for better reusability"



================================================================================
ëª…ë ¹ì–´: spark-test
íŒŒì¼: spark-test.md
================================================================================

# /spark-test - SPARK Testing Command

**Purpose**: Intelligent test generation, execution, and coverage analysis with SPARK enhancement

## Execution Instructions

When this command is called, I will engage the tester-spark testing specialist:

The tester-spark specialist will:
- Generate comprehensive test suites based on the codebase
- Execute tests and analyze results
- Optimize for 95%+ code coverage
- Ensure all quality standards are met
- Provide detailed coverage reports and recommendations

## Usage Examples

```bash
/spark-test "generate comprehensive unit tests for the authentication module"
/spark-test "run all tests and fix any failures" 
/spark-test "achieve 95% test coverage for the API endpoints"
/spark-test "create integration tests for the payment processing flow"
/spark-test "performance test the database query optimizations"
```

## Testing Capabilities

- **Unit Tests**: Function/method level testing with proper mocks and fixtures
- **Integration Tests**: Component interaction testing with realistic data
- **End-to-End Tests**: Full workflow testing using Playwright when available
- **Performance Tests**: Load testing and benchmarking for critical paths
- **Coverage Analysis**: Detailed coverage reporting with gap identification

## Quality Standards

All SPARK tests must meet:
- âœ… **95%+ Code Coverage**: Comprehensive coverage of critical functionality
- âœ… **Fast Execution**: Unit tests complete in < 10s, integration tests < 30s
- âœ… **Clear Naming**: Descriptive test names following AAA pattern
- âœ… **Proper Isolation**: Tests are independent and can run in any order
- âœ… **Realistic Scenarios**: Tests cover edge cases and error conditions

## SPARK Intelligence Integration

- ğŸ­ **QA Engineer Persona**: Activates testing-focused thinking patterns
- ğŸ§ª **Smart Test Generation**: AI-powered test case creation
- ğŸ“Š **Coverage Optimization**: Identifies gaps and suggests additional tests
- ğŸš€ **Optimized Token Usage**: Efficient test creation and execution

