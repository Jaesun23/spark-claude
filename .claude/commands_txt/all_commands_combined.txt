================================================================================
SPARK v3.5 - Î™®Îì† Î™ÖÎ†πÏñ¥ Ï†ïÏùò ÌÜµÌï© ÌååÏùº
================================================================================

Î™©Ï∞®:
----------------------------------------
‚Ä¢ multi-implement
‚Ä¢ spark-analyze
‚Ä¢ spark-audit
‚Ä¢ spark-clean
‚Ä¢ spark-design
‚Ä¢ spark-fix
‚Ä¢ spark-implement
‚Ä¢ spark-launch
‚Ä¢ spark-migrate
‚Ä¢ spark-optimize
‚Ä¢ spark-refactor
‚Ä¢ spark-test

================================================================================


================================================================================
Î™ÖÎ†πÏñ¥: multi-implement
ÌååÏùº: multi-implement.md
================================================================================

# /multi-implement - SPARK Parallel Multi-Team Implementation

**Purpose**: Execute multiple implementation tasks in parallel using up to 4 teams with JSON context relay

## üö® CRITICAL: ONE MESSAGE WITH MULTIPLE TOOL CALLS üö®

### **THE GOLDEN RULE FOR PARALLEL EXECUTION:**
```
YOU MUST USE A SINGLE MESSAGE WITH MULTIPLE TASK TOOL CALLS!
DO NOT SEND SEPARATE MESSAGES FOR EACH TASK!
```

## ‚ö° IMMEDIATE EXECUTION PROTOCOL

### **AS SOON AS YOU RECEIVE /multi-implement:**

```python
# ‚úÖ CORRECT - ALL IN ONE MESSAGE:
[Single Message]
‚îú‚îÄ‚îÄ Task("implementer-spark", "Team1: {task1} Read team1_current_task.json")
‚îú‚îÄ‚îÄ Task("implementer-spark", "Team2: {task2} Read team2_current_task.json")  
‚îú‚îÄ‚îÄ Task("implementer-spark", "Team3: {task3} Read team3_current_task.json")
‚îî‚îÄ‚îÄ Task("implementer-spark", "Team4: {task4} Read team4_current_task.json")
[Wait for all to complete]

# ‚ùå WRONG - SEQUENTIAL MESSAGES:
Message 1: Task("implementer-spark", "Team1...")
Message 2: Task("implementer-spark", "Team2...")  # NO! This waits for Team1!
Message 3: Task("implementer-spark", "Team3...")  # NO! This waits for Team2!
Message 4: Task("implementer-spark", "Team4...")  # NO! This waits for Team3!
```

### **EXECUTION STEPS:**
1. **PREPARE**: Create all team JSON files (team1_current_task.json, etc.)
2. **LAUNCH ALL AT ONCE**: Single message with 4 Task tool calls
3. **WAIT**: For all 4 teams to complete
4. **TEST ALL AT ONCE**: Single message with 4 tester Task calls
5. **REPORT**: Consolidate results

‚ö†Ô∏è **REMEMBER**: If you send Task calls in separate messages, they run SEQUENTIALLY, not in PARALLEL!

## üìù Orchestration Process

### Phase 0: Task Allocation
Analyze tasks and allocate to teams:
1. Parse task IDs from command
2. Team JSON files will be auto-generated by StateManager:
   ```python
   # StateManager automatically creates team#_current_task.json files
   # when write_team_state(team_id, task_data) is called
   # Uses hardcoded _default_state() structure with team_id added
   ```
3. Identify shared resources needing locks

### Phase 1: Parallel Implementation
Call all assigned teams SIMULTANEOUSLY (not sequentially):
```
# ALL FOUR CALLS AT ONCE - NO WAITING BETWEEN!
Task("team1-implementer-spark", "{task1}")
Task("team2-implementer-spark", "{task2}")
Task("team3-implementer-spark", "{task3}")
Task("team4-implementer-spark", "{task4}")
```

Each team implementer:
- Automatically reads their team#_current_task.json file
- Implements only their assigned feature
- Updates their JSON file with 'implementation' section
- Runs self-validation before exit (recommended)
- Respects file locks for shared resources

### Phase 1.5: Claude CODE Implementation Review
After all implementations complete, Claude CODE reviews all team results:

```python
# Claude CODE reviews each team's results
1. Read team1_current_task.json (check 'implementation' section)
2. Read team2_current_task.json (check 'implementation' section)  
3. Read team3_current_task.json (check 'implementation' section)
4. Read team4_current_task.json (check 'implementation' section)

# Decision for each team:
‚úÖ If team results satisfactory ‚Üí Proceed to Phase 2
‚ùå If issues found ‚Üí Re-call that team's implementer
```

### Phase 2: Parallel Testing
After Claude CODE approves all implementations, call testers:
```
Task("team1-tester-spark", "Test Team 1 implementation")
Task("team2-tester-spark", "Test Team 2 implementation")
Task("team3-tester-spark", "Test Team 3 implementation")
Task("team4-tester-spark", "Test Team 4 implementation")
```

Each team tester:
- Automatically reads their team#_current_task.json
- Creates comprehensive tests (95% coverage target)
- Updates their JSON file with 'testing' section
- Runs self-validation before exit (recommended)

### Phase 2.5: Claude CODE Testing Review
After all testing complete, Claude CODE reviews all test results:

```python
# Claude CODE reviews each team's test results
1. Read team1_current_task.json (check 'testing' section)
2. Read team2_current_task.json (check 'testing' section)
3. Read team3_current_task.json (check 'testing' section)  
4. Read team4_current_task.json (check 'testing' section)

# Decision for each team:
‚úÖ If test coverage ‚â•95% and all tests pass ‚Üí Proceed to Phase 3
‚ùå If issues found ‚Üí Re-call that team's tester
```

### Phase 3: Parallel Documentation
After Claude CODE approves all testing, call documenters:
```
Task("team1-documenter-spark", "Document Team 1 work")
Task("team2-documenter-spark", "Document Team 2 work")
Task("team3-documenter-spark", "Document Team 3 work")
Task("team4-documenter-spark", "Document Team 4 work")
```

Each team documenter:
- Automatically reads their team#_current_task.json
- Documents implementation and test results
- Updates their JSON file with 'documentation' section
- Runs self-validation before exit (recommended)

### Phase 3.5: Claude CODE Documentation Review
After all documentation complete, Claude CODE reviews all documentation results:

```python
# Claude CODE reviews each team's documentation
1. Read team1_current_task.json (check 'documentation' section)
2. Read team2_current_task.json (check 'documentation' section)
3. Read team3_current_task.json (check 'documentation' section)
4. Read team4_current_task.json (check 'documentation' section)

# Decision for each team:
‚úÖ If documentation complete ‚Üí Mark team as finished
‚ùå If issues found ‚Üí Re-call that team's documenter
```

### Phase 4: Final Consolidation
After all teams pass all phases, Claude CODE provides final report:

```python
# Final multi-team implementation report
- Summary of all team implementations
- Consolidated quality metrics
- Integration points and dependencies
- Overall project completion status
```

## üí° Quality Criteria

Same as single implementation:
- MyPy strict: 0 errors
- Ruff: 0 violations
- Test coverage: ‚â•95%
- Security scan: 0 issues
- Documentation: Complete

## üîß JSON Context Structure

Each team's JSON file (team#_current_task.json) contains:
```json
{
  "team_id": "team1",
  "task_id": "TASK-API-01",
  "status": "implementing|testing|documenting|completed",
  "task_details": {
    "description": "Implement user authentication endpoint",
    "files_to_modify": ["api/auth.py"],
    "requirements": ["JWT support", "Refresh tokens"]
  },
  "implementation": {
    "agent": "team1-implementer-spark",
    "timestamp": "ISO-8601",
    "status": "completed",
    "files_created": ["api/auth.py"],
    "files_modified": ["main.py"],
    "quality_metrics": {
      "linting_passed": true,
      "type_checking_passed": true
    }
  },
  "testing": {
    "agent": "team1-tester-spark",
    "timestamp": "ISO-8601",
    "status": "completed",
    "test_files": ["tests/test_auth.py"],
    "coverage": 96,
    "all_tests_pass": true
  },
  "documentation": {
    "agent": "team1-documenter-spark",
    "timestamp": "ISO-8601",
    "status": "completed",
    "docs_created": ["docs/auth_api.md"],
    "readme_updated": true
  },
  "locks_held": ["constants.py"],
  "self_validated": true
}
```

## üîí File Lock Management

For shared resources (constants.py, types.py):
- Teams request locks through JSON
- Orchestrator manages lock allocation
- 30-second timeout prevents deadlocks

## üöÄ Usage Examples

```bash
# 2 tasks in parallel
/multi-implement "TASK-API-01" "TASK-UI-02"

# 4 tasks in parallel 
/multi-implement "TASK-API-01" "TASK-UI-02" "TASK-SEC-03" "TASK-DATA-04"

# With task descriptions
/multi-implement "Create user auth endpoint" "Build dashboard component" "Add security middleware" "Implement data pipeline"
```

## üìä Performance Benefits

- **2 tasks**: ~1.8x faster than sequential
- **3 tasks**: ~2.5x faster than sequential
- **4 tasks**: ~3.1x faster than sequential

## ‚ö†Ô∏è Limitations

- Maximum 4 teams (context window constraint)
- All teams wait for slowest to complete each phase
- Shared file modifications are serialized
- No direct agent-to-agent communication


================================================================================
Î™ÖÎ†πÏñ¥: spark-analyze
ÌååÏùº: spark-analyze.md
================================================================================

# /spark-analyze - SPARK Analysis Command

**Purpose**: Multi-dimensional code and system analysis with evidence-based investigation

## Execution Instructions

When this command is called, I will delegate the analysis to the analyzer-spark specialist:

The analyzer-spark specialist will:
- Conduct comprehensive multi-perspective analysis
- Examine code from performance, security, and quality angles
- Provide evidence-based insights with specific metrics
- Generate actionable recommendations for improvements

## Usage Examples

```bash
/spark-analyze "performance bottlenecks in the authentication system"
/spark-analyze "security vulnerabilities in API endpoints"
/spark-analyze "code complexity and maintainability metrics"
/spark-analyze "dependency relationships and potential issues"
/spark-analyze "memory usage patterns in the data processing pipeline"
```

## Analysis Capabilities

- **Performance Analysis**: Bottlenecks, memory usage, optimization opportunities
- **Security Analysis**: Vulnerabilities, attack vectors, compliance issues  
- **Code Quality**: Complexity metrics, maintainability scores, technical debt
- **Architecture**: Structure analysis, coupling patterns, design issues
- **Dependencies**: Vulnerability scanning, usage optimization, conflict detection

## SPARK Intelligence Integration

- üé≠ **Code Analyst Persona**: Activates analytical thinking patterns
- üìä **Evidence-Based**: All findings backed by data and metrics
- üîç **Multi-Perspective**: Examines code from multiple angles
- üöÄ **Optimized Token Usage**: Focused analysis without token waste


================================================================================
Î™ÖÎ†πÏñ¥: spark-audit
ÌååÏùº: spark-audit.md
================================================================================

# /spark-audit - SPARK Security & Performance Audit Pipeline

**Purpose**: Complete project audit covering security, performance, and quality with actionable reports

## Execution Instructions

When this command is called, execute the following comprehensive audit pipeline:


## Usage Examples

```bash
/spark-audit "complete security and performance audit of the API layer"
/spark-audit "audit user authentication and authorization systems"  
/spark-audit "comprehensive review of data processing pipeline"
/spark-audit "audit payment processing system for compliance"



================================================================================
Î™ÖÎ†πÏñ¥: spark-clean
ÌååÏùº: spark-clean.md
================================================================================

# /spark-clean - SPARK Project Cleanup Command  

**Purpose**: Comprehensive project cleanup and technical debt reduction with SPARK intelligence

## Execution Instructions

When this command is called, I will activate the cleaner-spark optimization specialist:

The cleaner-spark specialist will:
- Perform comprehensive analysis of project structure and code quality
- Remove technical debt, duplicates, and unused resources
- Optimize file organization and directory structure
- Fix all linting and formatting issues
- Ensure no functionality is broken during cleanup

## Usage Examples

```bash
/spark-clean "full project cleanup and optimization"
/spark-clean "remove unused dependencies and dead code"
/spark-clean "fix all linting violations and code quality issues"
/spark-clean "optimize directory structure and file organization"
/spark-clean "clean up documentation and fix broken links"
```

## Cleanup Capabilities

- **File Management**: Remove duplicates, temporary files, empty directories
- **Code Quality**: Fix linting issues, improve formatting, add missing type hints
- **Dependencies**: Remove unused packages, optimize requirements, resolve conflicts
- **Structure**: Organize directories, consolidate functionality, improve imports
- **Documentation**: Clean outdated docs, fix broken references, improve README

## SPARK Quality Assurance

All cleanup operations ensure:
- ‚úÖ **No Functionality Broken**: All tests continue to pass
- ‚úÖ **Quality Improvements**: Better linting and type checking scores
- ‚úÖ **Structure Optimization**: Follows language/framework best practices  
- ‚úÖ **Token Efficiency**: Optimized usage maintained through cleanup process


================================================================================
Î™ÖÎ†πÏñ¥: spark-design
ÌååÏùº: spark-design.md
================================================================================

# /spark-design - SPARK Design Command

**Purpose**: System design and UI/UX creation with architecture and accessibility expertise

## Execution Instructions

When this command is called, I will collaborate with the designer-spark specialist:

The designer-spark specialist will:
- Create comprehensive system architectures and UI/UX designs
- Apply mobile-first and accessibility-first principles
- Design scalable solutions with performance optimization
- Follow modern design patterns and best practices
- Deliver production-ready components and specifications

## Usage Examples

```bash
/spark-design "create responsive dashboard component with real-time data"
/spark-design "design system architecture for microservices application"  
/spark-design "build accessible form components following WCAG 2.1"
/spark-design "create mobile-first navigation component"
/spark-design "design database schema for e-commerce platform"
```

## Design Capabilities

- **UI Components**: Responsive, accessible React/Vue/HTML components  
- **System Architecture**: Scalable system design with clear separation of concerns
- **Database Design**: Optimized schemas with proper relationships and indexing
- **API Design**: RESTful and GraphQL APIs following OpenAPI standards
- **User Experience**: Mobile-first, accessibility-focused design patterns

## Design Standards

All SPARK designs ensure:
- ‚úÖ **Responsive Design**: Mobile-first approach with all screen sizes supported
- ‚úÖ **Accessibility**: WCAG 2.1 AA compliance with semantic HTML and ARIA
- ‚úÖ **Performance**: Optimized loading, minimal bundle size, efficient rendering
- ‚úÖ **Scalability**: Designs that work from prototype to enterprise scale
- ‚úÖ **Consistency**: Follows established design systems and patterns

## SPARK Intelligence Integration

- üé≠ **Frontend/Architect Persona**: Activates design-focused thinking
- üé® **Modern Patterns**: Uses current best practices and design systems
- ‚ôø **Accessibility First**: Built-in WCAG compliance and inclusive design
- üöÄ **Optimized Token Usage**: Efficient design creation without bloat


================================================================================
Î™ÖÎ†πÏñ¥: spark-fix
ÌååÏùº: spark-fix.md
================================================================================

# /spark-fix - SPARK Troubleshooting Command

**Purpose**: Problem investigation, debugging, and root cause analysis with SPARK intelligence

## Execution Instructions

When this command is called, I will engage the troubleshooter-spark debugging specialist:

The troubleshooter-spark specialist will:
- Systematically investigate the reported issue
- Analyze error patterns, logs, and stack traces
- Apply scientific debugging methods to isolate root causes
- Develop targeted fixes with minimal side effects
- Verify solutions thoroughly to prevent regressions

## Usage Examples

```bash
/spark-fix "API endpoints returning 500 errors intermittently"
/spark-fix "memory leak in the data processing pipeline"
/spark-fix "tests failing after dependency update" 
/spark-fix "performance degradation in search functionality"
/spark-fix "authentication system not working in production"
```

## Troubleshooting Capabilities

- **Error Investigation**: Systematic analysis of logs, stack traces, and error patterns
- **Performance Debugging**: Profiling, bottleneck identification, optimization
- **Integration Issues**: API failures, database connectivity, service communication
- **Environment Problems**: Configuration, deployment, dependency conflicts
- **Code Logic Bugs**: Logic errors, edge cases, race conditions

## Debugging Process

SPARK troubleshooting follows systematic approach:
1. **Problem Reproduction**: Isolate and reproduce the issue consistently
2. **Root Cause Analysis**: Trace the issue to its underlying cause
3. **Solution Development**: Create targeted fix with minimal side effects
4. **Verification**: Test fix thoroughly and ensure no regressions
5. **Prevention**: Suggest improvements to prevent similar issues

## SPARK Intelligence Integration

- üé≠ **Debugger Persona**: Activates systematic problem-solving patterns
- üîç **Evidence-Based**: All conclusions backed by logs, metrics, and testing
- üß™ **Hypothesis Testing**: Scientific approach to isolating issues
- üöÄ **Optimized Token Usage**: Focused debugging without information overload


================================================================================
Î™ÖÎ†πÏñ¥: spark-implement
ÌååÏùº: spark-implement.md
================================================================================

# /implement - SPARK Implementation Command

**Purpose**: Quality-driven implementation workflow with 2Ìò∏'s intelligent orchestration and optimized token efficiency

## üöÄ Quality-Driven Multi-Agent Workflow

This command orchestrates a complete development pipeline with quality gates ensuring all deliverables meet SPARK standards before progressing.

### Workflow Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Implementer-Spark‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
         ‚Üì                ‚îÇ (Quality retry)
    Quality Gates ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì (Passed)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Tester-Spark    ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
         ‚Üì                ‚îÇ (Coverage retry)
    Test Validation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì (95%+ achieved)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Documenter-Spark ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
    ‚úÖ Completion Report
```

## üìù Claude CODE Action Protocol

### **UPON RECEIVING /implement COMMAND:**
```python
# Claude CODE's ORCHESTRATION PROTOCOL (systematic 3-phase execution)
1. Task("implementer-spark", user_request)  # CALL IMMEDIATELY
2. Wait for SubagentStop hook signal
3. Claude CODE reviews current_task.json:
   - Check `implementation` section
   - Review quality_metrics (linting, type checking)
   - Verify files_created and files_modified
   - Review next_steps and known_issues
4. DECISION:
   ‚úÖ If satisfied ‚Üí Task("tester-spark", implementation_context)
   ‚ùå If issues found ‚Üí Task("implementer-spark", retry_with_feedback)

5. Wait for tester SubagentStop hook signal  
6. Claude CODE reviews current_task.json:
   - Check `testing` section
   - Verify test coverage (target: 95%+)
   - Confirm all tests passing
   - Review test quality metrics
7. DECISION:
   ‚úÖ If satisfied ‚Üí Task("documenter-spark", context)
   ‚ùå If issues found ‚Üí Task("tester-spark", retry_with_feedback)

8. Wait for documenter SubagentStop hook signal
9. Claude CODE reviews current_task.json:
   - Check `documentation` section
   - Verify README updates
   - Confirm API documentation
   - Review usage examples
10. FINAL DECISION:
    ‚úÖ All phases complete ‚Üí Report success to user
    ‚ùå Issues found ‚Üí Task("documenter-spark", retry_with_feedback)
```

‚ö° **Core Principle**: Claude CODE reviews JSON results at each phase and decides next agent invocation

## üìù Orchestration Process

### Phase 1: Implementation
Claude CODE will delegate to implementer-spark specialist:

1. **Task Assignment**: Request the implementer-spark specialist to implement the feature
2. **Quality Validation**: The SPARK quality gates hook automatically validates:
   - Syntax correctness (0 errors)
   - Type checking (MyPy 0 errors)
   - Linting compliance (Ruff 0 violations)
   - Security scanning (0 issues)
   - Documentation presence (docstrings required)
3. **Auto-Retry**: If quality gates fail, the specialist automatically retries (max 3 attempts)

**Quality Review Checklist:**
- ‚úÖ Syntax validation (0 errors)
- ‚úÖ Type checking (MyPy 0 errors)  
- ‚úÖ Linting (Ruff 0 violations)
- ‚úÖ Security scan (0 issues)
- ‚úÖ Documentation (Docstrings required)

**Phase 1 ‚Üí Phase 2 Decision by Claude CODE:**
- Review `implementation` section in current_task.json
- Check quality_metrics in the JSON
- If satisfied ‚Üí Call tester-spark
- If issues found ‚Üí Call implementer-spark again with feedback

### Phase 2: Testing
After Claude CODE approves implementation, call tester-spark specialist:

1. **Test Development**: Request comprehensive test creation with 95%+ coverage target
2. **Test Validation**: The test runner hook automatically verifies:
   - All tests passing (0 failures)
   - Coverage ‚â• 95% (target achievement)
   - Edge cases covered (boundary testing)
   - Integration tests exist (system testing)
3. **Coverage Retry**: If coverage is below 95%, the specialist enhances tests (max 2 attempts)

**Test Quality Review:**
- ‚úÖ All tests passing (0 failures)
- ‚úÖ Coverage ‚â• 95% (target achieved)
- ‚úÖ Edge cases covered (boundary testing)
- ‚úÖ Integration tests exist (system testing)

**Phase 2 ‚Üí Phase 3 Decision by Claude CODE:**
- Review `testing` section in current_task.json
- Check test coverage (target: 95%+)
- If satisfied ‚Üí Call documenter-spark
- If issues found ‚Üí Call tester-spark again with feedback

### Phase 3: Documentation
After Claude CODE approves testing, call documenter-spark specialist:

1. **Documentation Creation**: Request comprehensive documentation including:
   - README updates with new features
   - API documentation for new endpoints
   - Usage examples and code samples
   - Inline docstrings for all functions
2. **Final Report**: Generate completion report with all deliverables

**Phase 3 Completion Criteria:**
- README.md updated
- API documentation complete (if applicable)
- Usage examples added
- All functions/classes have docstrings
- Final implementation report generated

## üí° Quality Standards

### Implementation Quality (Phase 1)
- **ÌïÑÏàò ÌÜµÍ≥º Ìï≠Î™©**: Syntax (0), MyPy (0), Ruff (0), Security (0), Docstrings (0)

### Testing Quality (Phase 2) 
- **ÌïÑÏàò Îã¨ÏÑ±**: Coverage ‚â•95%, Test failures (0), Edge cases covered

### Documentation Quality (Phase 3)
- **ÌïÑÏàò Ìè¨Ìï®**: README updates, API docs, Usage examples, Inline docstrings

## üöÄ Usage Examples

```bash
/implement "Create secure user authentication with JWT tokens"
/implement "Build responsive dashboard with real-time data"
/implement "Implement data validation pipeline with error handling"
```

## üìä SPARK Efficiency

- **Token Usage**: Lazy-loading architecture (only load required agents)
- **Quality Assurance**: 8 quality gates + retry system (max 3 attempts)
- **JSON Communication**: Unified current_task.json for all phases


================================================================================
Î™ÖÎ†πÏñ¥: spark-launch
ÌååÏùº: spark-launch.md
================================================================================

# /spark-launch - SPARK Full-Stack Feature Launch Pipeline

**Purpose**: Complete feature development from design to deployment with quality assurance

## üöÄ 5-Phase Automatic Development Pipeline

‚ö° **ÌïµÏã¨ ÏõêÏπô**: ÏàòÎèô ÌôïÏù∏ Ï†àÏ∞® ÏóÜÏùå - Î™®Îì† Îã®Í≥ÑÎäî Ï°∞Í±¥ Ï∂©Ï°± Ïãú ÏûêÎèô ÏßÑÌñâ

## üöÄ 5-Phase Development Pipeline

This command executes a comprehensive development workflow with multiple specialists:

### Phase 1: Design Architecture (ÏûêÎèô Ïã§Ìñâ)
I will immediately engage designer-spark specialist to:
- Create system architecture and UI/UX designs
- Define technical requirements and specifications
- Establish design patterns and component structures

**Phase 1 ‚Üí Phase 2 ÏûêÎèô ÏßÑÌñâ:**
- ‚úÖ ÏïÑÌÇ§ÌÖçÏ≤ò ÏÑ§Í≥Ñ ÏôÑÎ£å ‚Üí ÏûêÎèôÏúºÎ°ú Phase 2 ÏãúÏûë
- ‚úÖ UI/UX ÎîîÏûêÏù∏ ÏôÑÏÑ± ‚Üí Ï¶âÏãú Íµ¨ÌòÑ Îã®Í≥ÑÎ°ú Ï†ÑÌôò
- ‚úÖ Í∏∞Ïà† Ïä§Ìéô Î¨∏ÏÑú ÏûëÏÑ± ‚Üí ÎåÄÍ∏∞ ÏóÜÏù¥ Îã§Ïùå Îã®Í≥Ñ Ïã§Ìñâ

### Phase 2: Implementation (ÏûêÎèô Ïã§Ìñâ)
The implementer-spark specialist will immediately:
- Implement the core functionality based on design
- Follow established patterns and architecture
- Ensure code quality through SPARK quality gates

**Phase 2 ‚Üí Phase 3 ÏûêÎèô ÏßÑÌñâ:**
- ‚úÖ Î™®Îì† ÌíàÏßà Í≤åÏù¥Ìä∏ ÌÜµÍ≥º (5/5) ‚Üí ÏûêÎèôÏúºÎ°ú Phase 3 ÏãúÏûë
- ‚úÖ ÌïµÏã¨ Í∏∞Îä• Íµ¨ÌòÑ ÏôÑÎ£å ‚Üí Ï¶âÏãú ÌÖåÏä§Ìä∏ Îã®Í≥ÑÎ°ú Ï†ÑÌôò
- ‚úÖ Hook Í≤ÄÏ¶ù ÌÜµÍ≥º ‚Üí ÎåÄÍ∏∞ ÏóÜÏù¥ Îã§Ïùå Îã®Í≥Ñ Ïã§Ìñâ

### Phase 3: Comprehensive Testing (ÏûêÎèô Ïã§Ìñâ)
The tester-spark specialist will immediately:
- Create unit, integration, and end-to-end tests
- Achieve 95%+ code coverage
- Validate all functionality works as designed

**Phase 3 ‚Üí Phase 4 ÏûêÎèô ÏßÑÌñâ:**
- ‚úÖ ÌÖåÏä§Ìä∏ Ïª§Î≤ÑÎ¶¨ÏßÄ 95% Îã¨ÏÑ± ‚Üí ÏûêÎèôÏúºÎ°ú Phase 4 ÏãúÏûë
- ‚úÖ Î™®Îì† ÌÖåÏä§Ìä∏ ÌÜµÍ≥º (0 failures) ‚Üí Ï¶âÏãú Î¨∏ÏÑúÌôî Îã®Í≥ÑÎ°ú Ï†ÑÌôò
- ‚úÖ Í∏∞Îä• ÏôÑÏ†ÑÏÑ± Í≤ÄÏ¶ù ÏôÑÎ£å ‚Üí ÎåÄÍ∏∞ ÏóÜÏù¥ Îã§Ïùå Îã®Í≥Ñ Ïã§Ìñâ

### Phase 4: Documentation (ÏûêÎèô Ïã§Ìñâ)
The documenter-spark specialist will immediately:
- Create comprehensive API documentation
- Write user guides and examples
- Update project README and architecture docs

**Phase 4 ‚Üí Phase 5 ÏûêÎèô ÏßÑÌñâ:**
- ‚úÖ API Î¨∏ÏÑú ÏûëÏÑ± ÏôÑÎ£å ‚Üí ÏûêÎèôÏúºÎ°ú Phase 5 ÏãúÏûë
- ‚úÖ ÏÇ¨Ïö©Ïûê Í∞ÄÏù¥Îìú ÏôÑÏÑ± ‚Üí Ï¶âÏãú Git ÌÜµÌï© Îã®Í≥ÑÎ°ú Ï†ÑÌôò
- ‚úÖ Î™®Îì† docstring ÏóÖÎç∞Ïù¥Ìä∏ ‚Üí ÎåÄÍ∏∞ ÏóÜÏù¥ Îã§Ïùå Îã®Í≥Ñ Ïã§Ìñâ

### Phase 5: Git Integration (ÏûêÎèô Ïã§Ìñâ)
The gitter-spark specialist will immediately:
- Review all changes and create meaningful commits
- Prepare deployment-ready code
- Generate release notes and version updates


## Usage Examples

```bash
/spark-launch "user notification system with email and SMS support"
/spark-launch "real-time chat feature with file sharing capabilities"
/spark-launch "advanced search functionality with filters and sorting"
/spark-launch "user dashboard with analytics and reporting"
/spark-launch "payment processing system with multiple gateways"



================================================================================
Î™ÖÎ†πÏñ¥: spark-migrate
ÌååÏùº: spark-migrate.md
================================================================================

# /spark-migrate - SPARK Legacy Migration & Modernization Pipeline  

**Purpose**: Comprehensive legacy system migration with risk assessment and modern implementation

## Execution Instructions

When this command is called, execute the following legacy migration pipeline:


## Usage Examples

```bash
/spark-migrate "migrate PHP legacy system to modern Node.js architecture"
/spark-migrate "modernize jQuery frontend to React with TypeScript"
/spark-migrate "move from monolithic Rails app to microservices"
/spark-migrate "migrate SQL Server database to PostgreSQL with optimization"
/spark-migrate "convert legacy REST API to GraphQL with better performance"



================================================================================
Î™ÖÎ†πÏñ¥: spark-optimize
ÌååÏùº: spark-optimize.md
================================================================================

# /spark-optimize - SPARK Performance Optimization Pipeline

**Purpose**: Comprehensive performance optimization with analysis, implementation, and validation

## Execution Instructions

When this command is called, execute the following performance optimization pipeline:


## Usage Examples

```bash
/spark-optimize "optimize API response times and database query performance"
/spark-optimize "improve frontend loading speed and bundle size optimization"
/spark-optimize "optimize memory usage and garbage collection in data processing"  
/spark-optimize "enhance search functionality performance with indexing strategies"
/spark-optimize "optimize image processing pipeline for faster throughput"



================================================================================
Î™ÖÎ†πÏñ¥: spark-refactor
ÌååÏùº: spark-refactor.md
================================================================================

# /spark-refactor - SPARK Multi-Agent Refactoring Pipeline

**Purpose**: Complete code refactoring with analysis, improvement, testing, and documentation

## Execution Instructions

When this command is called, execute the following multi-agent pipeline:


## Usage Examples

```bash
/spark-refactor "refactor authentication module for better maintainability"
/spark-refactor "modernize legacy API endpoints to follow REST standards"
/spark-refactor "optimize database queries and improve performance"
/spark-refactor "restructure component hierarchy for better reusability"



================================================================================
Î™ÖÎ†πÏñ¥: spark-test
ÌååÏùº: spark-test.md
================================================================================

# /spark-test - SPARK Testing Command

**Purpose**: Intelligent test generation, execution, and coverage analysis with SPARK enhancement

## Execution Instructions

When this command is called, I will engage the tester-spark testing specialist:

The tester-spark specialist will:
- Generate comprehensive test suites based on the codebase
- Execute tests and analyze results
- Optimize for 95%+ code coverage
- Ensure all quality standards are met
- Provide detailed coverage reports and recommendations

## Usage Examples

```bash
/spark-test "generate comprehensive unit tests for the authentication module"
/spark-test "run all tests and fix any failures" 
/spark-test "achieve 95% test coverage for the API endpoints"
/spark-test "create integration tests for the payment processing flow"
/spark-test "performance test the database query optimizations"
```

## Testing Capabilities

- **Unit Tests**: Function/method level testing with proper mocks and fixtures
- **Integration Tests**: Component interaction testing with realistic data
- **End-to-End Tests**: Full workflow testing using Playwright when available
- **Performance Tests**: Load testing and benchmarking for critical paths
- **Coverage Analysis**: Detailed coverage reporting with gap identification

## Quality Standards

All SPARK tests must meet:
- ‚úÖ **95%+ Code Coverage**: Comprehensive coverage of critical functionality
- ‚úÖ **Fast Execution**: Unit tests complete in < 10s, integration tests < 30s
- ‚úÖ **Clear Naming**: Descriptive test names following AAA pattern
- ‚úÖ **Proper Isolation**: Tests are independent and can run in any order
- ‚úÖ **Realistic Scenarios**: Tests cover edge cases and error conditions

## SPARK Intelligence Integration

- üé≠ **QA Engineer Persona**: Activates testing-focused thinking patterns
- üß™ **Smart Test Generation**: AI-powered test case creation
- üìä **Coverage Optimization**: Identifies gaps and suggests additional tests
- üöÄ **Optimized Token Usage**: Efficient test creation and execution

