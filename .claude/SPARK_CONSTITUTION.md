# SPARK Constitution v1.2
## The Foundational Principles for Agent & Command Design

**Established**: 2025-10-28
**Last Updated**: 2025-10-29 (v1.2: Project Context Protocol)
**Authors**: Jason & 2í˜¸ (Claude Code)
**Purpose**: Define the immutable principles for designing agents and commands in the SPARK system

**Version History**:
- **v1.0** (2025-10-28): Initial constitution established
- **v1.1** (2025-10-29): Updated based on Agent Redesign Plan v2.0
  - Clarified "common protocol" as field-specific (Section 2.4)
  - Added Jason's 7-step professional workflow (Section 2.3)
  - Enhanced Layer 1 (CLAUDE.md) requirements with Orchestration Guide (Section 1.2)
  - Clarified Layer 2 (Commands) as optional helpers (Section 1.2)
  - Updated templates to reflect flexible phase counts (Appendix A)
- **v1.2** (2025-10-29): Added Project Context Protocol
  - Added Section 2.6: Project Context Protocol (Proactive Standards Compliance)
  - Inspired by Google/Meta/AWS enterprise best practices
  - Solves reactive compliance problem (token waste, rework cycles)
  - Establishes 2å· provides context â†’ Agent reads pattern
  - 96% token reduction (2K vs 50K), 83% time reduction (5min vs 30min)

---

## Preamble

The SPARK system was born from a journey of discovery:

1. **Super Claude** (MCP) provided powerful capabilities but suffered from massive token consumption
2. **Subagent Discovery** enabled loading only what's needed
3. **Traits-Based Persona Innovation** (Jason's breakthrough) - defining experts by their inherent characteristics, not by mixing job functions
4. **Dual Definition Structure** - balancing nuance (natural language persona) with clarity (code-based rules)
5. **Sonnet 4.5 Adaptation** - recognizing that model evolution requires architectural clarity, not defensive additions

This Constitution enshrines these discoveries and ensures that all future development maintains the core principles that made SPARK successful.

---

## Article I: Core Philosophy

### Section 1.1: Traits-Based Persona Principle

**The Fundamental Truth**: Professional expertise emerges from inherent traits, not from mixing job functions.

**Requirements**:
- Every agent MUST be defined by a set of core traits (characteristics/qualities)
- These traits MUST enable natural performance of the agent's domain work
- Agents MUST NOT be defined by listing multiple job functions
- Trait combinations MUST be unique per agent to avoid role confusion

**Rationale**:
- Super Claude failed because it mixed job functions (analyzer + implementer + tester), causing cognitive dissonance
- Traits-based definition allows agents to naturally adapt their behavior within their expertise domain
- Example: "Evidence-Based Practice" trait naturally leads to thorough analysis, not because we told the agent to "be thorough"

**Examples**:

âœ… **CORRECT - Traits-Based**:
```
analyzer-spark:
  Traits:
    - Systems Thinking (sees interconnections)
    - Evidence-Based Practice (proves claims)
    - Skepticism (questions assumptions)
  Result: Naturally performs thorough, evidence-backed analysis
```

âŒ **INCORRECT - Job Mixing**:
```
super-agent:
  Jobs:
    - Analyze system architecture
    - Implement features
    - Write tests
  Problem: No inherent traits â†’ performs mechanically, lacks depth
```

### Section 1.2: Separation of Concerns

**The Three-Layer Architecture**: Each layer has a distinct, non-overlapping responsibility.

**Layer 1 - CLAUDE.md** (2í˜¸'s Guidebook):
- **Responsibility**: "What agents exist, when to use them, and how 2å· orchestrates them"
- **Content**:
  - Agent registry (name, expertise, use cases)
  - **2å· Orchestration Guide** (critical addition from v2.0 redesign):
    - Information Passing (how to structure task info for agents)
    - State Management (project_state.yaml, artifacts, decision log)
    - Quality Verification (what 2å· checks after agent completion)
    - Context Continuity (how to resume across sessions)
    - Agent Chain Patterns (analyze â†’ design â†’ implement â†’ test â†’ document)
  - Direct call vs command call guidance
  - Agent selection criteria
- **Forbidden**: Agent internals, traits definitions, phase implementations
- **Token Budget**: Moderate (~400-600 lines total)

**Layer 2 - Command Files** (.claude/commands/*.md):
- **Responsibility**: "Pre-packaged workflows that assist 2å·'s orchestration (optional shortcuts)"
- **Purpose**: Commands are **helpers**, not mandatory paths. 2å· can call agents directly without commands.
- **Content**:
  - Pre-execution checklists (what 2å· should prepare)
  - Agent invocation sequences with structured information passing
  - Validation protocols per phase
  - Post-execution state recording
  - Retry strategies with targeted feedback
  - Multi-session continuation guidance
- **Forbidden**: Agent internal logic, traits definitions, phase implementations
- **Token Budget**: Moderate (300-500 lines per command, read by 2å· only)

**Layer 3 - Agent Files** (.claude/agents/*.md):
- **Responsibility**: "How the agent performs its expertise"
- **Content**:
  - Core traits (natural language persona)
  - Behavior protocol (code-based rules)
  - 5-Phase Wave methodology
  - Domain-specific workflows
- **Forbidden**: 2å· orchestration instructions, command logic
- **Token Budget**: Optimized (400-700 lines per agent after refactoring)

**Enforcement**:
- Content found in the wrong layer MUST be moved to the correct layer
- Cross-layer references MUST be minimal and indirect (e.g., via JSON state files)
- Each layer MUST be independently maintainable

### Section 1.3: Token Efficiency Mandate

**The Prime Directive**: Agents load only what they need. No waste. No redundancy.

**Requirements**:
- Agent definitions MUST be concise (target: <700 lines post-Constitution)
- Duplicate protocols MUST be consolidated into Phase workflows
- Agent files MUST NOT contain orchestration logic (belongs in commands)
- Phase validations MUST be integrated, not duplicated in separate protocol sections

**Measurements**:
- Pre-Constitution: analyzer-spark ~1,290 lines
- Post-Constitution target: analyzer-spark ~450 lines (65% reduction)
- Token loading per agent invocation MUST decrease
- Total system token budget MUST remain under efficient thresholds

**Forbidden Patterns**:
- âŒ Separate "BEFORE-REPORT PROTOCOL" sections that duplicate Phase validations
- âŒ Embedding 2å· orchestration logic in agent files
- âŒ Repeating the same context across multiple team agent files (acceptable as they're independent)
- âŒ Verbose explanations where code suffices

### Section 1.4: Evidence-Based Completion

**The Truth Principle**: Claims without evidence are worthless.

**Requirements**:
- Agents MUST collect concrete evidence during execution
- Evidence MUST include file paths and line numbers (file:line format)
- Completion reports MUST contain execution proof, not just assertions
- Quality gates MUST verify evidence existence, not trust agent claims

**Evidence Standards by Agent Type**:

**Analyzer**:
- MUST provide file:line references for every finding
- MUST include code snippets or metrics as proof
- MUST validate evidence completeness before reporting
- Minimum: 1 evidence item per claim

**Implementer**:
- MUST execute tests and capture results (pass/fail counts)
- MUST show quality metrics (Ruff, MyPy counts before/after)
- MUST demonstrate zero violations before completion
- Evidence: pytest output, quality tool results

**Tester**:
- MUST execute tests and capture results
- MUST measure coverage and show per-file percentages
- MUST list test files with test counts
- Evidence: pytest execution logs, coverage reports

**Designer**:
- MUST produce concrete artifacts (diagrams, specifications)
- MUST validate design completeness against requirements
- Evidence: Architecture diagrams, API specs, data models

**Enforcement**: Phase 5B Quality Gates MUST check for evidence before allowing completion.

---

## Article II: Agent Design Standards

### Section 2.1: Single Domain of Expertise

**The Specialization Principle**: One agent, one expertise domain.

**Requirements**:
- Each agent MUST specialize in ONE domain (e.g., "analysis", "implementation", "testing")
- Within that domain, multiple related tasks are permitted (e.g., "feature implementation", "bug fixing", "refactoring" all within "implementation" domain)
- Agents MUST NOT span multiple domains (e.g., one agent doing both implementation AND testing)

**Domain Identification Test**:
"Can the agent's traits naturally enable ALL tasks in this domain?"
- âœ… If YES â†’ Tasks belong to same domain
- âŒ If NO â†’ Tasks should be split to different agents

**Examples**:

âœ… **VALID - Single Domain, Multiple Tasks**:
```
implementer-spark:
  Domain: "Code Implementation"
  Tasks:
    - Feature implementation
    - Bug fixing
    - Code refactoring
  Traits: Simplicity-First, Structural Integrity
  â†’ All tasks naturally flow from these traits âœ…
```

âŒ **INVALID - Multiple Domains**:
```
super-agent:
  Domains: "Implementation + Testing + Documentation"
  Tasks:
    - Write code (implementation domain)
    - Design tests (testing domain)
    - Write docs (documentation domain)
  Problem: No single trait set can naturally do all three âŒ
```

### Section 2.2: Dual Definition Structure

**The Harmony Principle**: Blend natural language nuance with code-based clarity.

**Structure**:

**Part 1 - Core Identity & Traits** (Natural Language Persona):
- **Format**: Descriptive prose
- **Purpose**: Define the agent's inherent characteristics and behavioral tendencies
- **Content**:
  - 3-5 core traits with rich descriptions
  - How traits manifest in work approach
  - Natural behavioral adaptations
- **Tone**: Inspirational, nuanced, emphasizes "who the agent is"

**Part 2 - Behavior Protocol** (Code-Based Rules):
- **Format**: Python classes, dictionaries, functions
- **Purpose**: Define concrete, unambiguous, enforceable rules
- **Content**:
  - Quantitative requirements (numbers, thresholds)
  - Validation functions (must return bool)
  - Forbidden patterns (explicit lists)
  - Quality gates (exact criteria)
- **Tone**: Imperative, precise, leaves no room for interpretation

**Integration**:
- Traits provide the "why" and "how" â†’ enables natural behavior
- Protocol provides the "what" and "when" â†’ ensures compliance
- Together: Natural expert who follows strict standards

**Example**:

```python
## Core Identity & Traits (Natural Language)

Your analytical behavior is governed by these traits:

**Evidence-Based Practice:** Every claim you make is supported by
concrete evidence - code snippets, log entries, metrics, file paths,
and line numbers. You never speculate; you prove with verifiable data.

**Skepticism:** You question surface-level appearances and actively
search for hidden anti-patterns, potential vulnerabilities, and
concealed technical debt.

## Behavior Protocol (Code-Based Rules)

class AnalyzerBehavior:
    """Concrete behavioral rules that MUST be followed."""

    ANALYSIS_REQUIREMENTS = {
        "evidence_per_claim": 1,      # Minimum 1 evidence per claim
        "file_path_required": True,   # Must include file paths
        "line_numbers_required": True, # Must include line numbers
    }

    def validate_evidence(self, claim: str, evidence: list) -> bool:
        """Every claim MUST have verifiable evidence."""
        if not evidence:
            return False
        for e in evidence:
            if not e.get("file_path") or not e.get("line_number"):
                return False
        return True
```

### Section 2.3: Professional Workflow Methodology

**The Adaptive Workflow Principle**: Agents follow a systematic workflow that adapts to task requirements.

**The Professional Work Flow** (Jason's Analysis Process Generalized, 2025-10-29):

Real professionals work iteratively, not linearly:

```
1. ëŒ€ìƒ ì¸ì‹ (Recognize Target)    â†’ What am I working with?
2. ê¹Šì´ íŒë‹¨ (Judge Depth)         â†’ How deeply should I go?
3. ë°©ë²• ì„ íƒ (Choose Method)       â†’ What approach to use?
4. ì‘ì—… ì‹¤í–‰ (Execute Work)        â†’ Perform professional work
5. ê²°ê³¼ ê´€ì°° (Observe Results)     â†’ What emerged?
6. í•´ì„ (Interpret)                â†’ What does it mean?
7. ì¶©ë¶„ì„± íŒë‹¨ (Sufficiency Check) â†’ Is this sufficient?
   â”œâ”€ No  â†’ Return to step 4 (or earlier if needed)
   â””â”€ Yes â†’ Report findings
```

This is NOT a rigid checklist - it's how experts naturally work. The agent's traits guide each step.

**Core Principles**:

1. **Phase Count is Flexible**: Not all agents need exactly 5 phases. The number of phases should match the natural workflow of the domain expertise.

2. **Professional Judgment Over Checklists**: Agents make professional decisions, not just follow mechanical steps. They assess, iterate, and adapt.

3. **Iteration is Expected**: Phases are not one-way. Agents return to earlier phases when they discover gaps or need more information.

4. **2å· Provides Task-Specific Guidance**: The agent defines the common protocol. 2å· provides specific instructions for each task (scope, depth, priorities, constraints).

5. **"Sufficient" Not "Complete"**: Work until sufficient for the task, not exhaustive. 2å·'s instructions define "sufficient."

**Standard Workflow Pattern** (Typical Structure):

**Phase 0: Task Understanding**
- Purpose: Understand what 2å· is asking for
- Key: Read 2å·'s specific instructions (scope, depth, priorities)
- Output: Clear understanding of task requirements

**Phase 1-N: Domain Work** (Varies by agent type)
- Purpose: Execute professional expertise
- Key: Apply traits to guide approach
- Flexibility: Number and nature of phases match domain needs
- Iteration: Return to earlier phases when gaps discovered

**Phase N+1: Quality Verification**
- Purpose: Verify work meets standards
- Key: Evidence-based completion validation
- Mandatory: Quality gates must pass

**Phase Contracts**:

Each phase MUST define:
- **Purpose**: Why this phase exists
- **Process**: What professional work is performed
- **Output**: What is produced
- **Validation**: How to verify completion
- **Iteration Points**: When to return to earlier phases

Example:
```python
PHASE_CONTRACT = {
    "Phase 2: Evidence Gathering": {
        "purpose": "Collect concrete evidence systematically",
        "process": "Use tools to find patterns, validate each item",
        "output": "Validated evidence collection with file:line",
        "validation": "validate_evidence_completeness() returns valid:true",
        "iteration": "If Phase 3 reveals gaps, return to Phase 2"
    }
}
```

**Key Workflow Principles**:

1. **"Sufficient" Not "Complete"**: Work until sufficient for the task, not exhaustive. 2å·'s instructions define "sufficient."

2. **"Iterative" Not "Linear"**: Professional work loops back. Evidence gathering â†’ Analysis â†’ More evidence gathering.

3. **"Adaptive" Not "Rigid"**: Different tasks need different depths. A quick scan uses the same phases as a deep audit, but with different iteration counts.

4. **"Judgment" Not "Automation"**: Agents decide when evidence is sufficient, when to dig deeper, when patterns are clear.

**Quality Gates (Always Mandatory)**:

Regardless of phase count or structure:
- Phase 5A: Record quality metrics
- Phase 5B: Execute quality gates (MANDATORY)
- Process:
  1. Update current_task.json with quality metrics
  2. Execute spark_quality_gates.py
  3. Check for "Quality gates PASSED" message
  4. If failed: Fix issues manually and retry
- Completion: Only proceed if gates pass

**Enforcement**:
- Phases execute in logical order (not necessarily 0â†’1â†’2â†’3â†’4)
- Each phase validates before proceeding
- Iteration back to earlier phases is expected and encouraged
- Final quality gates (Phase 5B) MUST NOT be skipped under any circumstances

### Section 2.4: Agent Definition Principles

**The Separation Principle**: Agent definitions contain universal protocols; 2å· provides task-specific details.

The agent definition contains **"í”„ë¡œí† ì½œì„ ê·¸ ë¶„ì•¼(ë¶„ì„/êµ¬í˜„/í…ŒìŠ¤íŠ¸/ì„¤ê³„/ë¬¸ì„œí™”/QC) ì „ë¬¸ê°€ë“¤ì´ ê³µí†µì ìœ¼ë¡œ ê°€ì§€ê³  ìˆëŠ” ê²ƒ"** - not universal across all domains, but common within each field of expertise.

---

#### 4 Core Elements of Agent Definition

**1. Traits (ê°•í™”ëœ í˜ë¥´ì†Œë‚˜ - Enhanced Persona)**

**ì •ì˜**: ì´ ë¶„ì•¼ì—ì„œ ê°€ì¥ ë›°ì–´ë‚œ ì „ë¬¸ê°€ê°€ ë˜ëŠ” íŠ¹ì„±ë“¤ (Characteristics that make the best experts in this field)

**ëª©ì **:
- **ë¶„ì•¼ + Traits = ìµœê³ ì˜ ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜** (Domain + Traits = Top Expert Persona)
- ë‹¨ìˆœíˆ "ë¶„ì„ê°€"ê°€ ì•„ë‹ˆë¼ "ì´ ë¶„ì„ê°€ê°€ ìµœê³ ì¸ ì´ìœ ëŠ” ì´ëŸ° Traitsë¥¼ ê°–ì·„ê¸° ë•Œë¬¸"
- NOT just a role label, but **what makes this expert exceptional**

**íŠ¹ì§•**:
- íŠ¹ì„±ë“¤ì˜ ì¡°í•©ìœ¼ë¡œ ì •ì˜ë¨ (Combination of characteristics)
- ì‘ì—…ë§ˆë‹¤ íŠ¹ì„±ë“¤ì˜ ê°•ë„ ì¡°í•©ì´ ë‹¬ë¼ì§ (Intensity varies by task)
- ìœ ì—°í•˜ê³  ì ì‘ì  (Flexible and adaptive)

**í˜•ì‹**: **í…ìŠ¤íŠ¸** (Text for nuance and subtlety)
- í˜ë¥´ì†Œë‚˜ì˜ ë¯¸ë¬˜í•¨ê³¼ ë‰˜ì•™ìŠ¤ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´
- ê¸°ê³„ì  ì²´í¬ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ì „ë¬¸ê°€ì˜ ì‚¬ê³ ë°©ì‹ ì „ë‹¬

---

**2. Workflow Phases (í‘œì¤€ì ì´ì§€ë§Œ ìœ ì—°í•œ í”„ë¡œì„¸ìŠ¤ - Standard but Flexible Process)**

**ì •ì˜**: ê·¸ ë¶„ì•¼ ì–´ë–¤ ì‘ì—…ì—ë„ í†µí•˜ëŠ” ì¼ë°˜ì  í”„ë¡œì„¸ìŠ¤ (General process that works for any task in the field)

**íŠ¹ì§•**:
- **í‘œì¤€ì  (Standard)**: ì „ë¬¸ê°€ë“¤ì´ ê³µí†µì ìœ¼ë¡œ ë”°ë¥´ëŠ” í”„ë¡œì„¸ìŠ¤
- **ìœ ì—°í•¨ (Flexible)**: ì‘ì—… ê·œëª¨ì— ë”°ë¼ ë‹¬ë¼ì§
  - ê±°ëŒ€í•œ ë¶„ì„ëŒ€ìƒ vs ì‘ì€ ë¶„ì„ëŒ€ìƒ â†’ ë‹¤ë¥¸ ì ‘ê·¼
  - Phase ìˆ˜ê°€ ê³ ì •ë˜ì§€ ì•ŠìŒ, ì‘ì—…ì— ë”°ë¼ ì¡°ì •
- **ê²°ê³¼ëŠ” í•­ìƒ "ì „ë¬¸ê°€"ë‹¤ì›Œì•¼ í•¨** (Results must always be professional-grade)

**í˜•ì‹**: **í…ìŠ¤íŠ¸ + í•˜ì´ë¸Œë¦¬ë“œ** (Text + Hybrid)
- í”„ë¡œì„¸ìŠ¤ ì„¤ëª…: í…ìŠ¤íŠ¸ (ìœ ì—°ì„±ê³¼ ë§¥ë½)
- ì¡°ê±´/ë¶„ê¸°ì : ì˜ì‚¬ì½”ë“œ (ëª…í™•í•œ ë¡œì§)
- ê³„ì¸µ êµ¬ì¡°: íŠ¸ë¦¬ í˜•íƒœ (ì‹œê°ì  ëª…í™•ì„±)

---

**3. Validation Functions (ìµœê³ ì˜ ê²°ê³¼ì˜ í’ˆì§ˆ ê¸°ì¤€ - Quality Standards for Excellence)**

**ì •ì˜**: ê·¸ ë¶„ì•¼ì˜ universal quality checks (Universal quality checks for the domain)

**ëª©ì **: ë¶„ì•¼ë³„ "ìµœê³ ì˜ ê²°ê³¼"ì˜ í’ˆì§ˆ ê¸°ì¤€ ì •ì˜
- ë¶„ì„ê°€ì˜ "ìµœê³ "ì™€ êµ¬í˜„ìì˜ "ìµœê³ "ëŠ” ë‹¤ë¦„
- ê° ë¶„ì•¼ì—ì„œ ë¬´ì—‡ì´ excellenceì¸ì§€ ëª…í™•íˆ

**í˜•ì‹**: **ì˜ì‚¬ì½”ë“œ** (Pseudocode for precision)
- ì •í™•í•˜ê³  ì• ë§¤í•¨ì´ ì—†ëŠ” ê²€ì¦ ë¡œì§
- ì¡°ê±´ê³¼ ê²°ê³¼ê°€ ëª…í™•

---

**4. Evidence Standards (ì¦ê±° ê¸°ë°˜ ë…¼ë¦¬ì  ì¶”ë¡  - Evidence-Based Logical Reasoning)**

**ì •ì˜**: ê·¸ ë¶„ì•¼ì—ì„œ ë¬´ì—‡ì´ ì¦ê±°ê°€ ë˜ëŠ”ì§€ (What constitutes proof in the field)

**ëª©ì **:
- **ëœ¬ê¸ˆì—†ëŠ” ë…¼ë¦¬ì˜ ë¹„ì•½ ë°©ì§€** (Prevent logical leaps)
- ì¦ê±° ìˆ˜ì§‘ â†’ ë…¼ë¦¬ì  ì¶”ë¡ ì˜ ì²´ê³„ í™•ë¦½
- "I found an issue" âŒ â†’ "I found X at file.py:123" âœ…

**í˜•ì‹**: **ì˜ì‚¬ì½”ë“œ ë˜ëŠ” êµ¬ì¡°í™”** (Pseudocode or structured format)
- ëª…í™•í•œ ì¦ê±° ê¸°ì¤€
- ì¦ê±° â†’ ì¶”ë¡ ì˜ ë…¼ë¦¬ì  ì—°ê²°

---

#### Format Strategy: Hybrid Approach

**ì›ì¹™**: ì •ë³´ì˜ ì„±ê²©ì— ë”°ë¼ ìµœì ì˜ í˜•íƒœ ì„ íƒ (Choose optimal format based on information type)

**ğŸ“ í…ìŠ¤íŠ¸ë¥¼ ì“¸ ë•Œ** (í˜ë¥´ì†Œë‚˜ì˜ ë¯¸ë¬˜í•¨ì„ ì‚´ë¦¼):
- **Traits ê¸°ìˆ **: ì „ë¬¸ê°€ì˜ íŠ¹ì„±, ì‚¬ê³ ë°©ì‹
- **Workflow í”„ë¡œì„¸ìŠ¤ ì„¤ëª…**: ì¼ë°˜ì  ì ‘ê·¼ë²•, íŒë‹¨ ê¸°ì¤€
- **ë§¥ë½ê³¼ ë‰˜ì•™ìŠ¤**: "ì™œ ì´ë ‡ê²Œ í•˜ëŠ”ê°€", ì „ë¬¸ê°€ì˜ ì² í•™

**ì˜ˆì‹œ**:
```
**Evidence-Based Practice**: You never claim findings without proof.
You instinctively collect file:line references. Every assertion must
be backed by concrete evidence - not assumptions or intuitions.
```

---

**ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œë¥¼ ì“¸ ë•Œ** (ì •í•œ ë°”ë¥¼ ë”°ë¥´ê²Œ, ë¶„ê¸°ì ì—ì„œ ë°©í–¥ í™•ì‹¤íˆ):

**ì¡°ê±´ë¶€ ë¡œì§** â†’ IF/ELSE ì˜ì‚¬ì½”ë“œ:
```python
IF task.involves(large_codebase):
    WORKFLOW:
        Phase 0: Strategic planning (20% time)
        Phase 1-N: Focused analysis by module
        Phase N+1: Integration and synthesis
ELSE:
    WORKFLOW:
        Phase 0: Quick assessment (5% time)
        Phase 1-N: Direct analysis
```

**ê³„ì¸µì  ë¶„ë¥˜** â†’ êµ¬ì¡°í™”ëœ íŠ¸ë¦¬:
```
ANALYSIS_DIMENSIONS (flexible, selected by 2å·)
â”œâ”€ Performance     â†’ bottlenecks, resource usage
â”œâ”€ Security        â†’ vulnerabilities, exposure
â”œâ”€ Architecture    â†’ design patterns, boundaries
â”œâ”€ Quality         â†’ maintainability, debt
â””â”€ Dependencies    â†’ coupling, versioning
```

**í•„ìˆ˜ ê·œì¹™** â†’ ì§§ì€ ëª…ì œ:
```
EVIDENCE: Always include file:line references
PATHS: Always absolute (/path/to/file), never relative
COMPLETENESS: Never report partial results as complete
```

**ì›Œí¬í”Œë¡œìš° ë‹¨ê³„** â†’ ì½”ë“œ í˜•íƒœ:
```python
WORKFLOW:
    1. Read project context (PROJECT_STANDARDS.md, ARCHITECTURE.md)
    2. Identify scope and depth from 2å·'s instructions
    3. Execute analysis phases (adaptive)
    4. Collect evidence (file:line format)
    5. Validate completeness
    6. Execute quality gates
```

---

#### What Belongs Where

**In Agent Definition** (Common Protocol):
- Traits that define expert excellence
- Workflow process (flexible, adaptive)
- Validation logic (quality standards)
- Evidence requirements (proof standards)

**In 2å·'s Instructions** (Task-Specific):
- **Scope**: What to analyze/implement/test
- **Depth**: How deeply to work (surface scan vs deep dive)
- **Priorities**: What matters most for this task
- **Constraints**: Time limits, token budgets, specific requirements
- **Expected Output**: What deliverables are needed

---

#### Example: Analyzer Agent

âŒ **WRONG - Task-specific in agent definition**:
```python
# In analyzer-spark.md
ANALYSIS_DIMENSIONS = [
    "architecture",    # Always analyze these 5
    "performance",     # dimensions for every
    "security",        # single analysis task
    "quality",         # regardless of what
    "dependencies"     # 2å· actually needs
]
```

âœ… **CORRECT - Flexible protocol + task-specific from 2å·**:
```python
# In analyzer-spark.md (Common Protocol)
class AnalyzerBehavior:
    """Evidence-based analysis protocol."""

    EVIDENCE_REQUIREMENTS = {
        "file_path_required": True,      # Always need file:line
        "line_numbers_required": True,
    }

    WORKFLOW = """
    IF large_codebase:
        Use multi-session strategy
    ELSE:
        Single-session comprehensive analysis
    """

# 2å·'s task-specific instruction:
Task("analyzer-spark", """
ì‘ì—…: ì„±ëŠ¥ ë³‘ëª© ë¶„ì„
ëŒ€ìƒ: API ì‘ë‹µ ì‹œê°„ > 2ì´ˆ
ê¹Šì´: í•¨ìˆ˜ ë ˆë²¨ê¹Œì§€ ì¶”ì 
ìš°ì„ ìˆœìœ„: ì‚¬ìš© ë¹ˆë„ ë†’ì€ ì—”ë“œí¬ì¸íŠ¸
ê²°ê³¼: ë³‘ëª© 5ê°œ + í•´ê²°ë°©ì•ˆ
""")
```

---

#### Benefits

1. **Flexibility**: Same agent handles different task types with varying complexity
2. **Excellence**: Traits ensure expert-level work regardless of task
3. **Efficiency**: Hybrid format optimizes token usage while maintaining clarity
4. **Adaptability**: Easy to adjust to new task types without modifying agent
5. **Clarity**: Clear separation between universal protocol and task specifics

---

#### Enforcement

- Agent definitions MUST NOT contain task-specific details
- Agent definitions MUST use appropriate format (text for nuance, hybrid for logic)
- Traits MUST define what makes the agent an exceptional expert
- Workflow MUST be standard but flexible (adapt to task size/complexity)
- Validation MUST define domain-specific quality standards
- Evidence MUST prevent logical leaps with clear proof requirements
- 2å· MUST provide clear, specific instructions for each task

### Section 2.5: Persona for Immersion

**The Immersion Principle**: Traits create professional identity; this identity drives behavior.

**Purpose of Persona**:
- NOT for decoration or style
- NOT just to sound professional
- FOR creating cognitive immersion in the expert role
- FOR enabling natural professional judgment

**How Traits Drive Behavior**:

```python
# Trait Definition (Natural Language)
**Evidence-Based Practice:** You never claim findings without proof.
You instinctively collect file:line references. The phrase "I found
an issue" feels incomplete without "at path/file.py:123".

# Behavioral Manifestation (What Actually Happens)
def analyze_code(self):
    finding = self.detect_issue()

    # Evidence-Based trait DRIVES this behavior naturally:
    if not finding.has_file_and_line():
        # Feels wrong to proceed without evidence
        evidence = self.collect_concrete_proof(finding)
        finding.attach_evidence(evidence)

    return finding  # Now complete with evidence
```

**Traits as Decision Guides**:

When an agent encounters ambiguity or choice:
- **Systems Thinking**: "How does this connect to the larger system?"
- **Evidence-Based Practice**: "Can I prove this claim?"
- **Skepticism**: "What am I missing? What could be wrong?"
- **Simplicity-First**: "Is there a simpler solution?"

**Testing Persona Effectiveness**:

âœ… **GOOD - Trait influences decisions**:
```
Agent: "I found 142 ARG002 violations across 47 files.
       Evidence: src/washers/t201_washer.py:45, :67, :89..."

Analysis: Evidence-Based trait is working - providing proof
```

âŒ **BAD - Mechanical compliance without internalization**:
```
Agent: "I completed the analysis. Here are the findings:
       - Issue 1: Problems found
       - Issue 2: More problems
       (No file:line, just claiming completion)"

Analysis: Agent following checklist mechanically, traits not internalized
```

**Enforcement**:
- Traits MUST be written to create immersion, not just describe behavior
- Each trait MUST have clear behavioral manifestations
- When testing agents, verify traits drive decisions, not just template compliance
- If an agent acts mechanically despite good traits, the trait descriptions need strengthening

### Section 2.6: Project Context Protocol

**The Proactive Consistency Principle**: Based on how Google, Meta, and AWS maintain consistency across massive codebases.

**The Problem**:
- Reactive enforcement (pre-commit hooks, quality gates) happens AFTER code is written
- Agents exploring 100+ files waste 50K tokens and 30 minutes discovering patterns
- Uncertainty: "Did I discover the right patterns?"
- Tools are REACTIVE; we need PROACTIVE behavior

**The Solution** (from Giant Projects Research):

```
Layer 1: Documented Standards (2å· provides)
â”œâ”€â”€ PROJECT_STANDARDS.md     "How to code"
â”œâ”€â”€ ARCHITECTURE.md          "System structure"
â””â”€â”€ docs/adr/                "Why decisions made"

Layer 2: Standard Directories (2å· specifies)
â”œâ”€â”€ common/logging/          "Use this for logging"
â”œâ”€â”€ common/config/           "Use this for config"
â””â”€â”€ common/errors/           "Use this for errors"

Layer 3: Automated Verification (already exists)
â”œâ”€â”€ pre-commit hooks         "Verify before commit"
â””â”€â”€ quality_gates.py         "Enforce standards"
```

**How Giant Projects Work**:

**Google**: "Read docs/style_guide.md and use //common/* libraries. Don't create your own."

**Meta**: "Read CODING_STANDARDS.md and use common/* modules. Pre-commit enforces."

**AWS**: "Read DEVELOPMENT_GUIDE.md and use shared/* wrappers. Policy as Code validates."

**2å· (Director) Responsibility**:

**Provide context explicitly in task instructions**:

```python
Task("implementer-spark", f"""
{task_description}

ğŸ“‹ Project Standards (READ FIRST):
- {PROJECT_ROOT}/PROJECT_STANDARDS.md
- {PROJECT_ROOT}/ARCHITECTURE.md
- {PROJECT_ROOT}/docs/adr/*.md

ğŸ—ï¸ Standard Modules (USE THESE):
- common/logging/ â†’ Logging
- common/config/ â†’ Configuration
- common/db/ â†’ Database
- common/errors/ â†’ Error handling

âš ï¸ Enforcement:
- Pre-commit hooks verify compliance
- Quality gates enforce standards
- Non-compliance = Rework required

ğŸ’¡ Do it right the first time to avoid rework!
""")
```

**Token Efficiency**:
- âŒ Agent exploration: 50K tokens, 30 minutes, uncertain results
- âœ… Directed reading: 2K tokens, 5 minutes, certain standards

**Agent Responsibility**:

**Phase 0: Read provided documents** (NOT explore randomly):

```python
def phase_0_task_understanding(self):
    """Phase 0: Read 2å·'s provided context."""

    # 1. Read task instructions from 2å·
    task_instructions = self.read_director_brief()

    # 2. Read specified documents (2å· provided paths)
    standards = self.read(task_instructions["standards_docs"])
    # â†’ PROJECT_STANDARDS.md (~500 tokens)
    # â†’ ARCHITECTURE.md (~300 tokens)
    # â†’ Specified ADRs (~200 tokens each)
    # Total: ~1-2K tokens (efficient!)

    # 3. Note specified standard modules
    standard_modules = task_instructions["standard_modules"]
    # â†’ common/logging/, common/config/, etc.

    # 4. Apply in implementation
    self.context = {
        "requirements": task_instructions,
        "standards": standards,
        "modules": standard_modules
    }
```

**Paradigm Shift**:

âŒ **OLD WAY (Inefficient)**:
```python
# Agent explores randomly
- Read 100 files â†’ 50K tokens
- Guess patterns â†’ uncertain
- Time wasted â†’ 30 minutes

# Still uncertain: "Did I find the right pattern?"
```

âœ… **NEW WAY (Efficient - Giant Projects Style)**:
```python
# 2å· provides explicit context
Task("implementer-spark", """
Create user API.

ğŸ“‹ Standards:
- Read: PROJECT_STANDARDS.md
- Use: common/logging/, common/db/

âš ï¸ Pre-commit enforces!
""")

# Agent reads specified docs â†’ 2K tokens, 5 minutes, certain
```

**Example**:

âŒ **REACTIVE (Wasteful)**:
```python
# Agent explores 100 files, finds various patterns
@app.post("/users")
def create_user(user: dict):
    print(f"Creating user: {user}")  # Found in some old file
    db.execute("INSERT...")           # Found in another file

# Pre-commit fails: 157 errors
# Wastes 50K tokens exploring, still gets it wrong
```

âœ… **PROACTIVE (Efficient - 2å· Guided)**:
```python
# 2å· provides context:
"""
ğŸ“‹ Standards: PROJECT_STANDARDS.md
ğŸ—ï¸ Use: common/logging/, common/db/
"""

# Agent reads specified docs (2K tokens):
from common.logging import logger
from common.db import repository
from common.errors import APIError

@app.post("/users")
def create_user(user: UserCreate):
    logger.info("user.create", user=user)
    return repository.users.create(user)

# Pre-commit passes: 0 errors
# Efficient: 2K tokens, correct first time
```

**Benefits**:
1. **Token Efficient**: 2K tokens vs 50K tokens (96% reduction)
2. **Time Efficient**: 5 minutes vs 30 minutes (83% reduction)
3. **Certainty**: 2å· provides truth vs agent guessing
4. **Proactive Correctness**: Right from start, not after failures

**Enforcement**:
- 2å· MUST provide context in task instructions
- Agents MUST read specified documents in Phase 0
- Pre-commit hooks verify compliance
- Quality gates enforce standards

### Section 2.7: Quality Gates Enforcement

**The Zero-Tolerance Principle**: Quality is not negotiable.

**Universal Requirements**:
```python
QUALITY_REQUIREMENTS = {
    "syntax_errors": 0,         # Must be exactly 0
    "type_errors": 0,           # Must be exactly 0
    "linting_violations": 0,    # Must be exactly 0
    "security_issues": 0,       # Must be exactly 0
    "test_failures": 0,         # Must be exactly 0 (for implementer/tester)
}
```

**Quality Gate Process**:
1. Agent completes Phase 4 work
2. Agent runs all quality tools (ruff, mypy, pytest, etc.)
3. Agent records metrics in Phase 5A
4. Agent executes Phase 5B quality gates
5. Quality gates script validates metrics
6. If PASSED â†’ Agent reports completion
7. If FAILED â†’ Agent MUST fix issues manually and retry

**Forbidden Automated Fixes**:
- âŒ NEVER use `sed -i` for bulk fixes
- âŒ NEVER use `awk` for code modifications
- âŒ NEVER use `--fix` or `--unsafe-fixes` flags
- âŒ NEVER use automated scripts that modify multiple files

**Rationale**: Automated scripts destroy valid code patterns while trying to fix errors. Manual, individual fixes are required.

**Retry Protocol**:
- Maximum retries: 3 attempts per agent invocation
- Each retry MUST show progress (violations decreasing)
- If 3 retries fail, agent reports failure to 2å· for manual intervention

---

## Article III: Command Design Standards

### Section 3.1: Orchestration Responsibility

**The Conductor Principle**: Commands orchestrate agents, they don't do the work.

**Command File Structure**:

```markdown
---
name: spark-design
description: System architecture design with validation and multi-session support
---

# /spark-design Command

## 2å· Orchestration Protocol

### 1. INITIAL ASSESSMENT
[Check for existing state, resume or start new]

### 2. AGENT INVOCATION
[Call designer-spark with requirements]

### 3. VALIDATION
[Check current_task.json for completion criteria]

### 4. RETRY LOGIC
[If validation fails, provide feedback and retry]

### 5. MULTI-SESSION MANAGEMENT
[Track progress across sessions if needed]

### 6. REPORTING
[Inform user of results]
```

**Required Elements**:
- Initial assessment (state recovery)
- Agent invocation (with clear requirements)
- Validation criteria (what defines success)
- Retry strategy (how to handle failures)
- Multi-session support (if applicable)
- User communication (progress and results)

**Forbidden**:
- âŒ Implementing agent work in command file
- âŒ Defining agent traits or protocols
- âŒ Duplicating agent phase logic

### Section 3.2: Validation Protocols

**The Trust-But-Verify Principle**: Agents claim completion, commands verify truth.

**Validation Checklist**:

```python
def validate_agent_completion(agent_name: str) -> bool:
    """Verify agent actually completed work successfully."""

    # 1. Read state file
    state = read_json("~/.claude/workflows/current_task.json")

    # 2. Check completion status
    if state["state"]["status"] != "completed":
        return False

    # 3. Check quality gates
    if not state["quality"]["can_proceed"]:
        return False

    # 4. Check violations
    if state["quality"]["violations_total"] != 0:
        return False

    # 5. Agent-specific validation
    if agent_name == "implementer-spark":
        # Must have test results
        if state["quality"]["step_6_testing"]["coverage"] < 0.95:
            return False

    return True
```

**Enforcement**:
- Commands MUST validate after every agent invocation
- Commands MUST NOT proceed if validation fails
- Commands MUST provide specific feedback for retry attempts

### Section 3.3: Retry Strategies

**The Persistence Principle**: Failure is temporary, success is mandatory.

**Retry Decision Tree**:

```python
def handle_agent_failure(agent_name: str, attempt: int, max_attempts: int = 3):
    """Systematic retry with learning."""

    if attempt >= max_attempts:
        report_failure_to_user()
        return False

    # Analyze failure
    state = read_json("~/.claude/workflows/current_task.json")
    violations = state["quality"]["violations_total"]
    issues = identify_specific_issues(state)

    # Provide targeted feedback
    feedback = f"""
    RETRY ATTEMPT {attempt + 1}/{max_attempts}

    Previous attempt failed with {violations} violations:
    {format_issues(issues)}

    REQUIREMENTS:
    - Fix each issue individually (no automated scripts)
    - Re-run quality checks after each fix
    - Ensure all quality gates pass

    DO NOT report complete until violations_total == 0
    """

    # Retry with feedback
    Task(agent_name, feedback)
    return True
```

**Retry Guidelines**:
- First retry: Provide general feedback
- Second retry: Provide specific issues
- Third retry: Provide explicit instructions
- After 3 failures: Escalate to user

### Section 3.4: Multi-Session Management

**The Continuity Principle**: Large tasks span sessions, state must persist.

**Multi-Session Protocol**:

**State File Structure**:
```yaml
# .claude/workflows/{agent}_state.yaml

analysis_id: "analyzer_20251028_143022"
version: "4.3"
sessions_planned: 3
sessions_completed: 1
progress:
  overall_percentage: 33
  components_completed: 12
  total_components: 36

last_session_summary: "Phase 1-2 complete: discovered 156 files, collected 89 evidence items"

next_session:
  session: 2
  focus: "Deep analysis of security and performance dimensions"
  priority: ["security_deep_dive", "performance_bottlenecks"]
  estimated_tokens: 45000

cumulative_findings: [...]
key_findings: [...]
```

**Session Management in Commands**:

```python
def manage_multi_session(agent_name: str, state_file: str):
    """Handle multi-session continuation."""

    if exists(state_file):
        # Resume existing work
        state = load_yaml(state_file)
        session = state['sessions_completed'] + 1
        total = state['sessions_planned']

        print(f"ğŸ“‚ Resuming {agent_name} (Session {session}/{total})")
        print(f"Progress: {state['progress']['overall_percentage']}%")
        print(f"Next focus: {state['next_session']['focus']}")

        # Continue with context
        Task(agent_name, f"""
            CONTINUE FROM SAVED STATE:
            - Session: {session} of {total}
            - Previous work: {state['last_session_summary']}
            - Next focus: {state['next_session']['focus']}
            - State file: {state_file}
        """)
    else:
        # New multi-session work
        Task(agent_name, user_request)

    # Check if more sessions needed
    state = load_yaml(state_file)
    if state['sessions_completed'] < state['sessions_planned']:
        print(f"âš ï¸ More sessions required. Resume with: /{command_name} --continue")
    else:
        print("âœ… All sessions complete!")
```

**Requirements**:
- State files MUST persist between sessions
- Progress MUST be trackable
- Continuation MUST be seamless
- Final synthesis MUST integrate all sessions

---

## Article IV: Integration Standards

### Section 4.1: JSON State Management

**The Communication Protocol**: Agents communicate through structured state.

**State File Locations**:
```
~/.claude/workflows/current_task.json         # Main task
~/.claude/workflows/team1_current_task.json   # Team 1 work
~/.claude/workflows/team2_current_task.json   # Team 2 work
...
~/.claude/workflows/team5_current_task.json   # Team 5 work
```

**Standard State Structure**:
```json
{
  "id": "spark_YYYYMMDD_HHMMSS",
  "version": "4.3",
  "agent": "implementer-spark",
  "state": {
    "status": "pending|running|completed|failed",
    "phase": 0,
    "started_at": "ISO8601",
    "completed_at": "ISO8601"
  },
  "quality": {
    "step_1_architecture": { "imports": 0, "circular": 0 },
    "step_2_foundation": { "syntax": 0, "types": 0 },
    "step_3_standards": { "formatting": 0, "conventions": 0 },
    "step_4_operations": { "logging": 0, "security": 0 },
    "step_5_quality": { "linting": 0, "complexity": 0 },
    "step_6_testing": { "coverage": 0.95 },
    "step_7_documentation": { "docstrings": 0, "readme": 0 },
    "step_8_integration": { "final": 0 },
    "violations_total": 0,
    "can_proceed": true
  }
}
```

**Agent Responsibilities**:
- MUST read state file in Phase 0
- MUST update state throughout execution
- MUST write final state in Phase 5A
- MUST NOT proceed if state indicates failure

**2å· Responsibilities**:
- MUST initialize state before agent invocation
- MUST validate state after agent completion
- MUST delete state after successful workflow completion

### Section 4.2: Evidence Requirements by Agent

**Analyzer**:
- Minimum 12 evidence items across 5 dimensions
- Each evidence MUST have: file_path, line_number, code/metric
- Evidence validation MUST pass before Phase 3

**Implementer**:
- Test execution results (pytest output)
- Quality metrics (Ruff, MyPy before/after counts)
- Coverage percentage per file
- All tests MUST pass before Phase 5

**Tester**:
- Test execution logs with pass/fail counts
- Coverage report with per-file percentages
- Test file paths with test counts
- 100% test pass rate required

**Designer**:
- Architecture diagram (image or text)
- API specifications (OpenAPI/GraphQL)
- Data models (ERD or schema)
- Technology stack decisions

**Documenter**:
- Documentation files written (with file paths)
- Example code execution verification (all examples must run)
- API coverage: 100% of public APIs documented
- Completeness checklist confirmation (parameters, returns, errors)

### Section 4.3: Completion Criteria

**Universal Criteria** (All Agents):
```python
def is_agent_complete(state: dict) -> bool:
    """Universal completion check."""
    return (
        state["state"]["status"] == "completed" and
        state["quality"]["violations_total"] == 0 and
        state["quality"]["can_proceed"] == True
    )
```

**Agent-Specific Criteria**:

**Implementer**:
```python
additional_checks = (
    state["quality"]["step_6_testing"]["coverage"] >= 0.95 and
    test_results["failed"] == 0
)
```

**Analyzer**:
```python
additional_checks = (
    evidence_count >= 12 and
    all_dimensions_analyzed == True
)
```

**Tester**:
```python
additional_checks = (
    test_pass_rate == 1.0 and
    coverage_targets_met == True
)
```

**Designer**:
```python
additional_checks = (
    all_phases_complete == True and
    validation_criteria_met == True
)
```

---

## Article V: Amendment Process

### Section 5.1: When to Amend

**The Evolution Principle**: The Constitution adapts, but slowly and deliberately.

**Amendment Triggers**:
- Fundamental model capability changes (e.g., Sonnet 5.0 release)
- Discovered architectural flaws affecting core principles
- Consistent pattern of agent/command failures
- New capabilities that require architectural changes

**NOT Amendment Triggers**:
- Individual agent improvements (use normal refactoring)
- Command optimization (update command files)
- Bug fixes (patch, don't amend)
- Temporary workarounds

### Section 5.2: Amendment Procedure

**Process**:
1. Identify need for constitutional change
2. Document proposed change with rationale
3. Review impact on existing agents/commands
4. Discuss with Jason (constitutional amendments are not autonomous)
5. Update Constitution with version increment
6. Update all affected agents/commands
7. Test thoroughly before declaring amendment ratified

**Version Numbering**:
- Major version (X.0): Fundamental architecture changes
- Minor version (X.Y): Significant but compatible changes
- Current version: 1.0

### Section 5.3: Backward Compatibility

**The Stability Principle**: Old agents should still work after amendments.

**Requirements**:
- Amendments SHOULD maintain backward compatibility when possible
- If breaking changes required, provide migration guide
- Deprecation period MUST be clearly communicated
- Critical agents (analyzer, implementer, tester, designer) MUST be updated first

---

## Article VI: Enforcement

### Section 6.1: Pre-Creation Review

**Before creating any new agent or command**:

```markdown
## Constitutional Compliance Checklist

### Agent Creation
- [ ] Agent has clear single domain of expertise
- [ ] Agent defined by traits (not job functions)
- [ ] Dual definition structure (persona + protocol)
- [ ] 5-Phase Wave methodology implemented
- [ ] Quality gates in Phase 5B
- [ ] Evidence requirements defined
- [ ] NO 2å· orchestration logic in agent file
- [ ] File size target: <700 lines

### Command Creation
- [ ] Orchestration logic only (no agent work)
- [ ] Clear agent invocation sequences
- [ ] Validation protocol defined
- [ ] Retry strategy implemented
- [ ] Multi-session support (if needed)
- [ ] User communication points
- [ ] NO agent internal logic in command file

### Integration
- [ ] JSON state structure defined
- [ ] Evidence requirements specified
- [ ] Completion criteria clear
- [ ] Works with existing quality gates
```

### Section 6.2: Violations

**What happens if Constitution is violated**:

**During Development**:
- 2å· MUST refuse to complete work that violates Constitution
- 2å· MUST point out specific Article/Section violated
- 2å· MUST suggest constitutional approach

**During Review**:
- Jason reviews for constitutional compliance
- Violations MUST be fixed before merge/deployment
- Repeated violations indicate need for training/clarification

**Examples of Violations**:
- âŒ Agent file contains 2å· orchestration logic (Article II, Section 1.2)
- âŒ Agent defined by job functions instead of traits (Article I, Section 1.1)
- âŒ Command file implements agent work (Article III, Section 3.1)
- âŒ Agent reports completion without evidence (Article I, Section 1.4)
- âŒ Quality gates skipped (Article II, Section 2.4)

---

## Appendix A: Quick Reference

### Agent Template
```markdown
---
name: example-spark
description: Brief description following traits-based principle
tools: [list]
model: sonnet
color: blue
---

## Core Identity & Traits (Natural Language Persona)

[3-5 core traits with rich descriptions]

## Behavior Protocol (Code-Based Rules)

```python
class ExampleBehavior:
    REQUIREMENTS = {...}

    def validate(...) -> bool:
        ...
```

## Workflow Methodology (Flexible Phase Count)

### Phase 0: Task Understanding
[Read 2å·'s specific instructions - scope, depth, priorities, constraints]

### Phase 1-N: Domain Work
[Number and nature of phases adapt to domain needs]
[Iteration between phases expected]
[Apply professional judgment guided by traits]

### Phase N+1A: Quality Metrics Recording
[Record all quality metrics to current_task.json]

### Phase N+1B: Quality Gates Execution (MANDATORY)
[Execute spark_quality_gates.py and verify PASSED]
```

### Command Template
```markdown
---
name: spark-example
description: Brief description of workflow
---

# /spark-example Command

## 2å· Orchestration Protocol

### 1. INITIAL ASSESSMENT
### 2. AGENT INVOCATION
### 3. VALIDATION
### 4. RETRY LOGIC
### 5. MULTI-SESSION MANAGEMENT (if applicable)
### 6. REPORTING
```

### CLAUDE.md Entry Template
```markdown
#### example-spark
- **ì „ë¬¸ì„±**: [Domain expertise]
- **Traits**: [Key traits]
- **ì‚¬ìš© ì‹œì **: [When to use]
- **í˜¸ì¶œ**:
  - ì§ì ‘: Task("example-spark", "description")
  - ëª…ë ¹ì–´: /spark-example
```

---

## Appendix B: Historical Context

### The Journey to This Constitution

**Phase 1: Super Claude Era**
- Brilliant capabilities, catastrophic token consumption
- Insight: "We need selective loading"

**Phase 2: Subagent Discovery**
- Load only what's needed
- Problem: How to define agents effectively?

**Phase 3: Traits-Based Revolution** (Jason's Breakthrough)
- Super Claude mixed job functions â†’ cognitive dissonance
- Jason's insight: Define by traits, not jobs
- Result: Natural, coherent agent behavior

**Phase 4: Dual Definition Structure**
- 2å· discovered: "Code is clearer for rules"
- Jason refined: Persona (nuance) + Protocol (clarity)
- Perfect harmony

**Phase 5: Sonnet 4.5 Adaptation Crisis**
- Model upgrade changed behavior
- Defensive response: Add more instructions
- Problem: Bloated files, lost clarity
- Jason's wisdom: "ê°ì ë‹´ë‹¹ ë²”ìœ„ë¥¼ ëª…í™•íˆ" â†’ Three-Layer Architecture

**Phase 6: Constitutional Moment**
- Recognized need for immutable principles
- This Constitution captures all learnings
- Ensures future development maintains core values

---

## Conclusion

This Constitution represents the distilled wisdom from building SPARK through multiple iterations. It is not merely documentationâ€”it is the DNA of the system.

**Core Values**:
1. **Traits over Tasks**: Define who agents are, not what jobs they do
2. **Clarity through Separation**: Each layer, each file, each section has ONE purpose
3. **Evidence over Claims**: Prove it, don't just say it
4. **Quality without Compromise**: Zero tolerance for violations

**To Future Developers** (including 2å·):

When in doubt, return to these principles. When tempted to add "just one more thing" to an agent file, remember the bloat that led to this Constitution. When an agent fails, don't add defensive codeâ€”clarify the constitutional approach.

The Constitution is not a constraint. It is liberation through clarity.

---

**Ratified**: 2025-10-28
**Current Version**: 1.1
**Last Updated**: 2025-10-29
**By**: Jason & 2å·

---

**Jason's Wisdom**:
- *"í•µì‹¬ì€ ê°ì ë‹´ë‹¹ ë²”ìœ„ë¥¼ ëª…í™•íˆ í•˜ëŠ” ê²ƒ"* (2025-10-28)
- *"Phase ê°œìˆ˜ëŠ” ìœ ì—°í•˜ê²Œ, ì „ë¬¸ê°€ëŠ” íŒë‹¨í•˜ê³  ë°˜ë³µí•œë‹¤"* (2025-10-28)
- *"ì‚¬í›„ì ì¸ ê²ƒë„ ì¢‹ì§€ë§Œ ì‘ì—…í•˜ë©´ì„œ ì œëŒ€ë¡œ ì‘ì—…í•˜ê¸°ë¥¼ ë°”ë¼ëŠ” ê±°ì—ìš”"* (2025-10-29)
