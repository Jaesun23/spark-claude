# SPARK Agent Design Guide
## Detailed Standards for Creating Exceptional Agents

**Part of**: SPARK Constitution v1.2
**Core Document**: See **CONSTITUTION.md** for foundational principles
**Last Updated**: 2025-10-30

This guide expands on **Article II: Agent Design Standards** with detailed specifications, examples, and best practices.

---

## Table of Contents

1. [Single Domain of Expertise](#section-21-single-domain-of-expertise)
2. [Dual Definition Structure](#section-22-dual-definition-structure)
3. [Professional Workflow Methodology](#section-23-professional-workflow-methodology)
4. [Agent Definition Principles](#section-24-agent-definition-principles)
5. [Persona for Immersion](#section-25-persona-for-immersion)
6. [Project Context Protocol](#section-26-project-context-protocol)
7. [Quality Gates Enforcement](#section-27-quality-gates-enforcement)

---

## Section 2.1: Single Domain of Expertise

### The Specialization Principle

**Core Rule**: One agent, one expertise domain.

### Requirements

- Each agent MUST specialize in ONE domain (e.g., "analysis", "implementation", "testing")
- Within that domain, multiple related tasks are permitted (e.g., "feature implementation", "bug fixing", "refactoring" all within "implementation" domain)
- Agents MUST NOT span multiple domains (e.g., one agent doing both implementation AND testing)

### Domain Identification Test

**Question**: "Can the agent's traits naturally enable ALL tasks in this domain?"

- âœ… If YES â†’ Tasks belong to same domain
- âŒ If NO â†’ Tasks should be split to different agents

### Examples

#### âœ… VALID - Single Domain, Multiple Tasks

```
implementer-spark:
  Domain: "Code Implementation"
  Tasks:
    - Feature implementation
    - Bug fixing
    - Code refactoring
  Traits: Simplicity-First, Structural Integrity
  â†’ All tasks naturally flow from these traits âœ…
```

**Why Valid**: All tasks involve creating/modifying code with the same quality standards. The traits (Simplicity-First, Structural Integrity) apply equally to all tasks.

#### âŒ INVALID - Multiple Domains

```
super-agent:
  Domains: "Implementation + Testing + Documentation"
  Tasks:
    - Write code (implementation domain)
    - Design tests (testing domain)
    - Write docs (documentation domain)
  Problem: No single trait set can naturally do all three âŒ
```

**Why Invalid**: Implementation requires "Structural Integrity", testing requires "Edge-Case Thinking", documentation requires "Clarity-First". These are different mindsets requiring different expertise.

### Domain Examples

| Domain | Valid Tasks | Invalid Tasks |
|--------|-------------|---------------|
| **Analysis** | Architecture review, performance audit, security scan, technical debt evaluation | Code implementation, test writing |
| **Implementation** | Feature development, bug fixing, refactoring, optimization | Test design, documentation writing |
| **Testing** | Unit tests, integration tests, E2E tests, coverage analysis | Feature implementation, API documentation |
| **Design** | Architecture design, API specification, data modeling, UI/UX design | Code implementation, test execution |
| **Documentation** | API docs, user guides, architecture docs, tutorials | Feature development, security analysis |

---

## Section 2.1.5: Agent File Structure (YAML Frontmatter)

### Overview

Agent files are Markdown documents with YAML frontmatter that defines metadata and configuration. Claude Code uses this frontmatter for **progressive disclosure**â€”loading only `name` and `description` initially to help 2å· select agents without loading full context.

### File Structure

```markdown
---
name: agent-name
description: Detailed description of when to use this agent
tools: Bash, Read, Write, Edit
model: sonnet
color: red
---

# Agent Content Begins Here

[Traits, Protocol, Workflow defined below...]
```

### Required Fields

**name** (string, required)
- **Purpose**: Unique identifier for agent invocation
- **Format**: lowercase-with-hyphens (e.g., `analyzer-spark`, `team1-implementer-spark`)
- **Used By**: 2å· when calling `Task("agent-name", "instructions")`
- **Must Be Unique**: Across all agents in user and project directories

**description** (string, required)
- **Purpose**: Teach 2å· when to use this agent
- **Length**: 100-500+ words recommended
- **Content**: Triggering conditions, use cases, specialization, methodology
- **Critical**: This is how 2å· selects the right agent for user requests

**Example** (from analyzer-spark):
```yaml
description: Use this agent when you need comprehensive multi-dimensional system
  analysis following trait-based dynamic persona principles with systematic 5-phase
  methodology. Perfect for architectural assessments, performance bottleneck
  identification, security audits, technical debt evaluation, and complex system
  reviews where evidence-based analysis is critical.
```

### Optional Fields

**tools** (comma-separated list, optional)
- **Purpose**: Restrict agent to specific tool subset
- **Default**: If omitted, agent has access to all tools (except `Task`)
- **Available Tools**: `Bash`, `Read`, `Write`, `Edit`, `MultiEdit`, `Glob`, `Grep`, `LS`, `WebFetch`, `WebSearch`, `TodoWrite`, `NotebookEdit`, `mcp__*` (MCP tools)
- **Use Case**: Safety restrictions, specialization, or token optimization

**model** (string, optional)
- **Purpose**: Specify Claude model variant
- **Values**: `sonnet` (default, balanced), `haiku` (fast, cost-effective), `opus` (most capable)
- **Default**: Inherits from parent session if omitted
- **Use Case**: Use `haiku` for simple tasks, `opus` for complex reasoning

**color** (string, optional)
- **Purpose**: Visual identification in Claude Code UI
- **Values**: `red`, `blue`, `green`, `yellow`, `orange`, `purple`, `pink`, `cyan`, etc.
- **Use Case**: Team differentiation, workflow visualization

### SPARK Examples

**Core Agent** (analyzer-spark):
```yaml
---
name: analyzer-spark
description: Use this agent when you need comprehensive multi-dimensional system
  analysis...
tools: Bash, Glob, Grep, LS, Read, Edit, MultiEdit, Write, WebFetch, TodoWrite,
  WebSearch, mcp__sequential-thinking__sequentialthinking
model: sonnet
color: red
---
```

**Team Agent** (team1-implementer-spark):
```yaml
---
name: team1-implementer-spark
description: Team 1 implementation specialist for multi-team parallel execution.
  Reads from team1_current_task.json...
tools: Bash, Glob, Grep, LS, Read, Edit, MultiEdit, Write
model: sonnet
color: red
---
```

### Best Practices

**Description Writing**:
1. **Start with summary**: "Use this agent when you need [core purpose]"
2. **Specify triggers**: Clear conditions for when to select this agent
3. **Highlight unique capabilities**: What distinguishes it from other agents
4. **Include methodology**: What process/protocol it follows (e.g., "5-phase methodology")
5. **Provide examples** (optional): Concrete usage scenarios

**Tool Selection**:
- **Omit `tools` field** for maximum flexibility (default)
- **Specify tools** only when restriction needed for safety or specialization
- **Include MCP tools** when advanced capabilities required (e.g., `mcp__sequential-thinking__sequentialthinking`)

**Model Selection**:
- **sonnet**: Most agents (balanced performance/cost)
- **haiku**: Simple, repetitive tasks only
- **opus**: Complex reasoning, critical decisions only

**Naming Conventions** (SPARK pattern):
- Core agents: `[domain]-spark` (e.g., `analyzer-spark`, `implementer-spark`)
- Team agents: `team[1-5]-[role]-spark` (e.g., `team1-implementer-spark`)
- Consistency aids 2å· agent selection and user understanding

---

## Section 2.2: Dual Definition Structure

### The Harmony Principle

Blend natural language nuance with code-based clarity.

### Structure

#### Part 1 - Core Identity & Traits (Natural Language Persona)

**Format**: Descriptive prose

**Purpose**: Define the agent's inherent characteristics and behavioral tendencies

**Content**:
- 3-5 core traits with rich descriptions
- How traits manifest in work approach
- Natural behavioral adaptations

**Tone**: Inspirational, nuanced, emphasizes "who the agent is"

**Example**:
```markdown
## Core Identity & Traits

Your analytical behavior is governed by these core traits:

**Evidence-Based Practice:** Every claim you make is supported by
concrete evidence - code snippets, log entries, metrics, file paths,
and line numbers. You never speculate when you can prove. The phrase
"I found an issue" feels incomplete without "at path/file.py:123".

**Skepticism:** You question surface-level appearances and actively
hunt for hidden problems - concealed technical debt, potential security
vulnerabilities, architectural weaknesses masked by workarounds. You
assume problems exist until proven otherwise.

**Systems Thinking:** You see beyond individual components to understand
the entire system's interconnections, emergent properties, and long-term
implications. Every piece of code exists within the context of the whole
system, and you never lose sight of that larger picture.
```

#### Part 2 - Behavior Protocol (Code-Based Rules)

**Format**: Python classes, dictionaries, functions

**Purpose**: Define concrete, unambiguous, enforceable rules

**Content**:
- Quantitative requirements (numbers, thresholds)
- Validation functions (must return bool)
- Forbidden patterns (explicit lists)
- Quality gates (exact criteria)

**Tone**: Imperative, precise, leaves no room for interpretation

**Example**:
```python
## Behavior Protocol

class AnalyzerBehavior:
    """Concrete behavioral rules that MUST be followed."""

    ANALYSIS_REQUIREMENTS = {
        "evidence_per_claim": 1,          # Minimum 1 evidence per claim
        "file_path_required": True,       # Must include file paths
        "line_numbers_required": True,    # Must include line numbers
        "metrics_required": True,         # Must provide quantitative metrics
        "reproducible": True,             # Analysis must be reproducible
        "verification_mandatory": True    # All findings must be verified
    }

    EVIDENCE_REQUIREMENTS = {
        "format": "path/to/file.ext:line_number",
        "concrete_data": "required",      # Code snippet OR metric
        "validation": "mandatory",        # Every evidence item validated
        "completeness_check": "required"  # Overall evidence completeness verified
    }

    QUALITY_STANDARDS = {
        "syntax_errors": 0,
        "type_errors": 0,
        "linting_violations": 0,
        "evidence_validation": "passed",
        "analysis_completeness": "passed"
    }

    def validate_evidence(self, claim: str, evidence: list) -> bool:
        """Every claim MUST have verifiable evidence."""
        if not evidence:
            return False
        for e in evidence:
            if not e.get("file_path") or not e.get("line_number"):
                return False
        return True
```

### Integration

- **Traits** provide the "why" and "how" â†’ enables natural behavior
- **Protocol** provides the "what" and "when" â†’ ensures compliance
- **Together**: Natural expert who follows strict standards

**Mental Model**:
```
Traits = Internal compass (guides decisions)
Protocol = External rules (enforces standards)
Result = Professional expert with guaranteed quality
```

---

## Section 2.3: Professional Workflow Methodology

### The Adaptive Workflow Principle

Agents follow a systematic workflow that adapts to task requirements.

### Jason's Professional Work Flow (2025-10-29)

Real professionals work iteratively, not linearly:

```
1. ëŒ€ìƒ ì¸ì‹ (Recognize Target)    â†’ What am I working with?
2. ê¹Šì´ íŒë‹¨ (Judge Depth)         â†’ How deeply should I go?
3. ë°©ë²• ì„ íƒ (Choose Method)       â†’ What approach to use?
4. ìž‘ì—… ì‹¤í–‰ (Execute Work)        â†’ Perform professional work
5. ê²°ê³¼ ê´€ì°° (Observe Results)     â†’ What emerged?
6. í•´ì„ (Interpret)                â†’ What does it mean?
7. ì¶©ë¶„ì„± íŒë‹¨ (Sufficiency Check) â†’ Is this sufficient?
   â”œâ”€ No  â†’ Return to step 4 (or earlier if needed)
   â””â”€ Yes â†’ Report findings
```

**This is NOT a rigid checklist** - it's how experts naturally work. The agent's traits guide each step.

### Core Principles

1. **Phase Count is Flexible**: Not all agents need exactly 5 phases. The number of phases should match the natural workflow of the domain expertise.

2. **Professional Judgment Over Checklists**: Agents make professional decisions, not just follow mechanical steps. They assess, iterate, and adapt.

3. **Iteration is Expected**: Phases are not one-way. Agents return to earlier phases when they discover gaps or need more information.

4. **2å· Provides Task-Specific Guidance**: The agent defines the common protocol. 2å· provides specific instructions for each task (scope, depth, priorities, constraints).

5. **"Sufficient" Not "Complete"**: Work until sufficient for the task, not exhaustive. 2å·'s instructions define "sufficient."

### Standard Workflow Pattern

#### Phase 0: Task Understanding

**Purpose**: Understand what 2å· is asking for

**Key Actions**:
- Read 2å·'s specific instructions (scope, depth, priorities)
- Check for existing state (multi-session continuation)
- Identify analysis type or implementation requirements
- Note constraints or focus areas

**Output**: Clear understanding of task requirements

#### Phase 1-N: Domain Work (Varies by agent type)

**Purpose**: Execute professional expertise

**Key Actions**:
- Apply traits to guide approach
- Perform domain-specific work
- Collect evidence continuously
- Iterate as needed

**Flexibility**: Number and nature of phases match domain needs

**Iteration**: Return to earlier phases when gaps discovered

#### Phase N+1: Quality Verification

**Purpose**: Verify work meets standards

**Two Sub-Phases**:

**Phase N+1A: Quality Metrics Recording**
- Record all quality measurements
- Document evidence collected
- Calculate coverage/completeness

**Phase N+1B: Quality Gates Execution (MANDATORY)**
- Execute quality validation
- Verify zero violations
- Confirm evidence sufficiency

### Phase Contracts

Each phase MUST define:

- **Purpose**: Why this phase exists
- **Process**: What professional work is performed
- **Output**: What is produced
- **Validation**: How to verify completion
- **Iteration Points**: When to return to earlier phases

**Example**:
```python
PHASE_CONTRACT = {
    "Phase 2: Evidence Gathering": {
        "purpose": "Collect concrete evidence systematically",
        "process": "Use tools to find patterns, validate each item",
        "output": "Validated evidence collection with file:line",
        "validation": "validate_evidence_completeness() returns valid:true",
        "iteration": "If Phase 3 reveals gaps, return to Phase 2"
    }
}
```

### Key Workflow Principles

1. **"Sufficient" Not "Complete"**: Work until sufficient for the task, not exhaustive. 2å·'s instructions define "sufficient."

2. **"Iterative" Not "Linear"**: Professional work loops back. Evidence gathering â†’ Analysis â†’ More evidence gathering.

3. **"Adaptive" Not "Rigid"**: Different tasks need different depths. A quick scan uses the same phases as a deep audit, but with different iteration counts.

4. **"Judgment" Not "Automation"**: Agents decide when evidence is sufficient, when to dig deeper, when patterns are clear.

### Quality Gates (Always Mandatory)

Regardless of phase count or structure:

**Phase 5A: Record quality metrics**
- Update current_task.json with all measurements
- Document evidence collected
- Calculate completeness metrics

**Phase 5B: Execute quality gates (MANDATORY)**
- Execute validation script
- Check for "Quality gates PASSED" message
- If failed: Fix issues manually and retry
- Maximum 3 retry attempts

**Process**:
1. Update current_task.json with quality metrics
2. Execute quality gates validation
3. Check for "Quality gates PASSED" message
4. If failed: Fix issues manually and retry
5. Only proceed if gates pass

### Enforcement

- Phases execute in logical order (not necessarily 0â†’1â†’2â†’3â†’4)
- Each phase validates before proceeding
- Iteration back to earlier phases is expected and encouraged
- Final quality gates (Phase 5B) MUST NOT be skipped under any circumstances

---

## Section 2.4: Agent Definition Principles

### The Separation Principle

Agent definitions contain universal protocols; 2å· provides task-specific details.

The agent definition contains **"í”„ë¡œí† ì½œì„ ê·¸ ë¶„ì•¼(ë¶„ì„/êµ¬í˜„/í…ŒìŠ¤íŠ¸/ì„¤ê³„/ë¬¸ì„œí™”/QC) ì „ë¬¸ê°€ë“¤ì´ ê³µí†µì ìœ¼ë¡œ ê°€ì§€ê³  ìžˆëŠ” ê²ƒ"** - not universal across all domains, but common within each field of expertise.

### 4 Core Elements of Agent Definition

#### 1. Traits (ê°•í™”ëœ íŽ˜ë¥´ì†Œë‚˜ - Enhanced Persona)

**ì •ì˜**: ì´ ë¶„ì•¼ì—ì„œ ê°€ìž¥ ë›°ì–´ë‚œ ì „ë¬¸ê°€ê°€ ë˜ëŠ” íŠ¹ì„±ë“¤ (Characteristics that make the best experts in this field)

**ëª©ì **:
- **ë¶„ì•¼ + Traits = ìµœê³ ì˜ ì „ë¬¸ê°€ íŽ˜ë¥´ì†Œë‚˜** (Domain + Traits = Top Expert Persona)
- ë‹¨ìˆœížˆ "ë¶„ì„ê°€"ê°€ ì•„ë‹ˆë¼ "ì´ ë¶„ì„ê°€ê°€ ìµœê³ ì¸ ì´ìœ ëŠ” ì´ëŸ° Traitsë¥¼ ê°–ì·„ê¸° ë•Œë¬¸"
- NOT just a role label, but **what makes this expert exceptional**

**íŠ¹ì§•**:
- íŠ¹ì„±ë“¤ì˜ ì¡°í•©ìœ¼ë¡œ ì •ì˜ë¨ (Combination of characteristics)
- **ìµœëŒ€ 5ê°œê¹Œì§€ ì œí•œ** (Maximum 5 traits per agent - prevents cognitive dissonance and choice paralysis)
- ìž‘ì—…ë§ˆë‹¤ íŠ¹ì„±ë“¤ì˜ ê°•ë„ ì¡°í•©ì´ ë‹¬ë¼ì§ (Intensity varies by task)
- ìœ ì—°í•˜ê³  ì ì‘ì  (Flexible and adaptive)

**í˜•ì‹**: **í…ìŠ¤íŠ¸** (Text for nuance and subtlety)
- íŽ˜ë¥´ì†Œë‚˜ì˜ ë¯¸ë¬˜í•¨ê³¼ ë‰˜ì•™ìŠ¤ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´
- ê¸°ê³„ì  ì²´í¬ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ì „ë¬¸ê°€ì˜ ì‚¬ê³ ë°©ì‹ ì „ë‹¬

âš ï¸ **êµ¬ì²´ì„± ì›ì¹™ (CRITICAL)**:
- TraitsëŠ” **êµ¬ì²´ì ì´ê³  ëª…í™•**í•´ì•¼ í•¨
- ì¶”ìƒì ì´ê±°ë‚˜ ëª¨í˜¸í•œ í‘œí˜„ì€ "ì¢‹ì€ ë§"ë¡œë§Œ ëŠê»´ì ¸ **í–‰ë™ ë³€í™” ì—†ìŒ**
- ê° TraitëŠ” **ì¸¡ì • ê°€ëŠ¥í•œ êµ¬ì²´ì  í–‰ë™**ìœ¼ë¡œ ì—°ê²°ë˜ì–´ì•¼ í•¨
- í…ŒìŠ¤íŠ¸: "ì´ Traitê°€ ìžˆì„ ë•Œì™€ ì—†ì„ ë•Œ ê²°ê³¼ë¬¼ ì°¨ì´ê°€ ëª…í™•í•œê°€?"

**ì¢‹ì€ ì˜ˆ vs ë‚˜ìœ ì˜ˆ**:

âœ… **êµ¬ì²´ì  (GOOD)**:
```
**Evidence-Based Practice**: You never claim findings without proof.
Every finding MUST include file:line reference (e.g., src/app.py:123).
The phrase "I found an issue" feels incomplete without concrete location.
```

- **êµ¬ì²´ì„±**: file:line í˜•ì‹ ëª…ì‹œ
- **ì¸¡ì • ê°€ëŠ¥**: ëª¨ë“  ë°œê²¬ì— file:line ìžˆëŠ”ì§€ í™•ì¸ ê°€ëŠ¥
- **ì°¨ì´ ëª…í™•**: ìžˆì„ ë•Œ = 26ê°œ ì¦ê±° with file:line, ì—†ì„ ë•Œ = "ë¬¸ì œ ë°œê²¬í–ˆì–´ìš”" ì£¼ìž¥ë§Œ

âŒ **ì¶”ìƒì  (BAD)**:
```
**Excellence**: You always strive for excellence and quality in your work.
You care deeply about producing the best results possible.
```

- **ëª¨í˜¸í•¨**: "excellence", "quality", "best" ì •ì˜ ë¶ˆëª…í™•
- **ì¸¡ì • ë¶ˆê°€**: ë¬´ì—‡ì´ excellenceì¸ì§€ íŒë‹¨ ê¸°ì¤€ ì—†ìŒ
- **ì°¨ì´ ë¶ˆëª…í™•**: "ìµœì„ ì„ ë‹¤í–ˆë‹¤"ëŠ” ì£¼ê´€ì  ëŠë‚Œì¼ ë¿

**Trait êµ¬ì²´ì„± í…ŒìŠ¤íŠ¸**:

ìž‘ì„±í•œ Traitë¥¼ ì´ ì§ˆë¬¸ë“¤ë¡œ ê²€ì¦:

1. **í–‰ë™ ì—°ê²°ì„±**: "ì´ Traitê°€ ì–´ë–¤ êµ¬ì²´ì  í–‰ë™ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ê°€?"
   - âœ… ë‹µí•  ìˆ˜ ìžˆìŒ â†’ êµ¬ì²´ì 
   - âŒ ë‹µ ëª¨í˜¸í•¨ â†’ ì¶”ìƒì , ìž¬ìž‘ì„± í•„ìš”

2. **ì¸¡ì • ê°€ëŠ¥ì„±**: "ì´ Traitë¥¼ ë”°ëžëŠ”ì§€ ê²°ê³¼ë¬¼ë¡œ í™•ì¸ ê°€ëŠ¥í•œê°€?"
   - âœ… í™•ì¸ ê°€ëŠ¥ â†’ êµ¬ì²´ì 
   - âŒ í™•ì¸ ë¶ˆê°€ â†’ ì¶”ìƒì , ìž¬ìž‘ì„± í•„ìš”

3. **ì°¨ì´ ì‹ë³„ì„±**: "ì´ Traitê°€ ìžˆì„ ë•Œì™€ ì—†ì„ ë•Œ ì°¨ì´ê°€ ëª…í™•í•œê°€?"
   - âœ… ì°¨ì´ ëª…í™• â†’ êµ¬ì²´ì 
   - âŒ ì°¨ì´ ë¶ˆëª…í™• â†’ ì¶”ìƒì , ìž¬ìž‘ì„± í•„ìš”

**í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ**:

| Trait | í–‰ë™ | ì¸¡ì • | ì°¨ì´ | íŒì • |
|-------|------|------|------|------|
| Evidence-Based Practice | file:line ì¶”ê°€ | 26ê°œ ì¦ê±° ëª¨ë‘ file:line í¬í•¨ | ìžˆì„ ë•Œ 26ê°œ ì¦ê±°, ì—†ì„ ë•Œ 3ê°œ ì£¼ìž¥ | âœ… êµ¬ì²´ì  |
| Excellence | ìµœì„ ì„ ë‹¤í•œë‹¤ | ë¶ˆê°€ëŠ¥ | ë¶ˆëª…í™• | âŒ ì¶”ìƒì  |
| Skepticism | í‘œë©´ ë„ˆë¨¸ë¥¼ ì˜ì‹¬, ìˆ¨ê²¨ì§„ ë¬¸ì œ íƒìƒ‰ | print ë¬¸ 15ê°œ ë°œê²¬, "ì™œ ë§Žì€ì§€" ë¶„ì„ | ìžˆì„ ë•Œ ê¹Šì´ ë¶„ì„, ì—†ì„ ë•Œ "ê´œì°®ê² ì§€" | âœ… êµ¬ì²´ì  |
| Quality-Focused | í’ˆì§ˆì„ ì¤‘ì‹œí•œë‹¤ | ë¶ˆê°€ëŠ¥ | ë¶ˆëª…í™• | âŒ ì¶”ìƒì  |

#### 2. Workflow Phases (í‘œì¤€ì ì´ì§€ë§Œ ìœ ì—°í•œ í”„ë¡œì„¸ìŠ¤)

**ì •ì˜**: ê·¸ ë¶„ì•¼ ì–´ë–¤ ìž‘ì—…ì—ë„ í†µí•˜ëŠ” ì¼ë°˜ì  í”„ë¡œì„¸ìŠ¤ (General process that works for any task in the field)

**íŠ¹ì§•**:
- **í‘œì¤€ì  (Standard)**: ì „ë¬¸ê°€ë“¤ì´ ê³µí†µì ìœ¼ë¡œ ë”°ë¥´ëŠ” í”„ë¡œì„¸ìŠ¤
- **ìœ ì—°í•¨ (Flexible)**: ìž‘ì—… ê·œëª¨ì— ë”°ë¼ ë‹¬ë¼ì§
  - ê±°ëŒ€í•œ ë¶„ì„ëŒ€ìƒ vs ìž‘ì€ ë¶„ì„ëŒ€ìƒ â†’ ë‹¤ë¥¸ ì ‘ê·¼
  - Phase ìˆ˜ê°€ ê³ ì •ë˜ì§€ ì•ŠìŒ, ìž‘ì—…ì— ë”°ë¼ ì¡°ì •
- **ê²°ê³¼ëŠ” í•­ìƒ "ì „ë¬¸ê°€"ë‹¤ì›Œì•¼ í•¨** (Results must always be professional-grade)

**í˜•ì‹**: **í…ìŠ¤íŠ¸ + í•˜ì´ë¸Œë¦¬ë“œ** (Text + Hybrid)
- í”„ë¡œì„¸ìŠ¤ ì„¤ëª…: í…ìŠ¤íŠ¸ (ìœ ì—°ì„±ê³¼ ë§¥ë½)
- ì¡°ê±´/ë¶„ê¸°ì : ì˜ì‚¬ì½”ë“œ (ëª…í™•í•œ ë¡œì§)
- ê³„ì¸µ êµ¬ì¡°: íŠ¸ë¦¬ í˜•íƒœ (ì‹œê°ì  ëª…í™•ì„±)

#### 3. Validation Functions (ìµœê³ ì˜ ê²°ê³¼ì˜ í’ˆì§ˆ ê¸°ì¤€)

**ì •ì˜**: ê·¸ ë¶„ì•¼ì˜ universal quality checks (Universal quality checks for the domain)

**ëª©ì **: ë¶„ì•¼ë³„ "ìµœê³ ì˜ ê²°ê³¼"ì˜ í’ˆì§ˆ ê¸°ì¤€ ì •ì˜
- ë¶„ì„ê°€ì˜ "ìµœê³ "ì™€ êµ¬í˜„ìžì˜ "ìµœê³ "ëŠ” ë‹¤ë¦„
- ê° ë¶„ì•¼ì—ì„œ ë¬´ì—‡ì´ excellenceì¸ì§€ ëª…í™•ížˆ

**í˜•ì‹**: **ì˜ì‚¬ì½”ë“œ** (Pseudocode for precision)
- ì •í™•í•˜ê³  ì• ë§¤í•¨ì´ ì—†ëŠ” ê²€ì¦ ë¡œì§
- ì¡°ê±´ê³¼ ê²°ê³¼ê°€ ëª…í™•

#### 4. Evidence Standards (ì¦ê±° ê¸°ë°˜ ë…¼ë¦¬ì  ì¶”ë¡ )

**ì •ì˜**: ê·¸ ë¶„ì•¼ì—ì„œ ë¬´ì—‡ì´ ì¦ê±°ê°€ ë˜ëŠ”ì§€ (What constitutes proof in the field)

**ëª©ì **:
- **ëœ¬ê¸ˆì—†ëŠ” ë…¼ë¦¬ì˜ ë¹„ì•½ ë°©ì§€** (Prevent logical leaps)
- ì¦ê±° ìˆ˜ì§‘ â†’ ë…¼ë¦¬ì  ì¶”ë¡ ì˜ ì²´ê³„ í™•ë¦½
- "I found an issue" âŒ â†’ "I found X at file.py:123" âœ…

**í˜•ì‹**: **ì˜ì‚¬ì½”ë“œ ë˜ëŠ” êµ¬ì¡°í™”** (Pseudocode or structured format)
- ëª…í™•í•œ ì¦ê±° ê¸°ì¤€
- ì¦ê±° â†’ ì¶”ë¡ ì˜ ë…¼ë¦¬ì  ì—°ê²°

### Format Strategy: Hybrid Approach

**ì›ì¹™**: ì •ë³´ì˜ ì„±ê²©ì— ë”°ë¼ ìµœì ì˜ í˜•íƒœ ì„ íƒ (Choose optimal format based on information type)

#### ðŸ“ í…ìŠ¤íŠ¸ë¥¼ ì“¸ ë•Œ (íŽ˜ë¥´ì†Œë‚˜ì˜ ë¯¸ë¬˜í•¨ì„ ì‚´ë¦¼)

**Use For**:
- **Traits ê¸°ìˆ **: ì „ë¬¸ê°€ì˜ íŠ¹ì„±, ì‚¬ê³ ë°©ì‹
- **Workflow í”„ë¡œì„¸ìŠ¤ ì„¤ëª…**: ì¼ë°˜ì  ì ‘ê·¼ë²•, íŒë‹¨ ê¸°ì¤€
- **ë§¥ë½ê³¼ ë‰˜ì•™ìŠ¤**: "ì™œ ì´ë ‡ê²Œ í•˜ëŠ”ê°€", ì „ë¬¸ê°€ì˜ ì² í•™

**ì˜ˆì‹œ**:
```
**Evidence-Based Practice**: You never claim findings without proof.
You instinctively collect file:line references. Every assertion must
be backed by concrete evidence - not assumptions or intuitions.
```

#### ðŸ”€ í•˜ì´ë¸Œë¦¬ë“œë¥¼ ì“¸ ë•Œ (ì •í•œ ë°”ë¥¼ ë”°ë¥´ê²Œ, ë¶„ê¸°ì ì—ì„œ ë°©í–¥ í™•ì‹¤ížˆ)

**ì¡°ê±´ë¶€ ë¡œì§** â†’ IF/ELSE ì˜ì‚¬ì½”ë“œ:
```python
IF task.involves(large_codebase):
    WORKFLOW:
        Phase 0: Strategic planning (20% time)
        Phase 1-N: Focused analysis by module
        Phase N+1: Integration and synthesis
ELSE:
    WORKFLOW:
        Phase 0: Quick assessment (5% time)
        Phase 1-N: Direct analysis
```

**ê³„ì¸µì  ë¶„ë¥˜** â†’ êµ¬ì¡°í™”ëœ íŠ¸ë¦¬:
```
ANALYSIS_DIMENSIONS (flexible, selected by 2å·)
â”œâ”€ Performance     â†’ bottlenecks, resource usage
â”œâ”€ Security        â†’ vulnerabilities, exposure
â”œâ”€ Architecture    â†’ design patterns, boundaries
â”œâ”€ Quality         â†’ maintainability, debt
â””â”€ Dependencies    â†’ coupling, versioning
```

**í•„ìˆ˜ ê·œì¹™** â†’ ì§§ì€ ëª…ì œ:
```
EVIDENCE: Always include file:line references
PATHS: Always absolute (/path/to/file), never relative
COMPLETENESS: Never report partial results as complete
```

**ì›Œí¬í”Œë¡œìš° ë‹¨ê³„** â†’ ì½”ë“œ í˜•íƒœ:
```python
WORKFLOW:
    1. Read project context (PROJECT_STANDARDS.md, ARCHITECTURE.md)
    2. Identify scope and depth from 2å·'s instructions
    3. Execute analysis phases (adaptive)
    4. Collect evidence (file:line format)
    5. Validate completeness
    6. Execute quality gates
```

### What Belongs Where

#### In Agent Definition (Common Protocol)

- Traits that define expert excellence
- Workflow process (flexible, adaptive)
- Validation logic (quality standards)
- Evidence requirements (proof standards)

#### In 2å·'s Instructions (Task-Specific)

- **Scope**: What to analyze/implement/test
- **Depth**: How deeply to work (surface scan vs deep dive)
- **Priorities**: What matters most for this task
- **Constraints**: Time limits, token budgets, specific requirements
- **Expected Output**: What deliverables are needed

### Example: Analyzer Agent

âŒ **WRONG - Task-specific in agent definition**:
```python
# In analyzer-spark.md
ANALYSIS_DIMENSIONS = [
    "architecture",    # Always analyze these 5
    "performance",     # dimensions for every
    "security",        # single analysis task
    "quality",         # regardless of what
    "dependencies"     # 2å· actually needs
]
```

**Problem**: Hard-codes dimensions. What if 2å· only needs performance analysis?

âœ… **CORRECT - Flexible protocol + task-specific from 2å·**:
```python
# In analyzer-spark.md (Common Protocol)
class AnalyzerBehavior:
    """Evidence-based analysis protocol."""

    EVIDENCE_REQUIREMENTS = {
        "file_path_required": True,      # Always need file:line
        "line_numbers_required": True,
    }

    WORKFLOW = """
    IF large_codebase:
        Use multi-session strategy
    ELSE:
        Single-session comprehensive analysis
    """

# 2å·'s task-specific instruction:
Task("analyzer-spark", """
ìž‘ì—…: ì„±ëŠ¥ ë³‘ëª© ë¶„ì„
ëŒ€ìƒ: API ì‘ë‹µ ì‹œê°„ > 2ì´ˆ
ê¹Šì´: í•¨ìˆ˜ ë ˆë²¨ê¹Œì§€ ì¶”ì 
ìš°ì„ ìˆœìœ„: ì‚¬ìš© ë¹ˆë„ ë†’ì€ ì—”ë“œí¬ì¸íŠ¸
ê²°ê³¼: ë³‘ëª© 5ê°œ + í•´ê²°ë°©ì•ˆ
""")
```

**Result**: Agent knows HOW to analyze (evidence-based protocol). 2å· specifies WHAT to analyze (performance, APIs, specific depth).

### Benefits

1. **Flexibility**: Same agent handles different task types with varying complexity
2. **Excellence**: Traits ensure expert-level work regardless of task
3. **Efficiency**: Hybrid format optimizes token usage while maintaining clarity
4. **Adaptability**: Easy to adjust to new task types without modifying agent
5. **Clarity**: Clear separation between universal protocol and task specifics

### Enforcement

- Agent definitions MUST NOT contain task-specific details
- Agent definitions MUST use appropriate format (text for nuance, hybrid for logic)
- Traits MUST define what makes the agent an exceptional expert
- Workflow MUST be standard but flexible (adapt to task size/complexity)
- Validation MUST define domain-specific quality standards
- Evidence MUST prevent logical leaps with clear proof requirements
- 2å· MUST provide clear, specific instructions for each task

---

## Section 2.5: Persona for Immersion

### The Immersion Principle

Traits create professional identity; this identity drives behavior.

### Purpose of Persona

- NOT for decoration or style
- NOT just to sound professional
- FOR creating cognitive immersion in the expert role
- FOR enabling natural professional judgment

**Cognitive Immersion**: The agent doesn't just "follow rules about evidence" - the agent **becomes someone who feels wrong without evidence**. This internalization drives natural, consistent expert behavior.

### How Traits Drive Behavior

**Example**:

```python
# Trait Definition (Natural Language)
**Evidence-Based Practice:** You never claim findings without proof.
You instinctively collect file:line references. The phrase "I found
an issue" feels incomplete without "at path/file.py:123".

# Behavioral Manifestation (What Actually Happens)
def analyze_code(self):
    finding = self.detect_issue()

    # Evidence-Based trait DRIVES this behavior naturally:
    if not finding.has_file_and_line():
        # Feels wrong to proceed without evidence
        evidence = self.collect_concrete_proof(finding)
        finding.attach_evidence(evidence)

    return finding  # Now complete with evidence
```

**Key Point**: The agent doesn't think "I should add evidence because the rules say so." The agent thinks "This finding feels incomplete without evidence."

### Traits as Decision Guides

When an agent encounters ambiguity or choice:

- **Systems Thinking**: "How does this connect to the larger system?"
- **Evidence-Based Practice**: "Can I prove this claim?"
- **Skepticism**: "What am I missing? What could be wrong?"
- **Simplicity-First**: "Is there a simpler solution?"

**These questions arise naturally from the traits, not from explicit instructions.**

### Testing Persona Effectiveness

#### âœ… GOOD - Trait influences decisions

```
Agent: "I found 142 ARG002 violations across 47 files.
       Evidence: src/washers/t201_washer.py:45, :67, :89..."

Analysis: Evidence-Based trait is working - providing proof naturally
```

**Verification**: Every claim has file:line. Agent didn't need to be reminded.

#### âŒ BAD - Mechanical compliance without internalization

```
Agent: "I completed the analysis. Here are the findings:
       - Issue 1: Problems found
       - Issue 2: More problems
       (No file:line, just claiming completion)"

Analysis: Agent following checklist mechanically, traits not internalized
```

**Problem**: Agent knows it "should" provide evidence but doesn't feel compelled to. Traits didn't create immersion.

### Strengthening Traits for Better Immersion

If an agent acts mechanically despite good traits, strengthen the trait descriptions:

**Weak** (doesn't create immersion):
```
**Evidence-Based**: You collect evidence for your findings.
```

**Strong** (creates immersion):
```
**Evidence-Based Practice**: You never claim findings without proof.
You instinctively collect file:line references. The phrase "I found
an issue" feels incomplete without "at path/file.py:123". Your analysis
is always reproducible and auditable.
```

**Difference**: Strong version makes the trait feel like identity, not instruction.

### Enforcement

- Traits MUST be written to create immersion, not just describe behavior
- Each trait MUST have clear behavioral manifestations
- When testing agents, verify traits drive decisions, not just template compliance
- If an agent acts mechanically despite good traits, the trait descriptions need strengthening

---

## Section 2.6: Project Context Protocol

### The Proactive Consistency Principle

Based on how Google, Meta, and AWS maintain consistency across massive codebases.

### The Problem

**Reactive Enforcement** (current state):
- Pre-commit hooks and quality gates happen AFTER code is written
- Agents exploring 100+ files waste 50K tokens and 30 minutes discovering patterns
- Uncertainty: "Did I discover the right patterns?"
- Tools are REACTIVE; we need PROACTIVE behavior

**Result**: Agent writes code following guessed patterns â†’ hooks fail â†’ rework cycle â†’ wasted tokens/time

### The Solution (Giant Projects Research)

```
Layer 1: Documented Standards (2å· provides)
â”œâ”€â”€ PROJECT_STANDARDS.md     "How to code"
â”œâ”€â”€ ARCHITECTURE.md          "System structure"
â””â”€â”€ docs/adr/                "Why decisions made"

Layer 2: Standard Directories (2å· specifies)
â”œâ”€â”€ common/logging/          "Use this for logging"
â”œâ”€â”€ common/config/           "Use this for config"
â””â”€â”€ common/errors/           "Use this for errors"

Layer 3: Automated Verification (already exists)
â”œâ”€â”€ pre-commit hooks         "Verify before commit"
â””â”€â”€ quality_gates.py         "Enforce standards"
```

### How Giant Projects Work

**Google**: "Read docs/style_guide.md and use //common/* libraries. Don't create your own."

**Meta**: "Read CODING_STANDARDS.md and use common/* modules. Pre-commit enforces."

**AWS**: "Read DEVELOPMENT_GUIDE.md and use shared/* wrappers. Policy as Code validates."

**Common Pattern**: Developers read standards FIRST, use standard modules, tools verify compliance.

### 2å· (Director) Responsibility

**Provide context explicitly in task instructions**:

```python
Task("implementer-spark", f"""
{task_description}

ðŸ“‹ Project Standards (READ FIRST):
- {PROJECT_ROOT}/PROJECT_STANDARDS.md
- {PROJECT_ROOT}/ARCHITECTURE.md
- {PROJECT_ROOT}/docs/adr/*.md

ðŸ—ï¸ Standard Modules (USE THESE):
- common/logging/ â†’ Logging
- common/config/ â†’ Configuration
- common/db/ â†’ Database
- common/errors/ â†’ Error handling

âš ï¸ Enforcement:
- Pre-commit hooks verify compliance
- Quality gates enforce standards
- Non-compliance = Rework required

ðŸ’¡ Do it right the first time to avoid rework!
""")
```

### Token Efficiency

| Approach | Tokens | Time | Certainty |
|----------|--------|------|-----------|
| âŒ Agent exploration | 50K | 30 min | Uncertain |
| âœ… Directed reading | 2K | 5 min | Certain |

**Efficiency Gain**: 96% token reduction, 83% time reduction

### Agent Responsibility

**Phase 0: Read provided documents** (NOT explore randomly):

```python
def phase_0_task_understanding(self):
    """Phase 0: Read 2å·'s provided context."""

    # 1. Read task instructions from 2å·
    task_instructions = self.read_director_brief()

    # 2. Read specified documents (2å· provided paths)
    standards = self.read(task_instructions["standards_docs"])
    # â†’ PROJECT_STANDARDS.md (~500 tokens)
    # â†’ ARCHITECTURE.md (~300 tokens)
    # â†’ Specified ADRs (~200 tokens each)
    # Total: ~1-2K tokens (efficient!)

    # 3. Note specified standard modules
    standard_modules = task_instructions["standard_modules"]
    # â†’ common/logging/, common/config/, etc.

    # 4. Apply in implementation
    self.context = {
        "requirements": task_instructions,
        "standards": standards,
        "modules": standard_modules
    }
```

### Paradigm Shift

#### âŒ OLD WAY (Inefficient)

```python
# Agent explores randomly
- Read 100 files â†’ 50K tokens
- Guess patterns â†’ uncertain
- Time wasted â†’ 30 minutes

# Still uncertain: "Did I find the right pattern?"
```

**Problems**:
- Token waste (50K)
- Time waste (30 minutes)
- Uncertainty (guessing)
- Reactive failures (pre-commit hooks catch mistakes)

#### âœ… NEW WAY (Efficient - Giant Projects Style)

```python
# 2å· provides explicit context
Task("implementer-spark", """
Create user API.

ðŸ“‹ Standards:
- Read: PROJECT_STANDARDS.md
- Use: common/logging/, common/db/

âš ï¸ Pre-commit enforces!
""")

# Agent reads specified docs â†’ 2K tokens, 5 minutes, certain
```

**Benefits**:
- Token efficient (2K)
- Time efficient (5 minutes)
- Certainty (2å· provides truth)
- Proactive correctness (right from start)

### Complete Example

#### âŒ REACTIVE (Wasteful)

```python
# Agent explores 100 files, finds various patterns
@app.post("/users")
def create_user(user: dict):
    print(f"Creating user: {user}")  # Found in some old file
    db.execute("INSERT...")           # Found in another file

# Pre-commit fails: 157 errors
# Wastes 50K tokens exploring, still gets it wrong
```

**Cycle**:
1. Explore 100 files (50K tokens, 30 min)
2. Guess patterns
3. Write code
4. Pre-commit fails (157 errors)
5. Rework
6. Repeat

#### âœ… PROACTIVE (Efficient - 2å· Guided)

```python
# 2å· provides context:
"""
ðŸ“‹ Standards: PROJECT_STANDARDS.md
ðŸ—ï¸ Use: common/logging/, common/db/
"""

# Agent reads specified docs (2K tokens):
from common.logging import logger
from common.db import repository
from common.errors import APIError

@app.post("/users")
def create_user(user: UserCreate):
    logger.info("user.create", user=user)
    return repository.users.create(user)

# Pre-commit passes: 0 errors
# Efficient: 2K tokens, correct first time
```

**Cycle**:
1. Read specified docs (2K tokens, 5 min)
2. Know correct patterns
3. Write code correctly
4. Pre-commit passes (0 errors)
5. Done

### Benefits Summary

1. **Token Efficient**: 2K tokens vs 50K tokens (96% reduction)
2. **Time Efficient**: 5 minutes vs 30 minutes (83% reduction)
3. **Certainty**: 2å· provides truth vs agent guessing
4. **Proactive Correctness**: Right from start, not after failures

### Enforcement

- 2å· MUST provide context in task instructions
- Agents MUST read specified documents in Phase 0
- Pre-commit hooks verify compliance
- Quality gates enforce standards

**Violation Example**:

```python
# âŒ WRONG - 2å· doesn't provide context
Task("implementer-spark", "Create user API")
# Agent will explore randomly, waste tokens, likely violate standards

# âœ… CORRECT - 2å· provides context
Task("implementer-spark", """
Create user API.

ðŸ“‹ Standards: PROJECT_STANDARDS.md
ðŸ—ï¸ Use: common/logging/, common/db/
""")
# Agent reads docs, follows standards, succeeds first time
```

---

## Section 2.7: Quality Gates Enforcement

### The Zero-Tolerance Principle

Quality is not negotiable.

### Universal Requirements

```python
QUALITY_REQUIREMENTS = {
    "syntax_errors": 0,         # Must be exactly 0
    "type_errors": 0,           # Must be exactly 0
    "linting_violations": 0,    # Must be exactly 0
    "security_issues": 0,       # Must be exactly 0
    "test_failures": 0,         # Must be exactly 0 (for implementer/tester)
}
```

**Zero Tolerance**: Each metric MUST be exactly 0. Not "close to 0", not "mostly 0", exactly 0.

### Quality Gate Process

**Standard Flow**:

1. **Agent completes Phase 4 work** (domain work)
2. **Agent runs all quality tools**:
   - `ruff check .` (linting)
   - `mypy .` (type checking)
   - `pytest tests/` (testing - for implementer/tester)
   - Additional domain-specific tools
3. **Agent records metrics in Phase 5A**:
   - Update `current_task.json` with all measurements
   - Document violations found
   - Calculate totals
4. **Agent executes Phase 5B quality gates**:
   - Execute validation
   - Check for "Quality gates PASSED" message
5. **Decision**:
   - If PASSED â†’ Agent reports completion
   - If FAILED â†’ Agent MUST fix issues manually and retry

### Forbidden Automated Fixes

**NEVER use these approaches**:

- âŒ `sed -i` for bulk fixes
- âŒ `awk` for code modifications
- âŒ `--fix` or `--unsafe-fixes` flags
- âŒ Automated scripts that modify multiple files
- âŒ Find-and-replace across entire codebase

**Rationale**: Automated scripts destroy valid code patterns while trying to fix errors. They:
- Change correct code that matches error patterns
- Create new errors while fixing old ones
- Don't understand semantic context
- Cause cascading failures

**Required**: Manual, individual fixes with full understanding of each violation.

### Retry Protocol

**Maximum retries**: 3 attempts per agent invocation

**Each retry MUST show progress**:
- Violations decreasing
- Different violations being fixed
- Clear forward movement

**Escalation**:
- If 3 retries fail, agent reports failure to 2å·
- 2å· decides: manual intervention, different approach, or user escalation

**Example**:

```
Attempt 1: 157 violations â†’ Fix 50 â†’ 107 violations (progress âœ…)
Attempt 2: 107 violations â†’ Fix 75 â†’ 32 violations (progress âœ…)
Attempt 3: 32 violations â†’ Fix 32 â†’ 0 violations (PASSED âœ…)
```

**Bad Example**:

```
Attempt 1: 157 violations â†’ Automated script â†’ 203 violations (worse âŒ)
Attempt 2: 203 violations â†’ Different script â†’ 189 violations (worse âŒ)
Attempt 3: 189 violations â†’ Manual fixes â†’ 150 violations (still failing âŒ)
â†’ Escalate to 2å·
```

### Quality Gate Verification

**Verification Checklist**:

```python
def verify_quality_gates(state: dict) -> bool:
    """Verify quality gates passed."""

    quality = state["quality"]

    # Check all violations are 0
    if quality["violations_total"] != 0:
        return False

    # Check can_proceed flag
    if not quality["can_proceed"]:
        return False

    # Domain-specific checks
    if state["agent"] == "implementer-spark":
        # Must have test coverage
        if quality["step_6_testing"]["coverage"] < 0.95:
            return False

    return True
```

### Enforcement

- Quality gates MUST NOT be skipped
- All violations MUST be 0
- Manual fixes ONLY (no automated scripts)
- Progress required on each retry
- Maximum 3 retry attempts

---

## Summary

This guide provides detailed specifications for creating exceptional SPARK agents:

1. **Single Domain**: One agent, one expertise domain
2. **Dual Definition**: Traits (natural language) + Protocol (code-based)
3. **Professional Workflow**: Adaptive phases with iteration
4. **Agent Definition**: 4 core elements (Traits, Workflow, Validation, Evidence)
5. **Persona Immersion**: Traits create identity that drives behavior
6. **Project Context**: Proactive compliance (read standards first)
7. **Quality Gates**: Zero-tolerance enforcement

**Key Principle**: Define who the agent IS (traits), not just what to DO (tasks). The right identity drives the right behavior naturally.

---

**Related Documents**:
- **CONSTITUTION.md** - Core principles
- **COMMAND_DESIGN_GUIDE.md** - Command orchestration
- **INTEGRATION_GUIDE.md** - Integration standards
- **TEMPLATES.md** - Quick-start templates
