![img](https://www-cdn.anthropic.com/images/4zrzovbb/website/094d7021ebd5cf57eabd63b456899c97f5231c88-1000x1000.svg)

# 우리가 다중 에이전트 연구 시스템을 구축한 방법

게시됨2025년 6월 13일

우리의 Research 기능은 복잡한 주제를 더 효과적으로 탐구하기 위해 여러 Claude 에이전트를 사용합니다. 우리는 이 시스템을 구축하면서 겪은 엔지니어링 과제와 배운 교훈을 공유합니다.

Claude는 이제[Research 기능을 갖추었습니다](https://www.anthropic.com/news/research)웹, Google Workspace 및 모든 통합 서비스를 검색하여 복잡한 작업을 수행할 수 있게 해주는

이 다중 에이전트 시스템을 프로토타입에서 프로덕션으로 발전시키는 과정에서 시스템 아키텍처, 도구 설계, 그리고 프롬프트 엔지니어링에 대한 중요한 교훈을 얻었습니다. 다중 에이전트 시스템은 여러 에이전트(루프에서 자율적으로 도구를 사용하는 LLM들)가 함께 작업하는 시스템으로 구성됩니다. 우리의 Research 기능은 사용자 쿼리를 기반으로 연구 프로세스를 계획하는 에이전트를 포함하며, 이후 도구를 사용하여 동시에 정보를 검색하는 병렬 에이전트들을 생성합니다. 여러 에이전트를 가진 시스템은 에이전트 조정, 평가, 그리고 신뢰성 측면에서 새로운 도전 과제를 제시합니다.

이 글은 우리에게 효과적이었던 원칙들을 분석합니다. 여러분이 자신만의 멀티 에이전트 시스템을 구축할 때 이러한 원칙들을 적용하는 데 도움이 되기를 바랍니다.

### 멀티 에이전트 시스템의 장점

연구 작업은 필요한 단계들을 미리 예측하기 매우 어려운 개방형 문제들을 다룹니다. 복잡한 주제를 탐구하는 고정된 경로를 하드코딩할 수는 없는데, 이는 그 과정이 본질적으로 역동적이고 경로 의존적이기 때문입니다. 사람들이 연구를 수행할 때, 그들은 발견에 기반하여 접근 방식을 지속적으로 업데이트하는 경향이 있으며, 조사 과정에서 나타나는 단서들을 따라갑니다.

이러한 예측 불가능성은 AI 에이전트를 연구 작업에 특히 적합하게 만듭니다. 연구는 조사가 진행됨에 따라 방향을 바꾸거나 부수적인 연결고리를 탐색할 수 있는 유연성을 요구합니다. 모델은 중간 발견 사항을 바탕으로 어떤 방향을 추구할지 결정하면서 여러 차례에 걸쳐 자율적으로 작동해야 합니다. 선형적이고 일회성인 파이프라인으로는 이러한 작업을 처리할 수 없습니다.

검색의 본질은 압축입니다: 방대한 말뭉치에서 통찰을 추출하는 것입니다. 하위 에이전트들은 각자의 컨텍스트 윈도우를 가지고 병렬로 작동하여 압축을 촉진하며, 주요 연구 에이전트를 위해 가장 중요한 토큰들을 압축하기 전에 질문의 다양한 측면을 동시에 탐색합니다. 각 하위 에이전트는 또한 관심사의 분리를 제공합니다—서로 다른 도구, 프롬프트, 탐색 궤적—이는 경로 의존성을 줄이고 철저하고 독립적인 조사를 가능하게 합니다.

지능이 임계점에 도달하면, 다중 에이전트 시스템은 성능을 확장하는 중요한 방법이 됩니다. 예를 들어, 개별 인간은 지난 10만 년 동안 더 지능적이 되었지만, 인간 사회는 정보화 시대에 우리의 *집단* 지능과 협력 능력 덕분에 *기하급수적으로* 더 유능해졌습니다. 일반적으로 지능적인 에이전트라도 개별적으로 작동할 때는 한계에 직면하지만, 에이전트 그룹은 훨씬 더 많은 것을 성취할 수 있습니다.

우리의 내부 평가에 따르면 다중 에이전트 연구 시스템은 특히 여러 독립적인 방향을 동시에 추구하는 폭우선 쿼리에서 뛰어난 성능을 보입니다. Claude Opus 4를 주 에이전트로 하고 Claude Sonnet 4를 하위 에이전트로 하는 다중 에이전트 시스템이 우리의 내부 연구 평가에서 단일 에이전트 Claude Opus 4보다 90.2% 더 우수한 성능을 보였습니다. 예를 들어, S&P 500 정보기술 기업들의 모든 이사회 구성원을 식별하라는 요청을 받았을 때, 다중 에이전트 시스템은 이를 하위 에이전트들을 위한 작업으로 분해하여 정확한 답을 찾았지만, 단일 에이전트 시스템은 느리고 순차적인 검색으로 답을 찾지 못했습니다.

다중 에이전트 시스템이 주로 작동하는 이유는 문제를 해결하기 위해 충분한 토큰을 소비하는 데 도움이 되기 때문입니다. 우리의 분석에서 세 가지 요인이 [BrowseComp](https://openai.com/index/browsecomp/)평가(브라우징 에이전트가 찾기 어려운 정보를 찾는 능력을 테스트함)에서 토큰 사용량 자체가 분산의 80%를 설명하며, 도구 호출 횟수와 모델 선택이 다른 두 가지 설명 요인임을 발견했습니다. 이 발견은 별도의 컨텍스트 창을 가진 에이전트들에게 작업을 분산시켜 병렬 추론을 위한 더 많은 용량을 추가하는 우리의 아키텍처를 검증합니다. 최신 Claude 모델들은 토큰 사용에 대한 큰 효율성 승수 역할을 하며, Claude Sonnet 4로 업그레이드하는 것이 Claude Sonnet 3.7에서 토큰 예산을 두 배로 늘리는 것보다 더 큰 성능 향상을 가져다줍니다. 멀티 에이전트 아키텍처는 단일 에이전트의 한계를 초과하는 작업에 대해 토큰 사용량을 효과적으로 확장합니다.

단점도 있습니다: 실제로 이러한 아키텍처는 토큰을 빠르게 소모합니다. 우리 데이터에 따르면, 에이전트는 일반적으로 채팅 상호작용보다 약 4배 많은 토큰을 사용하고, 멀티 에이전트 시스템은 채팅보다 약 15배 많은 토큰을 사용합니다. 경제적 실행 가능성을 위해서는 멀티 에이전트 시스템이 향상된 성능에 대한 비용을 지불할 만큼 작업의 가치가 충분히 높은 작업을 필요로 합니다. 또한, 모든 에이전트가 동일한 컨텍스트를 공유해야 하거나 에이전트 간에 많은 종속성이 있는 일부 도메인은 현재 멀티 에이전트 시스템에 적합하지 않습니다. 예를 들어, 대부분의 코딩 작업은 연구보다 진정으로 병렬화 가능한 작업이 적고, LLM 에이전트는 아직 실시간으로 다른 에이전트와 조정하고 위임하는 데 뛰어나지 않습니다. 우리는 멀티 에이전트 시스템이 대규모 병렬화, 단일 컨텍스트 윈도우를 초과하는 정보, 그리고 수많은 복잡한 도구와의 인터페이싱을 포함하는 가치 있는 작업에서 뛰어난 성능을 발휘한다는 것을 발견했습니다.

### 연구를 위한 아키텍처 개요

우리의 연구 시스템은 오케스트레이터-워커 패턴을 사용한 멀티 에이전트 아키텍처를 사용하며, 리드 에이전트가 프로세스를 조정하면서 병렬로 작동하는 전문화된 서브 에이전트들에게 작업을 위임합니다.

![img](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F1198befc0b33726c45692ac40f764022f4de1bf2-4584x2579.png&w=3840&q=75)멀티 에이전트 아키텍처의 실제 작동: 사용자 쿼리는 리드 에이전트를 통해 흘러가며, 리드 에이전트는 다양한 측면을 병렬로 검색하기 위해 전문화된 서브 에이전트들을 생성합니다.

사용자가 쿼리를 제출하면, 리드 에이전트가 이를 분석하고 전략을 개발한 다음 서브에이전트들을 생성하여 다양한 측면을 동시에 탐색합니다. 위 다이어그램에서 보듯이, 서브에이전트들은 검색 도구를 반복적으로 사용하여 정보를 수집하는 지능형 필터 역할을 하며, 이 경우 2025년 AI 에이전트 회사들에 대한 정보를 수집한 후 회사 목록을 리드 에이전트에게 반환하여 최종 답변을 작성할 수 있도록 합니다.

검색 증강 생성(RAG)을 사용하는 기존 접근법은 정적 검색을 사용합니다. 즉, 입력 쿼리와 가장 유사한 청크 집합을 가져와서 이러한 청크를 사용하여 응답을 생성합니다. 반면, 우리의 아키텍처는 관련 정보를 동적으로 찾고, 새로운 발견에 적응하며, 결과를 분석하여 고품질 답변을 공식화하는 다단계 검색을 사용합니다.

![img](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3bde53c9578d74f6e05c3e515e20b910c5a8c20a-4584x4584.png&w=3840&q=75)사용자가 쿼리를 제출할 때 우리의 다중 에이전트 연구 시스템의 완전한 워크플로우를 보여주는 프로세스 다이어그램입니다. 사용자가 쿼리를 제출하면, 시스템은 반복적인 연구 프로세스에 진입하는 LeadResearcher 에이전트를 생성합니다. LeadResearcher는 접근 방식을 고민하고 계획을 메모리에 저장하여 컨텍스트를 유지하는 것으로 시작합니다. 컨텍스트 윈도우가 200,000 토큰을 초과하면 잘리게 되므로 계획을 유지하는 것이 중요하기 때문입니다. 그런 다음 특정 연구 작업을 가진 전문화된 서브에이전트들을 생성합니다(여기서는 두 개가 표시되어 있지만 개수는 제한이 없습니다). 각 서브에이전트는 독립적으로 웹 검색을 수행하고, 다음을 사용하여 도구 결과를 평가합니다[교차 사고](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#interleaved-thinking), 그리고 결과를 LeadResearcher에게 반환합니다. LeadResearcher는 이러한 결과들을 종합하고 추가 연구가 필요한지 결정합니다—필요하다면 추가 하위 에이전트를 생성하거나 전략을 개선할 수 있습니다. 충분한 정보가 수집되면, 시스템은 연구 루프를 종료하고 모든 발견 사항을 CitationAgent에게 전달합니다. CitationAgent는 문서와 연구 보고서를 처리하여 인용을 위한 특정 위치를 식별합니다. 이를 통해 모든 주장이 해당 출처에 적절히 귀속되도록 보장합니다. 인용이 완료된 최종 연구 결과는 사용자에게 반환됩니다.

### 연구 에이전트를 위한 프롬프트 엔지니어링 및 평가

다중 에이전트 시스템은 단일 에이전트 시스템과 주요한 차이점이 있으며, 여기에는 조정 복잡성의 급속한 증가가 포함됩니다. 초기 에이전트들은 간단한 쿼리에 대해 50개의 하위 에이전트를 생성하거나, 존재하지 않는 소스를 찾기 위해 웹을 끝없이 탐색하거나, 과도한 업데이트로 서로를 방해하는 등의 오류를 범했습니다. 각 에이전트는 프롬프트에 의해 제어되기 때문에, 프롬프트 엔지니어링이 이러한 행동을 개선하기 위한 우리의 주요 수단이었습니다. 다음은 에이전트 프롬프팅에 대해 우리가 배운 몇 가지 원칙들입니다:

1. **에이전트처럼 생각하세요.**프롬프트를 개선하려면 그 효과를 이해해야 합니다. 이를 돕기 위해 우리는 다음을 사용하여 시뮬레이션을 구축했습니다[Console](https://console.anthropic.com/)우리 시스템의 정확한 프롬프트와 도구를 사용하여 에이전트가 단계별로 작업하는 것을 관찰했습니다. 이를 통해 실패 모드가 즉시 드러났습니다: 에이전트가 이미 충분한 결과를 얻었음에도 계속 진행하거나, 지나치게 장황한 검색 쿼리를 사용하거나, 잘못된 도구를 선택하는 경우들이었습니다. 효과적인 프롬프팅은 에이전트에 대한 정확한 멘탈 모델을 개발하는 것에 의존하며, 이를 통해 가장 영향력 있는 변경사항을 명확하게 파악할 수 있습니다.
2. **오케스트레이터에게 위임하는 방법을 가르치세요.**우리 시스템에서는 주도 에이전트가 쿼리를 하위 작업으로 분해하고 이를 하위 에이전트들에게 설명합니다. 각 하위 에이전트는 목표, 출력 형식, 사용할 도구와 소스에 대한 지침, 그리고 명확한 작업 경계가 필요합니다. 상세한 작업 설명이 없으면 에이전트들이 작업을 중복하거나, 빈틈을 남기거나, 필요한 정보를 찾지 못합니다. 처음에는 주도 에이전트가 '반도체 부족 현상을 조사하라'와 같은 간단하고 짧은 지시를 내리도록 했지만, 이러한 지시가 종종 모호해서 하위 에이전트들이 작업을 잘못 해석하거나 다른 에이전트들과 정확히 동일한 검색을 수행한다는 것을 발견했습니다. 예를 들어, 한 하위 에이전트는 2021년 자동차 칩 위기를 탐구하는 동안 다른 2개 에이전트는 현재 2025년 공급망을 조사하는 작업을 중복으로 수행하여 효과적인 업무 분담이 이루어지지 않았습니다.
3. **쿼리 복잡도에 맞춰 노력을 조절하세요.**에이전트들은 서로 다른 작업에 적절한 노력을 판단하는 데 어려움을 겪기 때문에, 우리는 프롬프트에 확장 규칙을 내장했습니다. 단순한 사실 확인은 3-10회의 도구 호출을 하는 1개의 에이전트만 필요하고, 직접적인 비교는 각각 10-15회 호출을 하는 2-4개의 하위 에이전트가 필요할 수 있으며, 복잡한 연구는 명확히 분담된 책임을 가진 10개 이상의 하위 에이전트를 사용할 수 있습니다. 이러한 명시적인 가이드라인은 리드 에이전트가 자원을 효율적으로 할당하고 단순한 쿼리에 과도한 투자를 방지하는 데 도움이 되며, 이는 초기 버전에서 흔한 실패 모드였습니다.
4. **도구 설계와 선택이 중요합니다.**에이전트-도구 인터페이스는 인간-컴퓨터 인터페이스만큼 중요합니다. 올바른 도구를 사용하는 것은 효율적이며, 종종 반드시 필요합니다. 예를 들어, Slack에만 존재하는 맥락을 찾기 위해 웹을 검색하는 에이전트는 처음부터 실패할 운명입니다.[MCP 서버를 통해](https://modelcontextprotocol.io/introduction)모델이 외부 도구에 접근할 수 있게 하는 경우, 에이전트가 품질이 천차만별인 설명을 가진 처음 보는 도구들을 마주하게 되면서 이 문제가 더욱 복잡해집니다. 우리는 에이전트에게 명시적인 휴리스틱을 제공했습니다: 예를 들어, 먼저 사용 가능한 모든 도구를 검토하고, 도구 사용을 사용자 의도에 맞추고, 광범위한 외부 탐색을 위해 웹을 검색하거나, 일반적인 도구보다 전문화된 도구를 선호하는 것입니다. 잘못된 도구 설명은 에이전트를 완전히 잘못된 경로로 이끌 수 있으므로, 각 도구는 뚜렷한 목적과 명확한 설명을 가져야 합니다.
5. **에이전트가 스스로 개선하도록 하세요**Claude 4 모델이 뛰어난 프롬프트 엔지니어가 될 수 있다는 것을 발견했습니다. 프롬프트와 실패 모드가 주어지면, 에이전트가 왜 실패하는지 진단하고 개선 사항을 제안할 수 있습니다. 심지어 도구 테스트 에이전트도 만들었습니다. 결함이 있는 MCP 도구가 주어지면, 이 에이전트는 도구를 사용해보고 실패를 피하기 위해 도구 설명을 다시 작성합니다. 도구를 수십 번 테스트함으로써, 이 에이전트는 핵심적인 뉘앙스와 버그를 발견했습니다. 도구 사용성을 개선하는 이 과정은 새로운 설명을 사용하는 향후 에이전트들의 작업 완료 시간을 40% 단축시켰습니다. 대부분의 실수를 피할 수 있었기 때문입니다.
6. **넓게 시작해서 점차 좁혀나가세요.**검색 전략은 전문가의 인간 연구를 모방해야 합니다: 구체적인 내용을 파고들기 전에 전체적인 상황을 탐색하는 것입니다. 에이전트들은 종종 지나치게 길고 구체적인 쿼리를 기본으로 사용하여 적은 결과만을 반환받습니다. 우리는 에이전트들이 짧고 광범위한 쿼리로 시작하여 사용 가능한 정보를 평가한 다음, 점진적으로 초점을 좁혀나가도록 프롬프트를 제공함으로써 이러한 경향에 대응했습니다.
7. **사고 과정을 안내하세요.** [확장된 사고 모드](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)는 Claude가 가시적인 사고 과정에서 추가 토큰을 출력하도록 하여 제어 가능한 스크래치패드 역할을 할 수 있습니다. 리드 에이전트는 사고를 사용하여 접근 방식을 계획하고, 작업에 적합한 도구를 평가하며, 쿼리 복잡성과 서브에이전트 수를 결정하고, 각 서브에이전트의 역할을 정의합니다. 우리의 테스트 결과 확장된 사고가 지시 따르기, 추론, 효율성을 향상시키는 것으로 나타났습니다. 서브에이전트들도 계획을 세운 다음[교차 사고](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#interleaved-thinking)도구 결과 이후에 품질을 평가하고, 격차를 식별하며, 다음 쿼리를 개선합니다. 이는 하위 에이전트가 모든 작업에 더 효과적으로 적응할 수 있게 만듭니다.
8. **병렬 도구 호출이 속도와 성능을 변화시킵니다.**복잡한 연구 작업은 자연스럽게 많은 출처를 탐색하는 것을 포함합니다. 초기 에이전트들은 순차적 검색을 실행했는데, 이는 고통스러울 정도로 느렸습니다. 속도를 위해 두 가지 종류의 병렬화를 도입했습니다: (1) 리드 에이전트가 순차적이 아닌 병렬로 3-5개의 서브에이전트를 생성하고; (2) 서브에이전트들이 3개 이상의 도구를 병렬로 사용합니다. 이러한 변경사항들은 복잡한 쿼리에 대해 연구 시간을 최대 90%까지 단축시켜, Research가 다른 시스템들보다 더 많은 정보를 다루면서도 몇 시간이 아닌 몇 분 안에 더 많은 작업을 수행할 수 있게 했습니다.

우리의 프롬프팅 전략은 경직된 규칙보다는 좋은 휴리스틱을 심어주는 데 중점을 둡니다. 우리는 숙련된 인간이 연구 작업에 어떻게 접근하는지 연구하고 이러한 전략들을 프롬프트에 인코딩했습니다. 예를 들어 어려운 질문을 더 작은 작업으로 분해하기, 소스의 품질을 신중하게 평가하기, 새로운 정보에 기반해 검색 접근법을 조정하기, 그리고 언제 깊이(한 주제를 자세히 조사)에 집중할지 대 폭(여러 주제를 병렬로 탐색)에 집중할지 인식하기와 같은 전략들입니다. 또한 에이전트가 통제 불능 상태로 빠지는 것을 방지하기 위해 명시적인 가드레일을 설정하여 의도하지 않은 부작용을 사전에 완화했습니다. 마지막으로, 관찰 가능성과 테스트 케이스를 통한 빠른 반복 루프에 집중했습니다.

### 에이전트의 효과적인 평가

좋은 평가는 신뢰할 수 있는 AI 애플리케이션을 구축하는 데 필수적이며, 에이전트도 예외가 아닙니다. 하지만 멀티 에이전트 시스템을 평가하는 것은 독특한 도전 과제를 제시합니다. 전통적인 평가는 종종 AI가 매번 동일한 단계를 따른다고 가정합니다: 입력 X가 주어지면, 시스템은 경로 Y를 따라 출력 Z를 생성해야 한다는 것입니다. 하지만 멀티 에이전트 시스템은 이런 식으로 작동하지 않습니다. 동일한 시작점에서도 에이전트들은 목표에 도달하기 위해 완전히 다른 유효한 경로를 택할 수 있습니다. 한 에이전트는 세 개의 소스를 검색할 수 있고 다른 에이전트는 열 개를 검색할 수 있으며, 또는 같은 답을 찾기 위해 서로 다른 도구를 사용할 수도 있습니다. 우리는 항상 올바른 단계가 무엇인지 알 수 없기 때문에, 보통 에이전트들이 미리 정해둔 "올바른" 단계를 따랐는지 단순히 확인할 수는 없습니다. 대신, 에이전트들이 합리적인 과정을 따르면서 올바른 결과를 달성했는지를 판단하는 유연한 평가 방법이 필요합니다.

**작은 샘플로 즉시 평가를 시작하세요**초기 에이전트 개발에서는 저수준 성과 개선 요소가 풍부하기 때문에 변경사항이 극적인 영향을 미치는 경향이 있습니다. 프롬프트 조정만으로도 성공률이 30%에서 80%로 향상될 수 있습니다. 이렇게 큰 효과 크기에서는 몇 개의 테스트 케이스만으로도 변화를 감지할 수 있습니다. 우리는 실제 사용 패턴을 대표하는 약 20개의 쿼리 세트로 시작했습니다. 이러한 쿼리들을 테스트하면 변경사항의 영향을 명확하게 확인할 수 있었습니다. AI 개발팀들이 수백 개의 테스트 케이스가 포함된 대규모 평가만이 유용하다고 믿어서 평가 생성을 미루는 경우를 자주 듣습니다. 하지만 더 철저한 평가를 구축할 때까지 미루기보다는 몇 가지 예시로 소규모 테스트를 즉시 시작하는 것이 가장 좋습니다.

**잘 수행되었을 때의 LLM-as-judge 평가 척도.**연구 결과물은 자유 형식의 텍스트이고 단일한 정답이 거의 없기 때문에 프로그래밍 방식으로 평가하기 어렵습니다. LLM은 결과물을 채점하는 데 자연스럽게 적합합니다. 우리는 각 결과물을 루브릭의 기준에 따라 평가하는 LLM 심사관을 사용했습니다: 사실적 정확성(주장이 출처와 일치하는가?), 인용 정확성(인용된 출처가 주장과 일치하는가?), 완전성(요청된 모든 측면이 다뤄졌는가?), 출처 품질(낮은 품질의 2차 출처보다 1차 출처를 사용했는가?), 그리고 도구 효율성(적절한 도구를 합리적인 횟수만큼 사용했는가?). 각 구성 요소를 평가하기 위해 여러 심사관을 실험해봤지만, 0.0-1.0 점수와 합격-불합격 등급을 출력하는 단일 프롬프트로 단일 LLM 호출을 하는 것이 가장 일관되고 인간의 판단과 일치한다는 것을 발견했습니다. 이 방법은 평가 테스트 케이스가*했다*명확한 답변을 가지고 있었고, LLM 판정자를 사용하여 답변이 정확한지 간단히 확인할 수 있었습니다 (즉, R&D 예산이 가장 큰 상위 3개 제약회사를 정확히 나열했는가?). LLM을 판정자로 사용함으로써 수백 개의 출력을 확장 가능한 방식으로 평가할 수 있었습니다.

**인간 평가는 자동화가 놓치는 것을 포착합니다.**에이전트를 테스트하는 사람들은 평가에서 놓치는 엣지 케이스를 발견합니다. 여기에는 특이한 쿼리에 대한 환각적 답변, 시스템 오류, 또는 미묘한 소스 선택 편향이 포함됩니다. 우리의 경우, 인간 테스터들은 초기 에이전트가 학술 PDF나 개인 블로그와 같은 권위 있지만 순위가 낮은 소스보다 SEO에 최적화된 콘텐츠 팜을 일관되게 선택한다는 것을 발견했습니다. 프롬프트에 소스 품질 휴리스틱을 추가하는 것이 이 문제를 해결하는 데 도움이 되었습니다. 자동화된 평가의 세상에서도 수동 테스트는 여전히 필수적입니다.

다중 에이전트 시스템은 특정 프로그래밍 없이도 나타나는 창발적 행동을 보입니다. 예를 들어, 리드 에이전트에 대한 작은 변경이 하위 에이전트들의 행동을 예측할 수 없는 방식으로 바꿀 수 있습니다. 성공하려면 개별 에이전트의 행동뿐만 아니라 상호작용 패턴을 이해해야 합니다. 따라서 이러한 에이전트들을 위한 최적의 프롬프트는 단순한 엄격한 지시가 아니라, 업무 분담, 문제 해결 접근법, 그리고 노력 예산을 정의하는 협업 프레임워크입니다. 이를 올바르게 구현하려면 신중한 프롬프팅과 도구 설계, 견고한 휴리스틱, 관찰 가능성, 그리고 긴밀한 피드백 루프가 필요합니다. 참조하세요[우리 쿡북의 오픈소스 프롬프트](https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents/prompts)에서 우리 시스템의 예시 프롬프트를 확인할 수 있습니다.

### 생산 안정성 및 엔지니어링 과제

전통적인 소프트웨어에서는 버그가 기능을 손상시키거나 성능을 저하시키거나 장애를 일으킬 수 있습니다. 에이전트 시스템에서는 작은 변경사항이 큰 행동 변화로 연쇄 반응을 일으키기 때문에, 장기 실행 프로세스에서 상태를 유지해야 하는 복잡한 에이전트를 위한 코드를 작성하는 것이 매우 어려워집니다.

**에이전트는 상태를 가지며 오류가 누적됩니다.**에이전트는 장기간 실행되면서 여러 도구 호출에 걸쳐 상태를 유지할 수 있습니다. 이는 코드를 지속적으로 실행하고 그 과정에서 발생하는 오류를 처리해야 함을 의미합니다. 효과적인 완화 조치가 없다면, 사소한 시스템 장애도 에이전트에게는 치명적일 수 있습니다. 오류가 발생했을 때 처음부터 다시 시작할 수는 없습니다. 재시작은 비용이 많이 들고 사용자에게 좌절감을 줍니다. 대신, 우리는 오류가 발생한 지점에서 에이전트가 재개할 수 있는 시스템을 구축했습니다. 또한 모델의 지능을 활용하여 문제를 우아하게 처리합니다. 예를 들어, 도구가 실패할 때 에이전트에게 알려주고 적응할 수 있도록 하는 것이 놀랍도록 잘 작동합니다. 우리는 Claude를 기반으로 한 AI 에이전트의 적응성을 재시도 로직과 정기적인 체크포인트 같은 결정론적 안전장치와 결합합니다.

**디버깅은 새로운 접근법으로부터 이익을 얻습니다.**에이전트는 동적으로 결정을 내리며, 동일한 프롬프트를 사용하더라도 실행 간에 비결정적입니다. 이로 인해 디버깅이 더 어려워집니다. 예를 들어, 사용자들이 에이전트가 "명백한 정보를 찾지 못한다"고 보고했지만, 우리는 그 이유를 알 수 없었습니다. 에이전트가 잘못된 검색 쿼리를 사용하고 있었을까요? 부적절한 소스를 선택했을까요? 도구 오류에 부딪혔을까요? 완전한 프로덕션 추적을 추가함으로써 에이전트가 실패한 이유를 진단하고 체계적으로 문제를 해결할 수 있었습니다. 표준 관찰 가능성을 넘어서, 우리는 사용자 프라이버시를 유지하기 위해 개별 대화의 내용을 모니터링하지 않으면서도 에이전트 결정 패턴과 상호작용 구조를 모니터링합니다. 이러한 고수준 관찰 가능성은 근본 원인을 진단하고, 예상치 못한 행동을 발견하며, 일반적인 실패를 수정하는 데 도움이 되었습니다.

**배포에는 신중한 조정이 필요합니다.**에이전트 시스템은 거의 지속적으로 실행되는 프롬프트, 도구, 실행 로직의 고도로 상태를 가진 웹입니다. 이는 업데이트를 배포할 때마다 에이전트가 프로세스의 어느 지점에나 있을 수 있음을 의미합니다. 따라서 선의의 코드 변경이 기존 에이전트를 손상시키지 않도록 방지해야 합니다. 모든 에이전트를 동시에 새 버전으로 업데이트할 수는 없습니다. 대신 우리는[레인보우 배포를 사용합니다](https://brandon.dimcheff.com/2018/02/rainbow-deploys-with-kubernetes/)실행 중인 에이전트의 중단을 방지하기 위해, 두 버전을 동시에 실행하면서 기존 버전에서 새 버전으로 트래픽을 점진적으로 이동시키는 방식입니다.

**동기 실행은 병목 현상을 만듭니다.**현재 우리의 리드 에이전트는 하위 에이전트들을 동기적으로 실행하여, 각 하위 에이전트 그룹이 완료될 때까지 기다린 후 다음 단계로 진행합니다. 이는 조정을 단순화하지만 에이전트 간 정보 흐름에 병목 현상을 만듭니다. 예를 들어, 리드 에이전트가 하위 에이전트를 조정할 수 없고, 하위 에이전트들이 서로 협력할 수 없으며, 단일 하위 에이전트가 검색을 완료하기를 기다리는 동안 전체 시스템이 차단될 수 있습니다. 비동기 실행은 추가적인 병렬성을 가능하게 할 것입니다: 에이전트들이 동시에 작업하고 필요할 때 새로운 하위 에이전트를 생성하는 것입니다. 하지만 이러한 비동기성은 결과 조정, 상태 일관성, 그리고 하위 에이전트 간 오류 전파에서 어려움을 추가합니다. 모델들이 더 길고 복잡한 연구 작업을 처리할 수 있게 됨에 따라, 성능 향상이 복잡성을 정당화할 것으로 예상합니다.

### 결론

AI 에이전트를 구축할 때, 마지막 단계가 종종 여정의 대부분을 차지하게 됩니다. 개발자 머신에서 작동하는 코드베이스가 신뢰할 수 있는 프로덕션 시스템이 되려면 상당한 엔지니어링이 필요합니다. 에이전트 시스템에서 오류의 복합적 특성은 기존 소프트웨어에서는 사소한 문제였던 것이 에이전트를 완전히 탈선시킬 수 있음을 의미합니다. 한 단계가 실패하면 에이전트가 완전히 다른 궤적을 탐색하게 되어 예측할 수 없는 결과를 초래할 수 있습니다. 이 글에서 설명한 모든 이유로 인해, 프로토타입과 프로덕션 사이의 격차는 종종 예상보다 더 큽니다.

이러한 도전에도 불구하고, 멀티 에이전트 시스템은 개방형 연구 과제에서 가치 있는 것으로 입증되었습니다. 사용자들은 Claude가 그들이 고려하지 못했던 비즈니스 기회를 찾고, 복잡한 의료 옵션을 탐색하며, 까다로운 기술적 버그를 해결하고, 혼자서는 찾지 못했을 연구 연결고리를 발견함으로써 며칠간의 작업을 절약할 수 있도록 도왔다고 말했습니다. 멀티 에이전트 연구 시스템은 신중한 엔지니어링, 포괄적인 테스트, 세부사항에 집중한 프롬프트 및 도구 설계, 견고한 운영 관행, 그리고 현재 에이전트 역량에 대한 깊은 이해를 가진 연구, 제품, 엔지니어링 팀 간의 긴밀한 협력을 통해 대규모로 안정적으로 작동할 수 있습니다. 우리는 이미 이러한 시스템이 사람들이 복잡한 문제를 해결하는 방식을 변화시키는 것을 목격하고 있습니다.

![img](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F09a90e0aca54859553e93c18683e7fd33ff16d4c-2654x2148.png&w=3840&q=75)A [Clio](https://www.anthropic.com/research/clio)Research 기능을 현재 사람들이 가장 일반적으로 사용하는 방식을 보여주는 임베딩 플롯입니다. 주요 사용 사례 카테고리는 전문 도메인에서 소프트웨어 시스템 개발(10%), 전문적이고 기술적인 콘텐츠 개발 및 최적화(8%), 비즈니스 성장 및 수익 창출 전략 개발(8%), 학술 연구 및 교육 자료 개발 지원(7%), 그리고 사람, 장소 또는 조직에 대한 정보 연구 및 검증(5%)입니다.

### 감사의 말

Jeremy Hadfield, Barry Zhang, Kenneth Lien, Florian Scholz, Jeremy Fox, Daniel Ford가 작성했습니다. 이 작업은 Research 기능을 가능하게 만든 Anthropic의 여러 팀들의 공동 노력을 반영합니다. 이 복잡한 멀티 에이전트 시스템을 프로덕션에 도입한 헌신적인 노력을 보여준 Anthropic 앱 엔지니어링 팀에게 특별한 감사를 드립니다. 또한 훌륭한 피드백을 제공해 주신 초기 사용자들에게도 감사드립니다.

## 부록

다음은 멀티 에이전트 시스템에 대한 몇 가지 추가적인 기타 팁들입니다.

**여러 턴에 걸쳐 상태를 변경하는 에이전트들의 최종 상태 평가.**다중 턴 대화에서 지속적인 상태를 수정하는 에이전트를 평가하는 것은 독특한 도전을 제시합니다. 읽기 전용 연구 작업과 달리, 각 행동은 후속 단계를 위한 환경을 변경할 수 있어 기존 평가 방법이 처리하기 어려운 종속성을 만듭니다. 우리는 턴별 분석보다는 최종 상태 평가에 집중하는 것이 성공적임을 발견했습니다. 에이전트가 특정 프로세스를 따랐는지 판단하는 대신, 올바른 최종 상태를 달성했는지 평가하세요. 이 접근법은 에이전트가 의도된 결과를 제공하면서도 동일한 목표에 대한 대안적 경로를 찾을 수 있음을 인정합니다. 복잡한 워크플로우의 경우, 모든 중간 단계를 검증하려고 시도하기보다는 특정 상태 변경이 발생해야 하는 개별 체크포인트로 평가를 나누세요.

**장기간 대화 관리.**프로덕션 에이전트들은 종종 수백 번의 턴에 걸친 대화에 참여하므로, 신중한 컨텍스트 관리 전략이 필요합니다. 대화가 길어질수록 표준 컨텍스트 윈도우는 불충분해지며, 지능적인 압축 및 메모리 메커니즘이 필요하게 됩니다. 우리는 에이전트가 완료된 작업 단계를 요약하고 새로운 작업을 진행하기 전에 필수 정보를 외부 메모리에 저장하는 패턴을 구현했습니다. 컨텍스트 한계에 접근할 때, 에이전트는 신중한 인수인계를 통해 연속성을 유지하면서 깨끗한 컨텍스트를 가진 새로운 서브에이전트를 생성할 수 있습니다. 또한, 컨텍스트 한계에 도달했을 때 이전 작업을 잃는 대신 연구 계획과 같은 저장된 컨텍스트를 메모리에서 검색할 수 있습니다. 이러한 분산 접근 방식은 확장된 상호작용에서 대화의 일관성을 유지하면서 컨텍스트 오버플로우를 방지합니다.

**서브에이전트 출력을 파일시스템에 저장하여 '전화 게임' 현상을 최소화합니다.**직접적인 하위 에이전트 출력은 특정 유형의 결과에 대해 주 코디네이터를 우회할 수 있어 충실도와 성능을 모두 향상시킵니다. 하위 에이전트가 모든 것을 리드 에이전트를 통해 소통하도록 요구하는 대신, 전문화된 에이전트가 독립적으로 지속되는 출력을 생성할 수 있는 아티팩트 시스템을 구현하세요. 하위 에이전트는 도구를 호출하여 외부 시스템에 작업을 저장한 다음, 가벼운 참조를 코디네이터에게 다시 전달합니다. 이는 다단계 처리 중 정보 손실을 방지하고 대화 기록을 통해 대용량 출력을 복사하는 데 따른 토큰 오버헤드를 줄입니다. 이 패턴은 하위 에이전트의 전문화된 프롬프트가 일반적인 코디네이터를 통해 필터링하는 것보다 더 나은 결과를 생성하는 코드, 보고서 또는 데이터 시각화와 같은 구조화된 출력에 특히 잘 작동합니다.

------



# How we built our multi-agent research system

Published Jun 13, 2025

Our Research feature uses multiple Claude agents to explore complex topics more effectively. We share the engineering challenges and the lessons we learned from building this system.

Claude now has [Research capabilities](https://www.anthropic.com/news/research) that allow it to search across the web, Google Workspace, and any integrations to accomplish complex tasks.

The journey of this multi-agent system from prototype to production taught us critical lessons about system architecture, tool design, and prompt engineering. A multi-agent system consists of multiple agents (LLMs autonomously using tools in a loop) working together. Our Research feature involves an agent that plans a research process based on user queries, and then uses tools to create parallel agents that search for information simultaneously. Systems with multiple agents introduce new challenges in agent coordination, evaluation, and reliability.

This post breaks down the principles that worked for us—we hope you'll find them useful to apply when building your own multi-agent systems.

### Benefits of a multi-agent system

Research work involves open-ended problems where it’s very difficult to predict the required steps in advance. You can’t hardcode a fixed path for exploring complex topics, as the process is inherently dynamic and path-dependent. When people conduct research, they tend to continuously update their approach based on discoveries, following leads that emerge during investigation.

This unpredictability makes AI agents particularly well-suited for research tasks. Research demands the flexibility to pivot or explore tangential connections as the investigation unfolds. The model must operate autonomously for many turns, making decisions about which directions to pursue based on intermediate findings. A linear, one-shot pipeline cannot handle these tasks.

The essence of search is compression: distilling insights from a vast corpus. Subagents facilitate compression by operating in parallel with their own context windows, exploring different aspects of the question simultaneously before condensing the most important tokens for the lead research agent. Each subagent also provides separation of concerns—distinct tools, prompts, and exploration trajectories—which reduces path dependency and enables thorough, independent investigations.

Once intelligence reaches a threshold, multi-agent systems become a vital way to scale performance. For instance, although individual humans have become more intelligent in the last 100,000 years, human societies have become *exponentially* more capable in the information age because of our *collective* intelligence and ability to coordinate. Even generally-intelligent agents face limits when operating as individuals; groups of agents can accomplish far more.

Our internal evaluations show that multi-agent research systems excel especially for breadth-first queries that involve pursuing multiple independent directions simultaneously. We found that a multi-agent system with Claude Opus 4 as the lead agent and Claude Sonnet 4 subagents outperformed single-agent Claude Opus 4 by 90.2% on our internal research eval. For example, when asked to identify all the board members of the companies in the Information Technology S&P 500, the multi-agent system found the correct answers by decomposing this into tasks for subagents, while the single agent system failed to find the answer with slow, sequential searches.

Multi-agent systems work mainly because they help spend enough tokens to solve the problem. In our analysis, three factors explained 95% of the performance variance in the [BrowseComp](https://openai.com/index/browsecomp/) evaluation (which tests the ability of browsing agents to locate hard-to-find information). We found that token usage by itself explains 80% of the variance, with the number of tool calls and the model choice as the two other explanatory factors. This finding validates our architecture that distributes work across agents with separate context windows to add more capacity for parallel reasoning. The latest Claude models act as large efficiency multipliers on token use, as upgrading to Claude Sonnet 4 is a larger performance gain than doubling the token budget on Claude Sonnet 3.7. Multi-agent architectures effectively scale token usage for tasks that exceed the limits of single agents.

There is a downside: in practice, these architectures burn through tokens fast. In our data, agents typically use about 4× more tokens than chat interactions, and multi-agent systems use about 15× more tokens than chats. For economic viability, multi-agent systems require tasks where the value of the task is high enough to pay for the increased performance. Further, some domains that require all agents to share the same context or involve many dependencies between agents are not a good fit for multi-agent systems today. For instance, most coding tasks involve fewer truly parallelizable tasks than research, and LLM agents are not yet great at coordinating and delegating to other agents in real time. We’ve found that multi-agent systems excel at valuable tasks that involve heavy parallelization, information that exceeds single context windows, and interfacing with numerous complex tools.

### Architecture overview for Research

Our Research system uses a multi-agent architecture with an orchestrator-worker pattern, where a lead agent coordinates the process while delegating to specialized subagents that operate in parallel.

![img](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F1198befc0b33726c45692ac40f764022f4de1bf2-4584x2579.png&w=3840&q=75)The multi-agent architecture in action: user queries flow through a lead agent that creates specialized subagents to search for different aspects in parallel.

When a user submits a query, the lead agent analyzes it, develops a strategy, and spawns subagents to explore different aspects simultaneously. As shown in the diagram above, the subagents act as intelligent filters by iteratively using search tools to gather information, in this case on AI agent companies in 2025, and then returning a list of companies to the lead agent so it can compile a final answer.

Traditional approaches using Retrieval Augmented Generation (RAG) use static retrieval. That is, they fetch some set of chunks that are most similar to an input query and use these chunks to generate a response. In contrast, our architecture uses a multi-step search that dynamically finds relevant information, adapts to new findings, and analyzes results to formulate high-quality answers.

![img](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3bde53c9578d74f6e05c3e515e20b910c5a8c20a-4584x4584.png&w=3840&q=75)Process diagram showing the complete workflow of our multi-agent Research system. When a user submits a query, the system creates a LeadResearcher agent that enters an iterative research process. The LeadResearcher begins by thinking through the approach and saving its plan to Memory to persist the context, since if the context window exceeds 200,000 tokens it will be truncated and it is important to retain the plan. It then creates specialized Subagents (two are shown here, but it can be any number) with specific research tasks. Each Subagent independently performs web searches, evaluates tool results using [interleaved thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#interleaved-thinking), and returns findings to the LeadResearcher. The LeadResearcher synthesizes these results and decides whether more research is needed—if so, it can create additional subagents or refine its strategy. Once sufficient information is gathered, the system exits the research loop and passes all findings to a CitationAgent, which processes the documents and research report to identify specific locations for citations. This ensures all claims are properly attributed to their sources. The final research results, complete with citations, are then returned to the user.

### Prompt engineering and evaluations for research agents

Multi-agent systems have key differences from single-agent systems, including a rapid growth in coordination complexity. Early agents made errors like spawning 50 subagents for simple queries, scouring the web endlessly for nonexistent sources, and distracting each other with excessive updates. Since each agent is steered by a prompt, prompt engineering was our primary lever for improving these behaviors. Below are some principles we learned for prompting agents:

1. **Think like your agents.** To iterate on prompts, you must understand their effects. To help us do this, we built simulations using our [Console](https://console.anthropic.com/) with the exact prompts and tools from our system, then watched agents work step-by-step. This immediately revealed failure modes: agents continuing when they already had sufficient results, using overly verbose search queries, or selecting incorrect tools. Effective prompting relies on developing an accurate mental model of the agent, which can make the most impactful changes obvious.
2. **Teach the orchestrator how to delegate.** In our system, the lead agent decomposes queries into subtasks and describes them to subagents. Each subagent needs an objective, an output format, guidance on the tools and sources to use, and clear task boundaries. Without detailed task descriptions, agents duplicate work, leave gaps, or fail to find necessary information. We started by allowing the lead agent to give simple, short instructions like 'research the semiconductor shortage,' but found these instructions often were vague enough that subagents misinterpreted the task or performed the exact same searches as other agents. For instance, one subagent explored the 2021 automotive chip crisis while 2 others duplicated work investigating current 2025 supply chains, without an effective division of labor.
3. **Scale effort to query complexity.** Agents struggle to judge appropriate effort for different tasks, so we embedded scaling rules in the prompts. Simple fact-finding requires just 1 agent with 3-10 tool calls, direct comparisons might need 2-4 subagents with 10-15 calls each, and complex research might use more than 10 subagents with clearly divided responsibilities. These explicit guidelines help the lead agent allocate resources efficiently and prevent overinvestment in simple queries, which was a common failure mode in our early versions.
4. **Tool design and selection are critical.** Agent-tool interfaces are as critical as human-computer interfaces. Using the right tool is efficient—often, it’s strictly necessary. For instance, an agent searching the web for context that only exists in Slack is doomed from the start. With [MCP servers](https://modelcontextprotocol.io/introduction) that give the model access to external tools, this problem compounds, as agents encounter unseen tools with descriptions of wildly varying quality. We gave our agents explicit heuristics: for example, examine all available tools first, match tool usage to user intent, search the web for broad external exploration, or prefer specialized tools over generic ones. Bad tool descriptions can send agents down completely wrong paths, so each tool needs a distinct purpose and a clear description.
5. **Let agents improve themselves**. We found that the Claude 4 models can be excellent prompt engineers. When given a prompt and a failure mode, they are able to diagnose why the agent is failing and suggest improvements. We even created a tool-testing agent—when given a flawed MCP tool, it attempts to use the tool and then rewrites the tool description to avoid failures. By testing the tool dozens of times, this agent found key nuances and bugs. This process for improving tool ergonomics resulted in a 40% decrease in task completion time for future agents using the new description, because they were able to avoid most mistakes.
6. **Start wide, then narrow down.** Search strategy should mirror expert human research: explore the landscape before drilling into specifics. Agents often default to overly long, specific queries that return few results. We counteracted this tendency by prompting agents to start with short, broad queries, evaluate what’s available, then progressively narrow focus.
7. **Guide the thinking process.** [Extended thinking mode](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking), which leads Claude to output additional tokens in a visible thinking process, can serve as a controllable scratchpad. The lead agent uses thinking to plan its approach, assessing which tools fit the task, determining query complexity and subagent count, and defining each subagent’s role. Our testing showed that extended thinking improved instruction-following, reasoning, and efficiency. Subagents also plan, then use [interleaved thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#interleaved-thinking) after tool results to evaluate quality, identify gaps, and refine their next query. This makes subagents more effective in adapting to any task.
8. **Parallel tool calling transforms speed and performance.** Complex research tasks naturally involve exploring many sources. Our early agents executed sequential searches, which was painfully slow. For speed, we introduced two kinds of parallelization: (1) the lead agent spins up 3-5 subagents in parallel rather than serially; (2) the subagents use 3+ tools in parallel. These changes cut research time by up to 90% for complex queries, allowing Research to do more work in minutes instead of hours while covering more information than other systems.

Our prompting strategy focuses on instilling good heuristics rather than rigid rules. We studied how skilled humans approach research tasks and encoded these strategies in our prompts—strategies like decomposing difficult questions into smaller tasks, carefully evaluating the quality of sources, adjusting search approaches based on new information, and recognizing when to focus on depth (investigating one topic in detail) vs. breadth (exploring many topics in parallel). We also proactively mitigated unintended side effects by setting explicit guardrails to prevent the agents from spiraling out of control. Finally, we focused on a fast iteration loop with observability and test cases.

### Effective evaluation of agents

Good evaluations are essential for building reliable AI applications, and agents are no different. However, evaluating multi-agent systems presents unique challenges. Traditional evaluations often assume that the AI follows the same steps each time: given input X, the system should follow path Y to produce output Z. But multi-agent systems don't work this way. Even with identical starting points, agents might take completely different valid paths to reach their goal. One agent might search three sources while another searches ten, or they might use different tools to find the same answer. Because we don’t always know what the right steps are, we usually can't just check if agents followed the “correct” steps we prescribed in advance. Instead, we need flexible evaluation methods that judge whether agents achieved the right outcomes while also following a reasonable process.

**Start evaluating immediately with small samples**. In early agent development, changes tend to have dramatic impacts because there is abundant low-hanging fruit. A prompt tweak might boost success rates from 30% to 80%. With effect sizes this large, you can spot changes with just a few test cases. We started with a set of about 20 queries representing real usage patterns. Testing these queries often allowed us to clearly see the impact of changes. We often hear that AI developer teams delay creating evals because they believe that only large evals with hundreds of test cases are useful. However, it’s best to start with small-scale testing right away with a few examples, rather than delaying until you can build more thorough evals.

**LLM-as-judge evaluation scales when done well.** Research outputs are difficult to evaluate programmatically, since they are free-form text and rarely have a single correct answer. LLMs are a natural fit for grading outputs. We used an LLM judge that evaluated each output against criteria in a rubric: factual accuracy (do claims match sources?), citation accuracy (do the cited sources match the claims?), completeness (are all requested aspects covered?), source quality (did it use primary sources over lower-quality secondary sources?), and tool efficiency (did it use the right tools a reasonable number of times?). We experimented with multiple judges to evaluate each component, but found that a single LLM call with a single prompt outputting scores from 0.0-1.0 and a pass-fail grade was the most consistent and aligned with human judgements. This method was especially effective when the eval test cases *did* have a clear answer, and we could use the LLM judge to simply check if the answer was correct (i.e. did it accurately list the pharma companies with the top 3 largest R&D budgets?). Using an LLM as a judge allowed us to scalably evaluate hundreds of outputs.

**Human evaluation catches what automation misses.** People testing agents find edge cases that evals miss. These include hallucinated answers on unusual queries, system failures, or subtle source selection biases. In our case, human testers noticed that our early agents consistently chose SEO-optimized content farms over authoritative but less highly-ranked sources like academic PDFs or personal blogs. Adding source quality heuristics to our prompts helped resolve this issue. Even in a world of automated evaluations, manual testing remains essential.

Multi-agent systems have emergent behaviors, which arise without specific programming. For instance, small changes to the lead agent can unpredictably change how subagents behave. Success requires understanding interaction patterns, not just individual agent behavior. Therefore, the best prompts for these agents are not just strict instructions, but frameworks for collaboration that define the division of labor, problem-solving approaches, and effort budgets. Getting this right relies on careful prompting and tool design, solid heuristics, observability, and tight feedback loops. See the [open-source prompts in our Cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents/prompts) for example prompts from our system.

### Production reliability and engineering challenges

In traditional software, a bug might break a feature, degrade performance, or cause outages. In agentic systems, minor changes cascade into large behavioral changes, which makes it remarkably difficult to write code for complex agents that must maintain state in a long-running process.

**Agents are stateful and errors compound.** Agents can run for long periods of time, maintaining state across many tool calls. This means we need to durably execute code and handle errors along the way. Without effective mitigations, minor system failures can be catastrophic for agents. When errors occur, we can't just restart from the beginning: restarts are expensive and frustrating for users. Instead, we built systems that can resume from where the agent was when the errors occurred. We also use the model’s intelligence to handle issues gracefully: for instance, letting the agent know when a tool is failing and letting it adapt works surprisingly well. We combine the adaptability of AI agents built on Claude with deterministic safeguards like retry logic and regular checkpoints.

**Debugging benefits from new approaches.** Agents make dynamic decisions and are non-deterministic between runs, even with identical prompts. This makes debugging harder. For instance, users would report agents “not finding obvious information,” but we couldn't see why. Were the agents using bad search queries? Choosing poor sources? Hitting tool failures? Adding full production tracing let us diagnose why agents failed and fix issues systematically. Beyond standard observability, we monitor agent decision patterns and interaction structures—all without monitoring the contents of individual conversations, to maintain user privacy. This high-level observability helped us diagnose root causes, discover unexpected behaviors, and fix common failures.

**Deployment needs careful coordination.** Agent systems are highly stateful webs of prompts, tools, and execution logic that run almost continuously. This means that whenever we deploy updates, agents might be anywhere in their process. We therefore need to prevent our well-meaning code changes from breaking existing agents. We can’t update every agent to the new version at the same time. Instead, we use [rainbow deployments](https://brandon.dimcheff.com/2018/02/rainbow-deploys-with-kubernetes/) to avoid disrupting running agents, by gradually shifting traffic from old to new versions while keeping both running simultaneously.

**Synchronous execution creates bottlenecks.** Currently, our lead agents execute subagents synchronously, waiting for each set of subagents to complete before proceeding. This simplifies coordination, but creates bottlenecks in the information flow between agents. For instance, the lead agent can’t steer subagents, subagents can’t coordinate, and the entire system can be blocked while waiting for a single subagent to finish searching. Asynchronous execution would enable additional parallelism: agents working concurrently and creating new subagents when needed. But this asynchronicity adds challenges in result coordination, state consistency, and error propagation across the subagents. As models can handle longer and more complex research tasks, we expect the performance gains will justify the complexity.

### Conclusion

When building AI agents, the last mile often becomes most of the journey. Codebases that work on developer machines require significant engineering to become reliable production systems. The compound nature of errors in agentic systems means that minor issues for traditional software can derail agents entirely. One step failing can cause agents to explore entirely different trajectories, leading to unpredictable outcomes. For all the reasons described in this post, the gap between prototype and production is often wider than anticipated.

Despite these challenges, multi-agent systems have proven valuable for open-ended research tasks. Users have said that Claude helped them find business opportunities they hadn’t considered, navigate complex healthcare options, resolve thorny technical bugs, and save up to days of work by uncovering research connections they wouldn't have found alone. Multi-agent research systems can operate reliably at scale with careful engineering, comprehensive testing, detail-oriented prompt and tool design, robust operational practices, and tight collaboration between research, product, and engineering teams who have a strong understanding of current agent capabilities. We're already seeing these systems transform how people solve complex problems.

![img](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F09a90e0aca54859553e93c18683e7fd33ff16d4c-2654x2148.png&w=3840&q=75)A [Clio](https://www.anthropic.com/research/clio) embedding plot showing the most common ways people are using the Research feature today. The top use case categories are developing software systems across specialized domains (10%), develop and optimize professional and technical content (8%), develop business growth and revenue generation strategies (8%), assist with academic research and educational material development (7%), and research and verify information about people, places, or organizations (5%).

### Acknowlegements

Written by Jeremy Hadfield, Barry Zhang, Kenneth Lien, Florian Scholz, Jeremy Fox, and Daniel Ford. This work reflects the collective efforts of several teams across Anthropic who made the Research feature possible. Special thanks go to the Anthropic apps engineering team, whose dedication brought this complex multi-agent system to production. We're also grateful to our early users for their excellent feedback.

## Appendix

Below are some additional miscellaneous tips for multi-agent systems.

**End-state evaluation of agents that mutate state over many turns.** Evaluating agents that modify persistent state across multi-turn conversations presents unique challenges. Unlike read-only research tasks, each action can change the environment for subsequent steps, creating dependencies that traditional evaluation methods struggle to handle. We found success focusing on end-state evaluation rather than turn-by-turn analysis. Instead of judging whether the agent followed a specific process, evaluate whether it achieved the correct final state. This approach acknowledges that agents may find alternative paths to the same goal while still ensuring they deliver the intended outcome. For complex workflows, break evaluation into discrete checkpoints where specific state changes should have occurred, rather than attempting to validate every intermediate step.

**Long-horizon conversation management.** Production agents often engage in conversations spanning hundreds of turns, requiring careful context management strategies. As conversations extend, standard context windows become insufficient, necessitating intelligent compression and memory mechanisms. We implemented patterns where agents summarize completed work phases and store essential information in external memory before proceeding to new tasks. When context limits approach, agents can spawn fresh subagents with clean contexts while maintaining continuity through careful handoffs. Further, they can retrieve stored context like the research plan from their memory rather than losing previous work when reaching the context limit. This distributed approach prevents context overflow while preserving conversation coherence across extended interactions.

**Subagent output to a filesystem to minimize the ‘game of telephone.’** Direct subagent outputs can bypass the main coordinator for certain types of results, improving both fidelity and performance. Rather than requiring subagents to communicate everything through the lead agent, implement artifact systems where specialized agents can create outputs that persist independently. Subagents call tools to store their work in external systems, then pass lightweight references back to the coordinator. This prevents information loss during multi-stage processing and reduces token overhead from copying large outputs through conversation history. The pattern works particularly well for structured outputs like code, reports, or data visualizations where the subagent's specialized prompt produces better results than filtering through a general coordinator.

