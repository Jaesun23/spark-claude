# SPARK Plugin Implementation Plan

> **í”„ë¡œì íŠ¸**: SPARK Plugin Skills Integration
> **ì‘ì„±ì¼**: 2025-12-03
> **ì‘ì„±ì**: 2í˜¸ (Number Two)
> **ëª©ì **: Skills ì‹œìŠ¤í…œ í†µí•©ì„ í†µí•œ í† í° íš¨ìœ¨ ê°œì„  ë° ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
> **ì°¸ê³ **: ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì˜ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì‹œ ì°¸ê³  ì¼€ì´ìŠ¤ë¡œ í™œìš©

---

## ğŸ“‹ Executive Summary

**3ì¤„ ìš”ì•½**:
1. SPARKì˜ 21ê°œ agentsì— Skills ì‹œìŠ¤í…œì„ í†µí•©í•˜ì—¬ **ì¤‘ë³µ ì½˜í…ì¸  94% ì œê±°**
2. Constitution, standardsë¥¼ skillsë¡œ ë¶„ë¦¬í•˜ì—¬ **ë‹¨ì¼ ì§„ì‹¤ ê³µê¸‰ì›(Single Source of Truth)** êµ¬ì¶•
3. 5ì£¼ê°„ 5ë‹¨ê³„ ì‹¤í–‰ìœ¼ë¡œ **í† í° íš¨ìœ¨ 36% í–¥ìƒ** ë° **ìœ ì§€ë³´ìˆ˜ ë¹„ìš© 80% ì ˆê°**

**ì˜ˆìƒ ì„±ê³¼**:
- Agent ë¡œë”© ì†ë„: 3.9K â†’ 2.5K tokens (36% ê°œì„ )
- ì¤‘ë³µ ì½˜í…ì¸ : 25K â†’ 1.5K tokens (94% ì œê±°)
- Constitution ì—…ë°ì´íŠ¸: 21 files â†’ 1 file (95% ê°„ì†Œí™”)

---

## ğŸ¯ Project Overview

### í˜„ì¬ ìƒíƒœ (As-Is)

**ë¬¸ì œì **:
```
spark-plugin/
â”œâ”€â”€ agents/ (21 files)
â”‚   â”œâ”€â”€ analyzer-spark.md         # Constitution ì¤‘ë³µ (~1.2K tokens)
â”‚   â”œâ”€â”€ implementer-spark.md      # Constitution ì¤‘ë³µ (~1.2K tokens)
â”‚   â”œâ”€â”€ tester-spark.md           # Constitution ì¤‘ë³µ (~1.2K tokens)
â”‚   â””â”€â”€ ... (18 more with duplication)
â”œâ”€â”€ commands/ (12 files)          # âœ… ë¬¸ì œ ì—†ìŒ
â”œâ”€â”€ skills/ (empty)               # âš ï¸ ë¹„ì–´ìˆìŒ
â””â”€â”€ hooks/ (should be here)       # âš ï¸ .claude/hooks/ì— ìˆìŒ
```

**êµ¬ì²´ì  ë¬¸ì œ**:
1. **ì¤‘ë³µ**: Constitutionì´ ê° agentì— ~1,200 tokensì”© ì¤‘ë³µ (21 Ã— 1,200 = 25,200 tokens)
2. **ìœ ì§€ë³´ìˆ˜**: Constitution ì—…ë°ì´íŠ¸ ì‹œ 21ê°œ íŒŒì¼ ëª¨ë‘ ìˆ˜ì • í•„ìš”
3. **ì¼ê´€ì„±**: ìˆ˜ë™ ì—…ë°ì´íŠ¸ë¡œ ì¸í•œ ë²„ì „ ë¶ˆì¼ì¹˜ ìœ„í—˜
4. **í† í° ë‚­ë¹„**: Agent ë¡œë”©ë§ˆë‹¤ ì¤‘ë³µ ì½˜í…ì¸  ë¡œë“œ

### ëª©í‘œ ìƒíƒœ (To-Be)

**ê°œì„  ëª©í‘œ**:
```
spark-plugin/
â”œâ”€â”€ agents/ (21 files)
â”‚   â”œâ”€â”€ analyzer-spark.md         # skills: spark-constitution (~10 tokens)
â”‚   â”œâ”€â”€ implementer-spark.md      # skills: spark-constitution, code-standards (~20 tokens)
â”‚   â”œâ”€â”€ tester-spark.md           # skills: spark-constitution, testing-standards (~20 tokens)
â”‚   â””â”€â”€ ... (18 more, all referencing skills)
â”œâ”€â”€ commands/ (12 files)          # âœ… ë³€ê²½ ì—†ìŒ
â”œâ”€â”€ skills/ (4 skill directories)
â”‚   â”œâ”€â”€ spark-constitution/       # âœ… ìƒˆë¡œ ìƒì„±
â”‚   â”‚   â”œâ”€â”€ SKILL.md
â”‚   â”‚   â”œâ”€â”€ constitution-v1.2.md
â”‚   â”‚   â”œâ”€â”€ quality-gates.md
â”‚   â”‚   â”œâ”€â”€ protocols.md
â”‚   â”‚   â””â”€â”€ examples/
â”‚   â”œâ”€â”€ code-standards/           # âœ… ìƒˆë¡œ ìƒì„±
â”‚   â”œâ”€â”€ testing-standards/        # âœ… ìƒˆë¡œ ìƒì„±
â”‚   â””â”€â”€ architecture-patterns/    # âœ… ìƒˆë¡œ ìƒì„±
â””â”€â”€ hooks/                        # âœ… ì´ë™ ì™„ë£Œ
    â”œâ”€â”€ spark_persona_router.py
    â””â”€â”€ spark_quality_gates.py
```

**ì¸¡ì • ê°€ëŠ¥í•œ ê°œì„ **:
1. **í† í° íš¨ìœ¨**: Agent í‰ê·  3.9K â†’ 2.5K (36% â†“)
2. **ì¤‘ë³µ ì œê±°**: 25.2K â†’ 1.5K (94% â†“)
3. **ì—…ë°ì´íŠ¸ ì‹œê°„**: 21 files â†’ 1 file (95% â†“)
4. **ì¼ê´€ì„±**: 100% (ë‹¨ì¼ ì§„ì‹¤ ê³µê¸‰ì›)

---

## âœ… Success Criteria

### 1. Functional Success (ê¸°ëŠ¥ì  ì„±ê³µ)

**ê²€ì¦ í•­ëª©**:
- [ ] ëª¨ë“  21ê°œ agentsê°€ skillsë¥¼ ìë™ ë¡œë“œ
- [ ] Skillsì˜ supporting filesê°€ contextually ë¡œë“œ
- [ ] ê¸°ì¡´ slash commandsê°€ ì •ìƒ ì‘ë™
- [ ] Quality gatesê°€ ì—¬ì „íˆ í†µê³¼
- [ ] Parallel execution (`/multi-implement`) ì •ìƒ ì‘ë™

**ê²€ì¦ ë°©ë²•**:
```bash
# ê° agent ê°œë³„ í…ŒìŠ¤íŠ¸
/spark-implement "test feature"
/spark-test "test module"
/spark-analyze "test system"

# Parallel execution í…ŒìŠ¤íŠ¸
/multi-implement "task1,task2,task3"

# Quality gates ê²€ì¦
cat ~/.claude/workflows/current_task.json | jq '.quality'
```

### 2. Performance Success (ì„±ëŠ¥ì  ì„±ê³µ)

**ì¸¡ì • ì§€í‘œ**:
| Metric | Before | After | Target |
|--------|--------|-------|--------|
| Agent load tokens | 3.9K | 2.5K | <2.7K (30%â†“) |
| Duplicate content | 25.2K | 1.5K | <3K (88%â†“) |
| Constitution files | 21 | 1 | 1 (100%â†“) |
| Skills load time | N/A | <500ms | <1s |

**ê²€ì¦ ë°©ë²•**:
```bash
# Token counting script
python3 scripts/count_agent_tokens.py agents/implementer-spark.md
# Expected: ~2.5K tokens

# Duplicate detection
python3 scripts/detect_duplicates.py agents/
# Expected: 0 duplicates
```

### 3. Maintainability Success (ìœ ì§€ë³´ìˆ˜ ì„±ê³µ)

**ê²€ì¦ í•­ëª©**:
- [ ] Constitution ì—…ë°ì´íŠ¸ ì‹œ 1ê°œ íŒŒì¼ë§Œ ìˆ˜ì • í•„ìš”
- [ ] Skills ë²„ì „ ê´€ë¦¬ ê°€ëŠ¥ (v1.2, v1.3 ë“±)
- [ ] ìƒˆë¡œìš´ agent ì¶”ê°€ ì‹œ skills ìë™ ì‚¬ìš©
- [ ] Documentation ì™„ì „ì„± (README, examples)

**ê²€ì¦ ë°©ë²•**:
1. Constitution ìˆ˜ì • í…ŒìŠ¤íŠ¸
2. ëª¨ë“  agentsì— ë³€ê²½ì‚¬í•­ ìë™ ë°˜ì˜ í™•ì¸
3. ìƒˆ agent ìƒì„± í›„ skills ë¡œë“œ í™•ì¸

---

## âš ï¸ Risk Assessment

### Risk Matrix

| Risk | Impact | Probability | Mitigation | Contingency |
|------|--------|-------------|------------|-------------|
| Skills ë¡œë“œ ì‹¤íŒ¨ | High | Low | Phaseë³„ í…ŒìŠ¤íŠ¸ | Rollback script |
| Agent ë™ì‘ ë³€ê²½ | High | Medium | ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ìœ ì§€ | ë²„ì „ íƒœê·¸ |
| Token ì¦ê°€ | Medium | Low | Token ì¸¡ì • | Skill ìµœì í™” |
| í˜¸í™˜ì„± ê¹¨ì§ | Medium | Low | ìˆœì°¨ ë§ˆì´ê·¸ë ˆì´ì…˜ | Feature flag |
| Documentation ë¶€ì¡± | Low | Medium | ë¬¸ì„œ ìš°ì„  ì‘ì„± | Template ì œê³µ |

### Mitigation Strategies

**1. Skills ë¡œë“œ ì‹¤íŒ¨ ë°©ì§€**:
- Phase 1ì—ì„œ ë‹¨ì¼ skillë¡œ ê°œë… ê²€ì¦ (PoC)
- Phase 2ì—ì„œ ë‹¨ì¼ agentë¡œ í†µí•© í…ŒìŠ¤íŠ¸
- Phase 3 ì „ì— ì™„ì „í•œ rollback plan ìˆ˜ë¦½

**2. Agent ë™ì‘ ë³€ê²½ ë°©ì§€**:
- ê¸°ì¡´ integration tests ìœ ì§€
- Skills ì¶”ê°€ ì „í›„ behavior ë¹„êµ í…ŒìŠ¤íŠ¸
- Agent output ë³€ê²½ ì‚¬í•­ ëª¨ë‹ˆí„°ë§

**3. Token ì¦ê°€ ë°©ì§€**:
- SKILL.mdëŠ” 300 tokens ì´í•˜ ìœ ì§€
- Supporting filesëŠ” contextual loading í™œìš©
- Agent bodyì—ì„œ ì¤‘ë³µ ì œê±° ì‹œ token ì¸¡ì •

---

## ğŸš€ Detailed Implementation Plan

### Phase 1: Skills ìƒì„± (Week 1)

**ëª©í‘œ**: Core skill ìƒì„± ë° ê°œë… ê²€ì¦ (PoC)

**Priority**: ğŸ”´ Critical

#### Task 1.1: spark-constitution Skill ìƒì„±

**ì†Œìš”ì‹œê°„**: 4ì‹œê°„

**ì„¸ë¶€ ì‘ì—…**:

1. **ë””ë ‰í† ë¦¬ ìƒì„±**:
```bash
mkdir -p spark-plugin/skills/spark-constitution/examples
cd spark-plugin/skills/spark-constitution/
```

2. **SKILL.md ì‘ì„±**:

**íŒŒì¼**: `spark-plugin/skills/spark-constitution/SKILL.md`

```yaml
---
name: spark-constitution
description: SPARK Constitution v1.2 - Agent behavior standards, quality gates, and work protocols. Use when implementing features following SPARK methodology, executing quality gates, or understanding SPARK agent workflows.
---
```

```markdown
# SPARK Constitution v1.2

## Overview
This skill provides the SPARK Constitution v1.2, which defines:
- Agent behavior standards
- Quality gate requirements
- Work protocols (EVIDENCE-BEFORE-REPORT, TEST-BEFORE-REPORT)
- Token efficiency guidelines

## When to Use
- When implementing features as a SPARK agent
- When executing quality gates
- When following SPARK protocols
- When creating new SPARK agents

## Supporting Documents
Available in this skill:
- `constitution-v1.2.md`: Full constitution text
- `quality-gates.md`: 8-step quality gate definitions
- `protocols.md`: Work protocols detailed specification
- `examples/`: Example implementations for each agent type

## Quick Reference

### Core Principles
1. **Evidence-Based Reporting**: All analysis includes file:line references
2. **Zero-Tolerance Quality**: Ruff 0, MyPy 0, Coverage 95%+
3. **Test-Before-Report**: Never report complete without test evidence
4. **Token Efficiency**: 90K safety protocol, progressive disclosure
5. **Adaptive Workflow**: Professional judgment over mechanical progression

### Quality Gates (8 Steps)
1. Syntax Validation (0 errors)
2. Type Checking (mypy --strict)
3. Linting (ruff --strict)
4. Security (OWASP compliance)
5. Test Coverage (95% unit / 85% integration)
6. Performance (O(n) verification)
7. Documentation (100% docstrings)
8. Integration (E2E passing)

### Protocols
- **EVIDENCE-BEFORE-REPORT**: 12+ evidence items (file:line)
- **TEST-BEFORE-REPORT**: Run tests, record results, include in report
- **PROJECT-CONTEXT-DISCOVERY**: Read standards BEFORE implementing

For complete details, see supporting documents.
```

**ì˜ˆìƒ í† í°**: ~300 tokens

3. **Supporting files ì‘ì„±**:

**íŒŒì¼**: `spark-plugin/skills/spark-constitution/constitution-v1.2.md`

```markdown
# SPARK Constitution v1.2

[.claude/SPARK_CONSTITUTION.md ë‚´ìš©ì„ ì—¬ê¸°ë¡œ ë³µì‚¬]

## Version History
- v1.2 (2025-11-XX): Current version
- v1.1 (2025-10-XX): Quality gates mandatory
- v1.0 (2025-08-XX): Initial constitution
```

**ì˜ˆìƒ í† í°**: ~1,200 tokens

**íŒŒì¼**: `spark-plugin/skills/spark-constitution/quality-gates.md`

```markdown
# SPARK Quality Gates

## 8-Step Quality Framework

### Gate 1: Syntax Validation
**Requirement**: 0 syntax errors
**Command**: `python3 -m py_compile <file>`
**Pass Criteria**: Exit code 0

### Gate 2: Type Checking
**Requirement**: 0 type errors
**Command**: `mypy --strict <file>`
**Pass Criteria**: 0 errors, 0 warnings

[... ë‚˜ë¨¸ì§€ gates ì •ì˜ ...]

## Execution Protocol

### Phase 5A: Metrics Recording
Record before/after metrics in current_task.json

### Phase 5B: Gates Execution (MANDATORY)
Execute spark_quality_gates.py and verify PASSED
```

**ì˜ˆìƒ í† í°**: ~600 tokens

**íŒŒì¼**: `spark-plugin/skills/spark-constitution/protocols.md`

```markdown
# SPARK Work Protocols

## EVIDENCE-BEFORE-REPORT
**Purpose**: Prevent hallucination, ensure concrete analysis

**Requirements**:
- Minimum 12 evidence items
- Each evidence: file:line reference
- Concrete, verifiable facts only

**Example**:
```
âŒ Bad: "Performance is slow"
âœ… Good: "API response time 2.3s (src/api.py:156)"
```

## TEST-BEFORE-REPORT
**Purpose**: Never report complete without verification

[... ë‚˜ë¨¸ì§€ protocols ì •ì˜ ...]
```

**ì˜ˆìƒ í† í°**: ~400 tokens

4. **Examples ì‘ì„±**:

**íŒŒì¼**: `spark-plugin/skills/spark-constitution/examples/implementer-example.md`

```markdown
# Implementer Protocol Example

## Correct Implementation Workflow

### Phase 0: Context Discovery
âœ… Read PROJECT_STANDARDS.md
âœ… Read ARCHITECTURE.md
âœ… Identify common/* modules

### Phase 4: Testing (CRITICAL)
âœ… Run: `pytest tests/ -v --tb=short`
âœ… Result: 58/58 passed (100%)
âœ… Record in report

### Phase 5B: Quality Gates
âœ… Execute: `python3 ~/.claude/hooks/spark_quality_gates.py`
âœ… Result: "Quality gates PASSED"

## Report Format

**Implementation Complete**

**Test Results**:
- Unit tests: 46/46 passed (100%) âœ…
- Integration tests: 12/12 passed (100%) âœ…
- Total: 58/58 passed âœ…

**Quality Results**:
- Ruff: 0 violations âœ…
- MyPy: 0 errors âœ…
- Coverage: 97% âœ…

âœ… All quality gates passed.
```

**ê²€ì¦ ë°©ë²•**:
```bash
# File ì¡´ì¬ í™•ì¸
ls -la spark-plugin/skills/spark-constitution/
# Expected: SKILL.md, constitution-v1.2.md, quality-gates.md, protocols.md, examples/

# Token ì¹´ìš´íŠ¸
wc -w spark-plugin/skills/spark-constitution/SKILL.md
# Expected: ~200-250 words (~300 tokens)

# YAML validation
python3 -c "
import yaml
with open('spark-plugin/skills/spark-constitution/SKILL.md') as f:
    content = f.read()
    frontmatter = content.split('---')[1]
    yaml.safe_load(frontmatter)
print('âœ… Valid YAML')
"
```

**ì„±ê³µ ê¸°ì¤€**:
- [ ] SKILL.md ìƒì„± ì™„ë£Œ (valid YAML frontmatter)
- [ ] 4ê°œ supporting files ìƒì„± ì™„ë£Œ
- [ ] Total tokens < 3K
- [ ] YAML validation í†µê³¼

#### Task 1.2: code-standards Skill ìƒì„±

**ì†Œìš”ì‹œê°„**: 3ì‹œê°„

**ì„¸ë¶€ ì‘ì—…**:

1. **ë””ë ‰í† ë¦¬ ìƒì„±**:
```bash
mkdir -p spark-plugin/skills/code-standards/
cd spark-plugin/skills/code-standards/
```

2. **SKILL.md ì‘ì„±**:

**íŒŒì¼**: `spark-plugin/skills/code-standards/SKILL.md`

```yaml
---
name: code-standards
description: Python code quality standards including testing requirements, documentation standards, and security best practices. Use when implementing features, writing tests, or ensuring code quality compliance.
---
```

```markdown
# Code Standards

## Overview
Python development standards for SPARK projects:
- Code quality (Ruff, MyPy, Black, isort)
- Testing requirements (coverage, patterns)
- Documentation standards (docstrings, README)
- Security practices (Bandit, input validation)

## When to Use
- Implementing new features
- Writing or reviewing code
- Creating tests
- Ensuring quality compliance

## Supporting Documents
- `python-standards.md`: Python code style and quality
- `testing-standards.md`: Test requirements and patterns
- `documentation-standards.md`: Doc requirements
- `security-standards.md`: Security checklist

## Quick Reference

### Code Quality
- Ruff: 0 violations (--strict mode)
- MyPy: 0 errors (--strict mode)
- Black: Format all code
- isort: Sort all imports

### Testing
- Unit coverage: 95%+
- Integration coverage: 85%+
- E2E: 100% critical paths
- All tests must pass (100%)

### Documentation
- Public functions: 100% docstrings
- Modules: README.md
- Complex logic: Inline comments
- API: Full documentation

For complete details, see supporting documents.
```

3. **Supporting files ì‘ì„±**:

**íŒŒì¼**: `spark-plugin/skills/code-standards/python-standards.md`

```markdown
# Python Code Standards

## Code Style
- Follow PEP 8
- Use Black (default config)
- Use isort (default config)
- Max line length: 88 (Black default)

## Type Hints
- All function signatures: Required
- All class attributes: Required
- Use Pydantic for data models
- Use typing module (List, Dict, Optional, etc.)

## Naming Conventions
- Functions: snake_case
- Classes: PascalCase
- Constants: UPPER_CASE
- Private: _leading_underscore

[... ë‚˜ë¨¸ì§€ standards ...]
```

**íŒŒì¼**: `spark-plugin/skills/code-standards/testing-standards.md`

```markdown
# Testing Standards

## Coverage Requirements
- Unit tests: 95%+ coverage
- Integration tests: 85%+ coverage
- E2E tests: 100% critical paths
- All tests: 100% pass rate

## Test Structure
- Use pytest framework
- One test file per module (test_module.py)
- One test class per class
- Descriptive test names (test_should_do_x_when_y)

## Test Patterns
- AAA pattern (Arrange, Act, Assert)
- Mocking: pytest-mock (minimal)
- Fixtures: conftest.py
- Parametrize: @pytest.mark.parametrize

[... ë‚˜ë¨¸ì§€ standards ...]
```

**ê²€ì¦ ë°©ë²•**:
```bash
# Skills ëª©ë¡ í™•ì¸
ls -la spark-plugin/skills/
# Expected: spark-constitution/, code-standards/

# Skill íŒŒì¼ ê²€ì¦
cat spark-plugin/skills/code-standards/SKILL.md | grep "^name:"
# Expected: name: code-standards
```

**ì„±ê³µ ê¸°ì¤€**:
- [ ] SKILL.md ìƒì„± ì™„ë£Œ
- [ ] 4ê°œ supporting files ìƒì„± ì™„ë£Œ
- [ ] Total tokens < 2K

#### Task 1.3: ê°œë… ê²€ì¦ (PoC)

**ì†Œìš”ì‹œê°„**: 2ì‹œê°„

**ëª©ì **: Skillsê°€ ì‹¤ì œë¡œ ë¡œë“œë˜ëŠ”ì§€ ê²€ì¦

**ì„¸ë¶€ ì‘ì—…**:

1. **í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ agent ìƒì„±**:

**íŒŒì¼**: `spark-plugin/agents/test-skill-agent.md` (ì„ì‹œ)

```yaml
---
name: test-skill-agent
description: Test agent for skills PoC
tools: Read, Write
skills: spark-constitution, code-standards
model: haiku
---
```

```markdown
# Test Agent

You are a test agent to verify skills loading.

When invoked, please:
1. Confirm you can see the spark-constitution skill
2. Confirm you can see the code-standards skill
3. List the quality gates from spark-constitution
4. Report success or failure
```

2. **í…ŒìŠ¤íŠ¸ ì‹¤í–‰**:

```bash
# Claude Codeì—ì„œ:
Task("test-skill-agent", "Verify that you can access spark-constitution and code-standards skills")
```

**ì˜ˆìƒ ê²°ê³¼**:
```
âœ… Skills loaded successfully
âœ… spark-constitution: 8 quality gates visible
âœ… code-standards: Python standards visible
âœ… Supporting files accessible
```

3. **ê²€ì¦ í›„ ì •ë¦¬**:
```bash
# í…ŒìŠ¤íŠ¸ ì„±ê³µ ì‹œ ì„ì‹œ agent ì‚­ì œ
rm spark-plugin/agents/test-skill-agent.md
```

**ì„±ê³µ ê¸°ì¤€**:
- [ ] Agentê°€ skillsë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œ
- [ ] Skillsì˜ ë‚´ìš©ì— ì ‘ê·¼ ê°€ëŠ¥
- [ ] Supporting files ì°¸ì¡° ê°€ëŠ¥

**Phase 1 ì™„ë£Œ ì¡°ê±´**:
- [ ] Task 1.1 ì™„ë£Œ (spark-constitution)
- [ ] Task 1.2 ì™„ë£Œ (code-standards)
- [ ] Task 1.3 ì™„ë£Œ (PoC ì„±ê³µ)
- [ ] Total: 2 skills, ~5K tokens, 100% functional

---

### Phase 2: Agent ì—…ë°ì´íŠ¸ - Pilot (Week 2)

**ëª©í‘œ**: ë‹¨ì¼ agent(implementer-spark)ì— skills í†µí•© ë° ì™„ì „ í…ŒìŠ¤íŠ¸

**Priority**: ğŸ”´ Critical

#### Task 2.1: implementer-spark ë°±ì—…

**ì†Œìš”ì‹œê°„**: 10ë¶„

```bash
# ë°±ì—… ìƒì„±
cp spark-plugin/agents/implementer-spark.md \
   spark-plugin/agents/implementer-spark.md.backup

# Git commit (rollback point)
git add spark-plugin/agents/implementer-spark.md.backup
git commit -m "backup: implementer-spark before skills integration"
```

#### Task 2.2: implementer-spark YAML ìˆ˜ì •

**ì†Œìš”ì‹œê°„**: 30ë¶„

**Before**:
```yaml
---
name: implementer-spark
description: Feature implementation specialist ensuring zero-defect code delivery with comprehensive testing. Use for API endpoints, authentication systems, database layers, UI components, and microservices where structural integrity and test validation are critical.
tools: Bash, Glob, Grep, LS, Read, Edit, MultiEdit, Write, WebFetch, TodoWrite, WebSearch, mcp__sequential-thinking__sequentialthinking, mcp__context7__resolve-library-id, mcp__context7__get-library-docs, mcp__time__get_current_time
model: sonnet
color: pink
---
```

**After**:
```yaml
---
name: implementer-spark
description: Feature implementation specialist ensuring zero-defect code delivery with comprehensive testing. Use for API endpoints, authentication systems, database layers, UI components, and microservices where structural integrity and test validation are critical.
tools: Bash, Glob, Grep, LS, Read, Edit, MultiEdit, Write, WebFetch, TodoWrite, WebSearch, mcp__sequential-thinking__sequentialthinking, mcp__context7__resolve-library-id, mcp__context7__get-library-docs, mcp__time__get_current_time
skills: spark-constitution, code-standards
model: sonnet
color: pink
---
```

**ë³€ê²½ì‚¬í•­**: `skills: spark-constitution, code-standards` í•œ ì¤„ ì¶”ê°€

#### Task 2.3: implementer-spark Body ì¤‘ë³µ ì œê±°

**ì†Œìš”ì‹œê°„**: 2ì‹œê°„

**í˜„ì¬ íŒŒì¼ ë¶„ì„**:
```bash
# Constitution ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰
grep -n "Constitution\|Quality Gates\|EVIDENCE-BEFORE-REPORT\|TEST-BEFORE-REPORT" \
     spark-plugin/agents/implementer-spark.md
```

**ì œê±°í•  ì„¹ì…˜**:

1. **Constitution ì¸ìš©ë¬¸** (í˜„ì¬ ~200 tokens):
```markdown
## SPARK Constitution Compliance
[ì´ ì„¹ì…˜ ì „ì²´ ì œê±° - skillsë¡œ ëŒ€ì²´]
```

2. **Quality Gates ìƒì„¸** (í˜„ì¬ ~600 tokens):
```markdown
## Quality Gates (8 Steps)
[ì´ ì„¹ì…˜ ê°„ì†Œí™” - skills ì°¸ì¡°ë¡œ ëŒ€ì²´]
```

**Before** (ì œê±° ëŒ€ìƒ):
```markdown
## Quality Gates (8 Steps)

### Gate 1: Syntax Validation
**Requirement**: 0 syntax errors
**Command**: `python3 -m py_compile <file>`
**Pass Criteria**: Exit code 0

### Gate 2: Type Checking
[... ì „ì²´ 8 gates ìƒì„¸ ì„¤ëª… ...]
```

**After** (ê°„ì†Œí™”):
```markdown
## Quality Standards

All implementations must meet SPARK Constitution v1.2 standards:
- Zero-tolerance quality (Ruff 0, MyPy 0)
- 95%+ test coverage
- 100% tests passing
- Complete documentation

See `spark-constitution` skill for complete quality gates and protocols.
```

**í† í° ì ˆì•½**: ~800 tokens â†’ ~100 tokens = **700 tokens ì ˆì•½**

#### Task 2.4: í†µí•© í…ŒìŠ¤íŠ¸

**ì†Œìš”ì‹œê°„**: 1ì‹œê°„

**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**:

1. **Simple feature implementation**:
```bash
Task("implementer-spark", """
Implement a simple calculator function:
- add(a, b) -> int
- subtract(a, b) -> int
- With tests (100% coverage)
- With type hints
- With docstrings
""")
```

**ì˜ˆìƒ ê²°ê³¼**:
- Agentê°€ skillsë¥¼ ìë™ ë¡œë“œ
- Quality gates ì°¸ì¡°í•˜ì—¬ ì‘ì—…
- Test results í¬í•¨ëœ ë³´ê³ ì„œ
- 0 violations

2. **Quality gates ê²€ì¦**:
```bash
# Agent ì™„ë£Œ í›„
cat ~/.claude/workflows/current_task.json | jq '.quality'
```

**ì˜ˆìƒ ê²°ê³¼**:
```json
{
  "violations_total": 0,
  "can_proceed": true,
  "step_6_testing": {
    "ruff_violations": 0,
    "mypy_errors": 0,
    "coverage": 0.97
  }
}
```

3. **Token ì¸¡ì •**:
```bash
# Agent definition í† í° ì¹´ìš´íŠ¸
python3 scripts/count_agent_tokens.py spark-plugin/agents/implementer-spark.md
```

**ì˜ˆìƒ ê²°ê³¼**: ~2.5K tokens (ì´ì „: ~3.9K)

**ì„±ê³µ ê¸°ì¤€**:
- [ ] Skills ìë™ ë¡œë“œ í™•ì¸
- [ ] Quality gates ì •ìƒ ì‘ë™
- [ ] Tests 100% pass
- [ ] Token 30%+ ì ˆê°
- [ ] ê¸°ì¡´ ê¸°ëŠ¥ ì •ìƒ ì‘ë™

**Phase 2 ì™„ë£Œ ì¡°ê±´**:
- [ ] implementer-spark ì—…ë°ì´íŠ¸ ì™„ë£Œ
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] Token ì ˆê° í™•ì¸ (3.9K â†’ 2.5K)
- [ ] ë°±ì—… íŒŒì¼ ìœ ì§€ (rollback ê°€ëŠ¥)

---

### Phase 3: Agent ì—…ë°ì´íŠ¸ - ì „ì²´ (Week 3-4)

**ëª©í‘œ**: ë‚˜ë¨¸ì§€ 20ê°œ agentsì— skills í†µí•©

**Priority**: ğŸŸ¡ High

#### Task 3.1: Core 4 Agents ì—…ë°ì´íŠ¸

**ëŒ€ìƒ**: analyzer-spark, tester-spark, documenter-spark, designer-spark (4ê°œ)

**ì†Œìš”ì‹œê°„**: 6ì‹œê°„ (ê° 1.5ì‹œê°„)

**ê° Agentë³„ ì‘ì—…**:

1. **ë°±ì—…**:
```bash
for agent in analyzer-spark tester-spark documenter-spark designer-spark; do
  cp spark-plugin/agents/${agent}.md spark-plugin/agents/${agent}.md.backup
done
```

2. **YAML ì—…ë°ì´íŠ¸**:

**analyzer-spark**:
```yaml
skills: spark-constitution
```

**tester-spark**:
```yaml
skills: spark-constitution, testing-standards
```

**documenter-spark**:
```yaml
skills: spark-constitution, documentation-standards
```

**designer-spark**:
```yaml
skills: spark-constitution, architecture-patterns
```

3. **Body ì¤‘ë³µ ì œê±°** (ê° agentë³„ ~700 tokens ì ˆì•½)

4. **ê°œë³„ í…ŒìŠ¤íŠ¸**:
```bash
# analyzer-spark
Task("analyzer-spark", "Analyze src/main.py for performance bottlenecks")

# tester-spark
Task("tester-spark", "Create comprehensive tests for src/auth.py")

# documenter-spark
Task("documenter-spark", "Document the authentication API")

# designer-spark
Task("designer-spark", "Design architecture for caching layer")
```

**ì„±ê³µ ê¸°ì¤€** (ê° agent):
- [ ] Skills ìë™ ë¡œë“œ
- [ ] ê¸°ì¡´ ê¸°ëŠ¥ ì •ìƒ ì‘ë™
- [ ] Token 30%+ ì ˆê°

#### Task 3.2: Support 2 Agents ì—…ë°ì´íŠ¸

**ëŒ€ìƒ**: qc-spark (1ê°œ, designer-sparkëŠ” ìœ„ì—ì„œ ì™„ë£Œ)

**ì†Œìš”ì‹œê°„**: 1.5ì‹œê°„

**qc-spark**:
```yaml
skills: spark-constitution, code-standards
```

#### Task 3.3: Team 15 Agents ì—…ë°ì´íŠ¸

**ëŒ€ìƒ**: team[1-5]-{implementer,tester,documenter}-spark (15ê°œ)

**ì†Œìš”ì‹œê°„**: 4ì‹œê°„

**ì¼ê´„ ì‘ì—… ìŠ¤í¬ë¦½íŠ¸**:

**íŒŒì¼**: `scripts/update_team_agents.sh`

```bash
#!/bin/bash
# Team agents ì¼ê´„ ì—…ë°ì´íŠ¸

TEAMS="team1 team2 team3 team4 team5"
ROLES="implementer tester documenter"

for team in $TEAMS; do
  for role in $ROLES; do
    agent="${team}-${role}-spark"
    file="spark-plugin/agents/${agent}.md"

    echo "Updating ${agent}..."

    # ë°±ì—…
    cp "${file}" "${file}.backup"

    # YAML frontmatter ì—…ë°ì´íŠ¸
    # skills í•„ë“œ ì¶”ê°€ (roleì— ë”°ë¼ ë‹¤ë¦„)
    if [ "$role" == "implementer" ]; then
      skills="spark-constitution, code-standards"
    elif [ "$role" == "tester" ]; then
      skills="spark-constitution, testing-standards"
    else
      skills="spark-constitution"
    fi

    # sedë¡œ YAMLì— skills ì¶”ê°€ (color: pink ë‹¤ìŒ ì¤„ì—)
    sed -i.tmp "/^color: pink$/a\\
skills: ${skills}" "${file}"

    rm "${file}.tmp"

    echo "âœ… ${agent} updated"
  done
done

echo "âœ… All 15 team agents updated"
```

**ì‹¤í–‰**:
```bash
chmod +x scripts/update_team_agents.sh
./scripts/update_team_agents.sh
```

**ê²€ì¦**:
```bash
# ëª¨ë“  team agentsì˜ skills í•„ë“œ í™•ì¸
grep -h "^skills:" spark-plugin/agents/team*.md | sort | uniq -c
```

**ì˜ˆìƒ ê²°ê³¼**:
```
   5 skills: spark-constitution
   5 skills: spark-constitution, code-standards
   5 skills: spark-constitution, testing-standards
```

#### Task 3.4: Parallel Execution í…ŒìŠ¤íŠ¸

**ì†Œìš”ì‹œê°„**: 1ì‹œê°„

**í…ŒìŠ¤íŠ¸**:
```bash
/multi-implement "task1: simple calc,task2: string utils,task3: file reader"
```

**ê²€ì¦**:
- [ ] 3ê°œ team agents ë™ì‹œ ì‹¤í–‰
- [ ] ê° agentê°€ skills ë¡œë“œ
- [ ] ëª¨ë“  ì‘ì—… ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ

**Phase 3 ì™„ë£Œ ì¡°ê±´**:
- [ ] 20ê°œ agents ì—…ë°ì´íŠ¸ ì™„ë£Œ (implementer-sparkëŠ” Phase 2ì—ì„œ ì™„ë£Œ)
- [ ] ëª¨ë“  agents ê°œë³„ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] Parallel execution í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] Total token ì ˆê°: ~25K â†’ ~1.5K (94%)

---

### Phase 4: Additional Skills & Documentation (Week 4-5)

**ëª©í‘œ**: ì¶”ê°€ skills ìƒì„± ë° ì™„ì „í•œ ë¬¸ì„œí™”

**Priority**: ğŸŸ¢ Medium

#### Task 4.1: testing-standards Skill ìƒì„±

**ì†Œìš”ì‹œê°„**: 2ì‹œê°„

**íŒŒì¼**: `spark-plugin/skills/testing-standards/SKILL.md`

```yaml
---
name: testing-standards
description: Testing requirements, patterns, and best practices for SPARK projects. Includes coverage requirements, test structure, mocking patterns, and E2E testing. Use when creating tests or ensuring testing quality.
---
```

**Supporting files**:
- `testing-requirements.md`: Coverage, pass rate
- `testing-patterns.md`: AAA pattern, fixtures
- `mocking-guidelines.md`: When/how to mock
- `e2e-testing.md`: E2E patterns

#### Task 4.2: documentation-standards Skill ìƒì„±

**ì†Œìš”ì‹œê°„**: 2ì‹œê°„

**íŒŒì¼**: `spark-plugin/skills/documentation-standards/SKILL.md`

```yaml
---
name: documentation-standards
description: Documentation requirements for code, APIs, and architecture. Includes docstring formats, README templates, API documentation standards, and architecture documentation patterns. Use when documenting code or creating technical documentation.
---
```

**Supporting files**:
- `docstring-formats.md`: Google/Numpy style
- `readme-template.md`: Standard README
- `api-documentation.md`: OpenAPI/Swagger
- `architecture-docs.md`: ADR templates

#### Task 4.3: architecture-patterns Skill ìƒì„±

**ì†Œìš”ì‹œê°„**: 3ì‹œê°„

**íŒŒì¼**: `spark-plugin/skills/architecture-patterns/SKILL.md`

```yaml
---
name: architecture-patterns
description: Software architecture patterns, design patterns, and best practices. Includes layered architecture, microservices patterns, API design, and scalability patterns. Use when designing system architecture or making architectural decisions.
---
```

**Supporting files**:
- `layered-architecture.md`: Layer patterns
- `api-patterns.md`: RESTful, GraphQL
- `scalability-patterns.md`: Caching, sharding
- `templates/`: Code templates

#### Task 4.4: Documentation ì‘ì„±

**ì†Œìš”ì‹œê°„**: 4ì‹œê°„

**ë¬¸ì„œ ëª©ë¡**:

1. **spark-plugin/README.md**:
```markdown
# SPARK Plugin v4.3

## Installation
## Usage
## Agent List
## Skills List
## Commands List
## Contributing
```

2. **spark-plugin/SKILLS.md**:
```markdown
# SPARK Skills Reference

## Available Skills
- spark-constitution
- code-standards
- testing-standards
- documentation-standards
- architecture-patterns

## How to Use Skills
## Creating New Skills
```

3. **spark-plugin/MIGRATION.md**:
```markdown
# Migration Guide: Skills Integration

## For Existing SPARK Users
## Changes in v4.3
## Breaking Changes (none)
## Update Process
```

4. **examples/**: Usage examples
```
spark-plugin/examples/
â”œâ”€â”€ using-implementer-spark.md
â”œâ”€â”€ using-analyzer-spark.md
â”œâ”€â”€ parallel-execution.md
â””â”€â”€ creating-custom-skills.md
```

**Phase 4 ì™„ë£Œ ì¡°ê±´**:
- [ ] 3ê°œ ì¶”ê°€ skills ìƒì„± ì™„ë£Œ
- [ ] 4ê°œ ì£¼ìš” ë¬¸ì„œ ì‘ì„± ì™„ë£Œ
- [ ] 4ê°œ ì˜ˆì‹œ ë¬¸ì„œ ì‘ì„± ì™„ë£Œ
- [ ] Total: 5 skills, ì™„ì „í•œ ë¬¸ì„œí™”

---

### Phase 5: Hooks ì´ë™ & Final Integration (Week 5)

**ëª©í‘œ**: Hooksë¥¼ pluginìœ¼ë¡œ ì´ë™í•˜ê³  ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸

**Priority**: ğŸŸ¢ Medium

#### Task 5.1: Hooks ì´ë™

**ì†Œìš”ì‹œê°„**: 1ì‹œê°„

```bash
# ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p spark-plugin/hooks

# Hooks ì´ë™
mv .claude/hooks/spark_persona_router.py spark-plugin/hooks/
mv .claude/hooks/spark_quality_gates.py spark-plugin/hooks/

# Symlink ìƒì„± (í˜¸í™˜ì„± ìœ ì§€)
ln -s ../../spark-plugin/hooks/spark_persona_router.py .claude/hooks/
ln -s ../../spark-plugin/hooks/spark_quality_gates.py .claude/hooks/

# ê²€ì¦
ls -la .claude/hooks/
ls -la spark-plugin/hooks/
```

**ê²€ì¦**:
```bash
# Hooksê°€ ì—¬ì „íˆ ì‘ë™í•˜ëŠ”ì§€
python3 .claude/hooks/spark_quality_gates.py <<< '{"subagent":"implementer-spark","self_check":true}'
# Expected: "Quality gates framework ready"
```

#### Task 5.2: Plugin Metadata ì—…ë°ì´íŠ¸

**ì†Œìš”ì‹œê°„**: 30ë¶„

**íŒŒì¼**: `spark-plugin/.claude-plugin/plugin.json`

```json
{
  "name": "spark-agents",
  "description": "SPARK v4.3 - 21 specialized AI agents with skills-based knowledge management, zero-tolerance quality gates, and 95.5% token reduction",
  "version": "4.3.1",
  "author": {
    "name": "Jason (Jaesun23)",
    "email": "jaesun23@users.noreply.github.com"
  },
  "changelog": {
    "4.3.1": "Skills integration: 94% token reduction, single source of truth",
    "4.3.0": "Initial plugin release with 21 agents"
  },
  "requires": {
    "claude-code": ">=1.0.0"
  },
  "contents": {
    "agents": 21,
    "commands": 12,
    "skills": 5,
    "hooks": 2
  }
}
```

#### Task 5.3: í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸

**ì†Œìš”ì‹œê°„**: 3ì‹œê°„

**í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤**:

**Scenario 1: Single Agent**
```bash
# ê° core agent í…ŒìŠ¤íŠ¸
/spark-implement "simple feature"
/spark-analyze "performance check"
/spark-test "module testing"
/spark-design "system architecture"
```

**Scenario 2: Pipeline**
```bash
# Full workflow
/spark-launch "user profile feature"
# Expected: design â†’ implement â†’ test â†’ document â†’ commit
```

**Scenario 3: Parallel**
```bash
# 5 tasks simultaneously
/multi-implement "task1,task2,task3,task4,task5"
```

**Scenario 4: Skills Update**
```bash
# Constitution ìˆ˜ì •
echo "New principle: X" >> spark-plugin/skills/spark-constitution/constitution-v1.2.md

# Agent ì¬ì‹¤í–‰ (constitution ìë™ ë°˜ì˜ í™•ì¸)
/spark-implement "test feature"
# Agentê°€ ìƒˆ principle Xë¥¼ ì°¸ì¡°í•˜ëŠ”ì§€ í™•ì¸
```

**ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] 21ê°œ agents ëª¨ë‘ ì •ìƒ ì‘ë™
- [ ] 12ê°œ commands ëª¨ë‘ ì •ìƒ ì‘ë™
- [ ] 5ê°œ skills ëª¨ë‘ ìë™ ë¡œë“œ
- [ ] Hooks ì •ìƒ ì‘ë™
- [ ] Quality gates í†µê³¼
- [ ] Parallel execution ì •ìƒ
- [ ] Skills ì—…ë°ì´íŠ¸ ìë™ ë°˜ì˜

#### Task 5.4: Performance Measurement

**ì†Œìš”ì‹œê°„**: 2ì‹œê°„

**ì¸¡ì • í•­ëª©**:

1. **Token Efficiency**:

**ìŠ¤í¬ë¦½íŠ¸**: `scripts/measure_tokens.py`

```python
#!/usr/bin/env python3
"""Token efficiency measurement."""

import os
import tiktoken

def count_tokens(text):
    enc = tiktoken.encoding_for_model("gpt-4")
    return len(enc.encode(text))

# Before (single agent with embedded constitution)
with open('spark-plugin/agents/implementer-spark.md.backup') as f:
    before_tokens = count_tokens(f.read())

# After (agent with skills reference)
with open('spark-plugin/agents/implementer-spark.md') as f:
    after_tokens = count_tokens(f.read())

print(f"Before: {before_tokens} tokens")
print(f"After:  {after_tokens} tokens")
print(f"Saved:  {before_tokens - after_tokens} tokens ({(before_tokens - after_tokens) / before_tokens * 100:.1f}%)")

# Skills overhead
skills_tokens = 0
for skill in ['spark-constitution', 'code-standards']:
    with open(f'spark-plugin/skills/{skill}/SKILL.md') as f:
        skills_tokens += count_tokens(f.read())

print(f"\nSkills overhead: {skills_tokens} tokens")
print(f"Net savings: {before_tokens - (after_tokens + skills_tokens)} tokens")
```

**ì‹¤í–‰**:
```bash
python3 scripts/measure_tokens.py
```

**ì˜ˆìƒ ê²°ê³¼**:
```
Before: 3,900 tokens
After:  2,500 tokens
Saved:  1,400 tokens (35.9%)

Skills overhead: 600 tokens
Net savings: 800 tokens (20.5%)
```

2. **Duplication Analysis**:

**ìŠ¤í¬ë¦½íŠ¸**: `scripts/detect_duplicates.py`

```python
#!/usr/bin/env python3
"""Detect duplicate content across agents."""

import os
from collections import Counter
import difflib

def get_agent_files():
    return [f for f in os.listdir('spark-plugin/agents/')
            if f.endswith('.md') and not f.endswith('.backup')]

def extract_sections(content):
    """Extract major sections from markdown."""
    sections = {}
    current_section = None
    current_content = []

    for line in content.split('\n'):
        if line.startswith('## '):
            if current_section:
                sections[current_section] = '\n'.join(current_content)
            current_section = line[3:].strip()
            current_content = []
        else:
            current_content.append(line)

    if current_section:
        sections[current_section] = '\n'.join(current_content)

    return sections

# Compare all agents
duplicates = []
agents = get_agent_files()

for i, agent1 in enumerate(agents):
    with open(f'spark-plugin/agents/{agent1}') as f:
        sections1 = extract_sections(f.read())

    for agent2 in agents[i+1:]:
        with open(f'spark-plugin/agents/{agent2}') as f:
            sections2 = extract_sections(f.read())

        # Find similar sections
        for sec1, content1 in sections1.items():
            for sec2, content2 in sections2.items():
                similarity = difflib.SequenceMatcher(None, content1, content2).ratio()
                if similarity > 0.8:  # 80% similar
                    duplicates.append({
                        'agent1': agent1,
                        'agent2': agent2,
                        'section1': sec1,
                        'section2': sec2,
                        'similarity': similarity
                    })

print(f"Found {len(duplicates)} duplicate sections")
for dup in duplicates[:10]:  # Show first 10
    print(f"  {dup['agent1']}::{dup['section1']} â‰ˆ {dup['agent2']}::{dup['section2']} ({dup['similarity']:.1%})")
```

**ì‹¤í–‰**:
```bash
python3 scripts/detect_duplicates.py
```

**ì˜ˆìƒ ê²°ê³¼**:
```
Found 0 duplicate sections
âœ… No duplicates detected
```

**Phase 5 ì™„ë£Œ ì¡°ê±´**:
- [ ] Hooks ì´ë™ ì™„ë£Œ ë° ì •ìƒ ì‘ë™
- [ ] Plugin metadata ì—…ë°ì´íŠ¸
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ëª¨ë‘ í†µê³¼
- [ ] Token ì ˆê° ì¸¡ì • ì™„ë£Œ (ëª©í‘œ: 30%+)
- [ ] ì¤‘ë³µ ì œê±° í™•ì¸ (ëª©í‘œ: 0 duplicates)

---

## ğŸ”„ Rollback Plan

### When to Rollback

**Trigger Conditions**:
1. Skills ë¡œë“œ ì‹¤íŒ¨ (agentsê°€ skillsë¥¼ ë¡œë“œí•˜ì§€ ëª»í•¨)
2. Agent ë™ì‘ ë³€ê²½ (ê¸°ì¡´ outputê³¼ ë‹¤ë¥¸ ê²°ê³¼)
3. Quality gates ì‹¤íŒ¨ ì¦ê°€ (ì´ì „ë³´ë‹¤ ì‹¤íŒ¨ìœ¨ ë†’ìŒ)
4. Critical bugs (production blocking issues)

### Rollback Procedure

#### Option 1: Individual Agent Rollback

**Scenario**: íŠ¹ì • agentë§Œ ë¬¸ì œ ë°œìƒ

```bash
# ë°±ì—…ì—ì„œ ë³µì›
cp spark-plugin/agents/implementer-spark.md.backup \
   spark-plugin/agents/implementer-spark.md

# Gitì—ì„œ ë³µì›
git checkout spark-plugin/agents/implementer-spark.md

# ê²€ì¦
Task("implementer-spark", "simple test")
```

#### Option 2: Phase Rollback

**Scenario**: ì „ì²´ Phaseê°€ ë¬¸ì œ

```bash
# Phase 3 ì „ì²´ rollback (20 agents)
git checkout <phase-2-commit-hash> -- spark-plugin/agents/

# ê²€ì¦
/spark-implement "test"
/spark-analyze "test"
```

#### Option 3: Full Rollback

**Scenario**: Skills ì‹œìŠ¤í…œ ì „ì²´ ë¬¸ì œ

```bash
# Skills integration ì „ì²´ rollback
git checkout <before-skills-commit-hash>

# ë˜ëŠ” íƒœê·¸ ì‚¬ìš©
git tag skills-integration-start  # Phase 1 ì‹œì‘ ì „
git checkout skills-integration-start
```

### Rollback Verification

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] All 21 agents ì •ìƒ ì‘ë™
- [ ] All 12 commands ì •ìƒ ì‘ë™
- [ ] Quality gates í†µê³¼
- [ ] Integration tests í†µê³¼

### Prevention Measures

1. **Git Tags**: ê° Phase ì‹œì‘/ì™„ë£Œ ì‹œ íƒœê·¸
```bash
git tag skills-phase-1-start
git tag skills-phase-1-complete
git tag skills-phase-2-start
# ...
```

2. **Backups**: ëª¨ë“  íŒŒì¼ ë³€ê²½ ì „ .backup ìƒì„±

3. **Progressive Rollout**: Phaseë³„ ìˆœì°¨ ì§„í–‰ (í•œ ë²ˆì— ì „ì²´ X)

4. **Testing**: ê° Phase ì™„ë£Œ í›„ ê²€ì¦ í›„ ë‹¤ìŒ Phase

---

## ğŸ“Š Post-Implementation

### Phase 6: Monitoring & Optimization (Ongoing)

#### Task 6.1: Usage Monitoring

**Metrics to Track**:

1. **Token Efficiency**:
```bash
# Weekly measurement
python3 scripts/measure_tokens.py > reports/tokens_$(date +%Y%m%d).txt
```

2. **Skills Usage**:
```bash
# Agent invocations per skill
grep "skills:" ~/.claude/logs/*.log | sort | uniq -c
```

3. **Quality Gates**:
```bash
# Pass/fail rate
grep "Quality gates" ~/.claude/logs/*.log | grep -c "PASSED"
grep "Quality gates" ~/.claude/logs/*.log | grep -c "FAILED"
```

#### Task 6.2: Skills Refinement

**Monthly Reviews**:
- Review skills usage statistics
- Identify unused supporting files (can be removed)
- Identify frequently accessed content (optimize for tokens)
- Update based on agent feedback

#### Task 6.3: Documentation Maintenance

**Quarterly Updates**:
- Update examples based on real usage
- Add new FAQs based on user questions
- Refine skills descriptions for better triggers
- Version updates (constitution v1.3, etc.)

### Success Metrics Dashboard

**Target Metrics** (3 months post-implementation):

| Metric | Target | Measurement |
|--------|--------|-------------|
| Token reduction | 30%+ | Weekly script |
| Duplicate content | 0 sections | Monthly scan |
| Constitution updates | 1 file only | Manual verification |
| Skills load success | 99%+ | Log analysis |
| Quality gates pass | 95%+ | Log analysis |
| Agent satisfaction | 4.5/5 | Survey (if team) |

**Monitoring Schedule**:
- Daily: Critical errors (skills load failures)
- Weekly: Token efficiency, quality gates
- Monthly: Duplicate detection, skills usage
- Quarterly: Full review, documentation updates

---

## ğŸ“‹ Appendix

### A. File Templates

#### A1. Skill Template

**File**: `templates/SKILL_TEMPLATE.md`

```yaml
---
name: skill-name
description: Brief description of what this skill does and when to use it (include triggers). Max 1024 chars.
allowed-tools: Read, Write  # Optional: restrict tools
---
```

```markdown
# Skill Name

## Overview
What this skill provides...

## When to Use
- Trigger condition 1
- Trigger condition 2
- Trigger condition 3

## Supporting Documents
- `document1.md`: Description
- `document2.md`: Description
- `examples/`: Example files

## Quick Reference
[Key information that agents need most frequently]

### Section 1
Content...

### Section 2
Content...

For complete details, see supporting documents.
```

#### A2. Agent Update Template

**File**: `templates/AGENT_UPDATE_TEMPLATE.md`

```yaml
---
name: agent-name
description: [Keep existing]
tools: [Keep existing]
skills: spark-constitution, [additional-skills]  # â† ADD THIS LINE
model: [Keep existing]
color: [Keep existing]
---
```

```markdown
# Agent Name

[Keep existing content but REMOVE duplicate sections:]

âŒ REMOVE:
- ## SPARK Constitution (duplicate of skill)
- ## Quality Gates (detailed version, duplicate of skill)
- ## Protocols (duplicate of skill)

âœ… KEEP:
- ## Core Identity & Traits (agent-specific)
- ## Behavior Protocol (agent-specific)
- ## Professional Workflow (agent-specific)
- Quick reference to skills (not full duplication)

âœ… ADD:
- Reference to skills in relevant sections
```

### B. Checklists

#### B1. Pre-Phase Checklist

```markdown
Before starting each phase:
- [ ] Previous phase 100% complete
- [ ] All tests passing
- [ ] Git committed (clean state)
- [ ] Backups created
- [ ] Time allocated
- [ ] Dependencies verified
```

#### B2. Post-Phase Checklist

```markdown
After completing each phase:
- [ ] All tasks completed
- [ ] Tests executed and passing
- [ ] Metrics measured
- [ ] Documentation updated
- [ ] Git committed with tag
- [ ] Rollback tested
- [ ] Stakeholder notified
```

#### B3. Agent Update Checklist

```markdown
For each agent update:
- [ ] Backup created (.backup file)
- [ ] YAML skills field added
- [ ] Duplicate content removed
- [ ] Token count measured
- [ ] Individual test executed
- [ ] Quality gates verified
- [ ] Git committed
```

### C. Scripts

#### C1. Batch Agent Update Script

**File**: `scripts/batch_update_agents.sh`

```bash
#!/bin/bash
# Batch update multiple agents with skills

set -e  # Exit on error

AGENTS=("$@")
if [ ${#AGENTS[@]} -eq 0 ]; then
  echo "Usage: $0 agent1 agent2 agent3..."
  exit 1
fi

for agent in "${AGENTS[@]}"; do
  file="spark-plugin/agents/${agent}.md"

  if [ ! -f "$file" ]; then
    echo "âŒ File not found: $file"
    continue
  fi

  echo "Processing ${agent}..."

  # 1. Backup
  cp "$file" "${file}.backup"
  echo "  âœ… Backup created"

  # 2. Add skills field (after color: pink)
  # This is a placeholder - actual implementation needs agent-specific skills
  sed -i.tmp "/^color: pink$/a\\
skills: spark-constitution" "$file"
  rm "${file}.tmp"
  echo "  âœ… Skills field added"

  # 3. Verify YAML
  python3 -c "
import yaml
with open('$file') as f:
    content = f.read()
    frontmatter = content.split('---')[1]
    yaml.safe_load(frontmatter)
print('  âœ… YAML valid')
"

  # 4. Token count
  tokens=$(python3 scripts/count_agent_tokens.py "$file")
  echo "  â„¹ï¸  Tokens: ${tokens}"

  echo "âœ… ${agent} updated successfully"
  echo ""
done

echo "ğŸ‰ All agents updated!"
```

**Usage**:
```bash
./scripts/batch_update_agents.sh analyzer-spark tester-spark documenter-spark
```

#### C2. Token Counter Script

**File**: `scripts/count_agent_tokens.py`

```python
#!/usr/bin/env python3
"""Count tokens in agent definition file."""

import sys
import tiktoken

def count_tokens(file_path):
    with open(file_path) as f:
        content = f.read()

    enc = tiktoken.encoding_for_model("gpt-4")
    tokens = len(enc.encode(content))

    return tokens

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python3 count_agent_tokens.py <file>")
        sys.exit(1)

    file_path = sys.argv[1]
    tokens = count_tokens(file_path)

    print(f"{tokens}")
```

**Usage**:
```bash
python3 scripts/count_agent_tokens.py spark-plugin/agents/implementer-spark.md
# Output: 2534
```

### D. Decision Log

| Date | Decision | Rationale | Impact |
|------|----------|-----------|--------|
| 2025-12-03 | Use skills system | 94% token reduction | High |
| TBD | Skill versioning | Support v1.2, v1.3 | Medium |
| TBD | Hooks location | Plugin self-contained | Low |

### E. Lessons Learned

**To be updated during implementation**:

- Phase 1: [Learnings]
- Phase 2: [Learnings]
- Phase 3: [Learnings]
- Phase 4: [Learnings]
- Phase 5: [Learnings]

---

## ğŸ“ Using This Plan as a Reference

### For Other Projects

**ì´ ê³„íšì„ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì— ì ìš©í•  ë•Œ**:

1. **Executive Summary ì‘ì„±**:
   - 3ì¤„ë¡œ í”„ë¡œì íŠ¸ ìš”ì•½
   - ì¸¡ì • ê°€ëŠ¥í•œ ëª©í‘œ (ìˆ«ìë¡œ!)
   - ì˜ˆìƒ ì„±ê³¼

2. **í˜„ì¬ ìƒíƒœ vs ëª©í‘œ ìƒíƒœ**:
   - êµ¬ì²´ì ì¸ íŒŒì¼ êµ¬ì¡° ë¹„êµ
   - ì¸¡ì • ê°€ëŠ¥í•œ ê°œì„  ì§€í‘œ
   - Before/After ëª…í™•íˆ

3. **Success Criteria**:
   - ê¸°ëŠ¥ì , ì„±ëŠ¥ì , ìœ ì§€ë³´ìˆ˜ì  ì„±ê³µ ì •ì˜
   - ê°ê° ì¸¡ì • ë°©ë²• í¬í•¨
   - Pass/Fail ëª…í™•íˆ

4. **Risk Assessment**:
   - Risk matrix (Impact Ã— Probability)
   - ê° riskë³„ mitigation strategy
   - Contingency plan

5. **Detailed Plan**:
   - Task breakdown (Phase â†’ Task â†’ Subtask)
   - ê° taskë³„:
     - ì†Œìš”ì‹œê°„ ì¶”ì •
     - êµ¬ì²´ì  íŒŒì¼ ê²½ë¡œ
     - ì½”ë“œ ì˜ˆì‹œ
     - ê²€ì¦ ë°©ë²•
     - ì„±ê³µ ê¸°ì¤€

6. **Rollback Plan**:
   - When to rollback (trigger conditions)
   - How to rollback (step-by-step)
   - Verification after rollback

7. **Post-Implementation**:
   - Monitoring metrics
   - Maintenance schedule
   - Success dashboard

### Key Principles

**ì´ ê³„íšì˜ í•µì‹¬ ì›ì¹™**:

1. **ì¸¡ì • ê°€ëŠ¥**: ëª¨ë“  ëª©í‘œì™€ ì„±ê³µ ê¸°ì¤€ì´ ìˆ«ìë¡œ ì¸¡ì • ê°€ëŠ¥
2. **êµ¬ì²´ì **: "ê°œì„ í•œë‹¤" âŒ â†’ "3.9Kâ†’2.5Kë¡œ 36% ê°œì„ " âœ…
3. **ì‹¤í–‰ ê°€ëŠ¥**: ê° taskê°€ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ êµ¬ì²´ì 
4. **ê²€ì¦ ê°€ëŠ¥**: ê° ë‹¨ê³„ë§ˆë‹¤ ê²€ì¦ ë°©ë²• ëª…ì‹œ
5. **ì•ˆì „ì„±**: ë°±ì—…, rollback, ë‹¨ê³„ì  ì§„í–‰

---

**Document Version**: 1.0
**Last Updated**: 2025-12-03
**Author**: 2í˜¸ (Number Two)
**Status**: Ready for Execution âœ…
