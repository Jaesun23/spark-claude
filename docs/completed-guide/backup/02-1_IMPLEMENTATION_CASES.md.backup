# 구현방법 실전 사례집 (Implementation Cases)

> **목적**: 5단계 프로세스를 실제 프로젝트에 적용한 완전한 사례 분석
>
> **기반**: [02_IMPLEMENTATION_APPROACH_GUIDE.md](./02_IMPLEMENTATION_APPROACH_GUIDE.md)
>
> **버전**: v1.0 (2025-11-11)
>
> **출처**: Gemini 엔터프라이즈 리서치 + memory 프로젝트 재구성

---

## 목차

1. [개요](#1-개요)
2. [사례 1: 문서 자동생성 (결정론적 시스템)](#2-사례-1-문서-자동생성-결정론적-시스템)
3. [사례 2: AI 외부 메모리 (확률론적 시스템)](#3-사례-2-ai-외부-메모리-확률론적-시스템)
4. [사례 3: 채팅 애플리케이션 (실시간 시스템)](#4-사례-3-채팅-애플리케이션-실시간-시스템)
5. [사례 비교 분석](#5-사례-비교-분석)

---

## 1. 개요

### 1-1. 사례집의 목적

이 문서는 **구현방법 작성 가이드 (02_IMPLEMENTATION_APPROACH_GUIDE.md)**의 5단계 프로세스를 실제 프로젝트에 적용한 **완전한 사례 분석**을 제공합니다.

3가지 사례는 서로 **완전히 다른 성격**의 시스템입니다:
1. **문서 자동생성**: 결정론적 시스템 (정확성 최우선)
2. **AI 외부 메모리**: 확률론적 시스템 (관련성 중심)
3. **채팅 애플리케이션**: 실시간 시스템 (가용성 우선)

각 사례는 다음을 보여줍니다:
- **Layer 2 NFR 프로파일** → **속성 질문** 매핑
- **충돌 패턴** 발견 및 해결 (있는 경우)
- **5단계 프로세스** 완전 적용
- **ADR로의 전환** 과정

### 1-2. 사례 선정 기준

| 사례 | 시스템 유형 | Layer 2 특성 | 충돌 패턴 |
|------|-----------|------------|----------|
| **문서 생성** | 결정론적 | 정확성 최우선, B2B, 온프레미스, 배치 | ❌ 없음 |
| **AI 메모리** | 확률론적 | 속도 최우선, API, 클라우드, 즉시 | ✅ 속도+즉시성 |
| **채팅 앱** | 실시간 | 가용성 우선, B2C, 브라우저, 실시간 | ❌ 없음 |

**핵심**: 같은 5단계 프로세스가 **완전히 다른 시스템**에 어떻게 적용되는지 보여줍니다.

### 1-3. 읽는 방법

각 사례는 다음 구조로 작성되었습니다:

```
📄 사례 X: [프로젝트명]
├─ 핵심정의 (입력)
│  ├─ Layer 2 NFR 프로파일
│  ├─ 충돌 패턴 (있다면)
│  └─ Trade-off 결정
├─ 1단계: 기능 분해
├─ 2단계: 속성 질문 ⭐
│  ├─ Layer 2 NFR → 속성 질문 매핑
│  └─ 충돌 패턴 해결 (있다면)
├─ 3단계: 제약조건 파악
├─ 4단계: 기술 옵션 탐색
├─ 5단계: 통합 설계
└─ ADR 연결 예시
```

**추천**:
- 첫 번째 사례를 **완전히** 읽어서 구조를 이해하세요
- 나머지 사례는 **비교하며** 읽으세요 (차이점에 주목)

---

## 2. 사례 1: 문서 자동생성 (결정론적 시스템)

### 2-1. 프로젝트 개요

**비즈니스 목표**: 기업 법률팀을 위한 AI 계약서 초안 작성 서비스

**핵심 가치**:
- 기존 계약서, 내부 규정, 최신 판례(사용자 제공 자료)를 학습
- 복잡한 계약서 초안을 1시간 → 5분으로 단축
- 100% 정확성 보장 (잘못된 정보는 치명적)

**기술 스택**: RAG (Retrieval-Augmented Generation)

### 2-2. 핵심정의 (입력)

#### Layer 2 NFR 프로파일

**핵심정의 3-Layer 결과**:

```
Layer 1: 아키텍처 패밀리
- L1-Q1 (실패 파급력): A (치명적 - 법적/금전적 피해)
- L1-Q2 (정보 형태): A (구조화 - JSON, XML)
- L1-Q3 (응답 시점): C (배치 - 일 단위)
→ 패밀리: 비동기 CRUD/트랜잭션

Layer 2: NFR 우선순위
- L2-Q1 (핵심 품질): A (100% 정확성 최우선)
- L2-Q2 (규모 특성): A (B2B - 10개 엔터프라이즈 고객사)
- L2-Q3 (데이터 노출): A (절대 격리 - 온프레미스/VPC 필수)
- L2-Q4 (데이터 최신성): C (배치 - 일 1회 업데이트)

충돌 패턴: ❌ 없음
- 정확성 + B2B + 온프레미스 + 배치 → 모두 양립 가능

Layer 3: 환경 제약
- L3-Q1 (레거시 연동): 기존 OAuth 2.0 인증 시스템
- L3-Q2 (배포 제약): 온프레미스 또는 VPC
- L3-Q3 (배포 빈도): 월 1회 (안정성 중시)
```

#### Trade-off 결정

```
핵심 Trade-off:
- 정확성 > 속도 (응답 30초 걸려도 OK)
- 보안 > 확장성 (온프레미스 배포)
- 안정성 > 신속성 (월 1회 배포)
```

### 2-3. 1단계: 기능 분해

#### 핵심정의 → 기능 매핑

| 문제 (핵심정의에서) | 필요한 기능 |
|-------------------|-----------|
| 과거 자료 검색에 과도한 시간 소모 | RAG 기반 문서 검색 |
| 오래된 규정 인용 리스크 | 데이터 최신성 관리 (일 배치) |
| 매번 유사한 계약서 작성 | 템플릿 기반 문서 생성 |
| 법률팀 승인 프로세스 | Human-in-the-loop 검증 |

#### 기능 목록

**기능 A: RAG 기반 문서 검색**
- **설명**: 사용자 제공 자료(계약서, 규정, 판례)를 벡터 DB에 저장하고 시맨틱 검색
- **해결하는 문제**: 과거 자료 검색 시간 단축

**기능 B: 일 배치 데이터 업데이트**
- **설명**: 매일 자동으로 최신 판례 수집 및 벡터 DB 업데이트
- **해결하는 문제**: 오래된 규정 인용 리스크

**기능 C: AI 계약서 초안 생성**
- **설명**: LLM + 검색된 자료로 계약서 초안 자동 생성
- **해결하는 문제**: 반복 작업 자동화

**기능 D: 법률팀 승인 워크플로**
- **설명**: AI 생성 초안을 법률팀이 검토 및 최종 승인
- **해결하는 문제**: 100% 정확성 보장

### 2-4. 2단계: 속성 질문 ⭐

#### Layer 2 NFR → 속성 질문 매핑

```
┌────────────────────────────────────────┐
│ Layer 2 NFR 프로파일 (핵심정의에서)      │
├────────────────────────────────────────┤
│ L2-Q1: A (정확성 최우선 - 100%)         │
│ L2-Q2: A (B2B - 10개사)                │
│ L2-Q3: A (절대 격리 - 온프레미스)       │
│ L2-Q4: C (배치 - 일 1회)               │
└────────────────────────────────────────┘
              ↓ 매핑
┌────────────────────────────────────────┐
│ 2단계: 속성 질문                        │
├────────────────────────────────────────┤
│ 성능: "동시 몇 명?" "처리량은?"         │
│ 품질: "실패하면?" "얼마나 정확?"        │
│ 환경: "어디서?" "보안 요구사항?"        │
└────────────────────────────────────────┘
```

#### 기능 A: RAG 기반 문서 검색

**성능 속성**:

Q1: 검색 결과는 얼마나 빨라야 하나? (← L2-Q4: 배치)
- A1: P95 < 5초 (법률팀 사용, 실시간 대화 아님)

Q2: 동시에 몇 명? (← L2-Q2: B2B 10개사)
- A2: 동시 접속 최대 100명 (각 사 10명)

Q3: 데이터 양은?
- A3: 10개사 × 10만 문서 = 100만 건

**→ NFR**:
- 검색 응답 시간: P95 < 5초
- 동시 처리: 100명
- 인덱싱: 100만 문서

**품질 속성**:

Q1: 검색이 틀리면? (← L2-Q1: 정확성 최우선)
- A1: 치명적 (잘못된 규정 인용 → 법적 리스크)

Q2: 검색 정확도는? (← L2-Q1)
- A2: Top-3 결과에 정답 포함 확률 99%

Q3: 가용성은?
- A3: 99% (사내 시스템, 단기 다운타임 허용)

**→ NFR**:
- 검색 정확도: 99% (Top-3)
- 할루시네이션: 절대 불가 (Human 검증 필수)
- 가용성: 99%

**환경 속성**:

Q1: 어디서 실행? (← L2-Q3: 온프레미스)
- A1: 온프레미스 또는 VPC (데이터 외부 유출 금지)

Q2: 플랫폼 제약?
- A2: Kubernetes 가능 (기존 인프라)

Q3: 보안 요구사항? (← L2-Q3)
- A3: 고객사 간 물리적 격리 (10개사별 별도 배포)

**→ NFR**:
- 배포 모델: Kubernetes (온프레미스) 또는 AWS VPC
- 데이터 전송: Private Endpoint만 허용
- 격리: 고객사별 별도 인스턴스

#### 기능 C: AI 계약서 초안 생성

**성능 속성**:

Q1: 생성 속도는? (← L2-Q1: 정확성 > 속도)
- A1: P95 < 30초 (느려도 OK, 정확성 우선)

Q2: 동시 생성 요청?
- A2: 최대 10개 (B2B 규모)

**→ NFR**:
- 생성 시간: P95 < 30초
- 동시 처리: 10개

**품질 속성**:

Q1: 생성 오류는? (← L2-Q1: 100% 정확성)
- A1: 치명적 → **Human-in-the-loop 필수**

Q2: LLM 할루시네이션은?
- A2: 절대 불가 → RAG로 Grounding 필수

**→ NFR**:
- Human 승인: 최종 검토 필수
- RAG Grounding: 모든 주장에 출처 표기
- 추적성: 어떤 문서 참조했는지 기록

#### 충돌 패턴 분석

**Layer 2 충돌 체크**:
```
L2-Q1 (정확성) + L2-Q2 (B2B) + L2-Q3 (온프레미스) + L2-Q4 (배치)
→ 충돌 없음! ✅

이유:
- 정확성 최우선: 속도 희생 가능 (30초 OK)
- B2B 규모: 100명 동시 접속 (확장성 부담 낮음)
- 온프레미스: 물리적 격리 가능 (10개사 → 10개 배포)
- 배치 업데이트: 실시간 불필요 (일 1회 충분)
```

**결론**: 이 사례는 **충돌 패턴이 없는** 깨끗한 요구사항입니다.

### 2-5. 3단계: 제약조건 파악

#### 팀 역량
- **현재 역량**: Python/FastAPI, Kubernetes, RAG 경험 있음
- **제약**: LLM 미세조정 경험 없음 (GPT-4 API 사용)
- **영향**: Self-hosted LLM 대신 Azure OpenAI 사용

#### 비용/시간
- **예산**: 개발 6개월, 인프라 초기 투자 $50K
- **기간**: PoC 2개월 + 개발 4개월
- **인프라**: 온프레미스 (고객사별 별도 배포)

#### 기존 시스템
- **연동 필요**: OAuth 2.0 (기존 인증)
- **DB**: PostgreSQL 12 (사용자 관리)
- **제약**: 인증 시스템 재구축 불가

#### 규제/정책
- **보안 정책**: 데이터 외부 유출 절대 금지
- **규제**: 금융법, 개인정보보호법 준수
- **영향**: 온프레미스 또는 VPC 필수

### 2-6. 4단계: 기술 옵션 탐색

#### 기능 A: 벡터 데이터베이스 선택

**속성 요구사항 (2단계에서)**:
- 100만 문서 인덱싱
- 온프레미스 배포 가능
- 고객사별 물리적 격리
- P95 < 5초 검색

**제약 조건 (3단계에서)**:
- Kubernetes 배포
- 온프레미스 필수
- Python 생태계

---

**옵션 1: Milvus (Open Source, Self-hosted)**

**장점**:
- ✅ 오픈소스, 온프레미스 배포 가능
- ✅ Kubernetes 네이티브
- ✅ Billion-scale 지원
- ✅ 완전한 제어 (물리적 격리 가능)

**단점**:
- ❌ Kubernetes 운영 복잡성
- ❌ 전문 인력 필요

**속성 충족도**:
- 온프레미스: ✅
- 물리적 격리: ✅ (고객사별 별도 컬렉션)
- 성능: ✅ (P95 < 5초 충분)
- 규모: ✅ (100만 문서)

**비용**: 초기 투자 $10K (서버), 운영 인력 1명

---

**옵션 2: Pinecone (SaaS)**

**장점**:
- ✅ 완전 관리형 (운영 부담 없음)
- ✅ 빠른 속도 (< 100ms)
- ✅ 사용 편의성

**단점**:
- ❌ **치명적**: SaaS 전용, 온프레미스 불가 💥
- ❌ 벤더 종속성
- ❌ L2-Q3 (온프레미스 필수) 위배

**속성 충족도**:
- 온프레미스: ❌ (불가능)
- 물리적 격리: ⚠️ (논리적만)
- 성능: ✅
- 규모: ✅

**결정**: **탈락** (온프레미스 요구사항 미충족)

---

**옵션 3: PGvector (PostgreSQL Extension)**

**장점**:
- ✅ 기존 PostgreSQL 활용
- ✅ 아키텍처 단순
- ✅ 온프레미스 배포 가능

**단점**:
- ❌ 100만 문서 규모 검증 부족
- ❌ 전문 벡터 DB 대비 성능 낮음
- ⚠️ P95 < 5초 달성 불확실

**속성 충족도**:
- 온프레미스: ✅
- 물리적 격리: ✅
- 성능: ⚠️ (검증 필요)
- 규모: ⚠️ (불확실)

**비용**: 거의 없음 (기존 PostgreSQL)

---

**권장안**: 옵션 1 (Milvus - Open Source)

**근거**:
1. **속성 충족도**: 모든 NFR 만족 (온프레미스, 물리적 격리, 성능, 규모)
2. **필수 요구사항**: L2-Q3 (온프레미스)를 충족하는 유일한 프로덕션급 솔루션
3. **제약 충족도**: Kubernetes 경험 있음 (팀 역량)
4. **검증된 기술**: LangChain, LlamaIndex 등 Python 생태계 지원

**Pinecone**: 온프레미스 불가 (탈락)
**PGvector**: 규모 검증 부족 (PoC에서 재검토 가능)

#### 기능 C: LLM 선택

**옵션 1: Azure OpenAI (GPT-4) ✅**
- **장점**: VPC Endpoint 지원, 데이터 격리 가능, 검증됨
- **단점**: 비용 (but B2B 규모라 감당 가능)
- **권장**: ✅

**옵션 2: Self-hosted LLaMA 3**
- **장점**: 비용 절감, 완전 제어
- **단점**: 팀 경험 부족, 품질 검증 필요
- **결정**: PoC 이후 고려

### 2-7. 5단계: 통합 설계

#### 시스템 아키텍처

```
┌─────────────────────────────────────────┐
│         법률팀 (데스크톱 브라우저)         │
│             Chrome/Edge                 │
└────────────┬────────────────────────────┘
             │ HTTPS (내부망)
             ↓
┌─────────────────────────────────────────┐
│       API Gateway (NGINX + OAuth)       │
└────────────┬────────────────────────────┘
             │
    ┌────────┴────────┐
    ↓                 ↓
┌──────────┐    ┌──────────────┐
│ FastAPI  │    │ Document Gen │
│ (Search) │    │   Service    │
└────┬─────┘    └──────┬───────┘
     │                 │
     ↓                 ↓
┌──────────┐    ┌──────────────┐
│  Milvus  │    │ Azure OpenAI │
│(Vectors) │    │   (GPT-4)    │
└──────────┘    └──────────────┘
     │
     ↓
┌──────────────────────┐
│ PostgreSQL (Metadata)│
└──────────────────────┘

[배치 파이프라인]
Airflow → Document Processing → Milvus Indexing
(일 1회, 새 판례 수집)
```

**컴포넌트 설명**:
- **API Gateway**: OAuth 2.0 인증, 내부망 접근 제어
- **FastAPI**: RAG 검색, 문서 생성 오케스트레이션
- **Milvus**: 벡터 검색 (고객사별 별도 컬렉션)
- **Azure OpenAI**: GPT-4 (Private Endpoint)
- **PostgreSQL**: 사용자, 문서 메타데이터
- **Airflow**: 일 배치 인덱싱 파이프라인

#### 데이터 스키마 v1.0

```sql
-- Documents (PostgreSQL)
CREATE TABLE documents (
  id UUID PRIMARY KEY,
  tenant_id VARCHAR(50) NOT NULL,  -- 고객사 ID
  title TEXT NOT NULL,
  doc_type VARCHAR(50),  -- 'contract', 'regulation', 'case'
  uploaded_at TIMESTAMP DEFAULT NOW(),
  INDEX idx_tenant_type (tenant_id, doc_type)
);

-- Chunks (Milvus Collection per Tenant)
Collection: tenant_A_documents
Fields:
  - chunk_id: VARCHAR (Primary)
  - doc_id: VARCHAR (Reference)
  - content: TEXT
  - embedding: VECTOR(1536)  -- Ada-002
  - doc_type: VARCHAR (TAG, indexed)
  - created_at: TIMESTAMP (TAG, indexed)
```

**설계 원칙**:
- **물리적 격리**: 고객사별 Milvus 컬렉션 분리
- **메타데이터**: PostgreSQL에 원본 정보, Milvus는 검색만
- **추적성**: chunk_id → doc_id로 출처 추적 가능

**진화 계획**:
- v1.0: 10개사, 100만 문서 (현재 충분)
- v2.0: 멀티모달 (이미지, 표 인식) 추가

#### API 설계 v1.0

```python
# REST API

# 문서 검색
POST /api/search
Request:
{
  "query": "상가건물 임대차보호법",
  "filters": {
    "doc_type": ["regulation", "case"],
    "date_range": {"start": "2024-01-01"}
  },
  "top_k": 5
}
Response:
{
  "results": [
    {
      "chunk_id": "abc123",
      "doc_id": "doc456",
      "content": "...",
      "score": 0.92,
      "metadata": {
        "doc_type": "regulation",
        "title": "...",
        "created_at": "2024-10-15"
      }
    }
  ]
}

# 계약서 초안 생성
POST /api/generate
Request:
{
  "template": "lease_agreement",
  "context": "상가 임대차, 보증금 5천만원",
  "search_results": ["chunk_abc", "chunk_def"]
}
Response:
{
  "draft_id": "draft789",
  "content": "... (AI 생성 초안)",
  "sources": [
    {"doc_id": "doc456", "title": "..."}
  ],
  "status": "pending_review"  # Human 승인 대기
}

# Human 승인
POST /api/drafts/{draft_id}/approve
```

### 2-8. ADR 연결 예시

#### ADR-001: 벡터 데이터베이스로 Milvus 채택

**날짜**: 2024-11-11
**상태**: 결정됨

**맥락 (Context)**:
- Stage 1 요구사항: "온프레미스 배포 필수" (L2-Q3)
- Stage 2 요구사항: "100만 문서, P95 < 5초" (2단계 속성 질문)
- Stage 2 요구사항: "물리적 격리" (고객사별)

**결정 (Decision)**: Milvus (Open Source, Self-hosted)를 채택

**근거 (Rationale)**:
- **Pinecone**: SaaS 전용, 온프레미스 불가 ❌ (L2-Q3 위배)
- **Milvus**: 오픈소스, Kubernetes 배포 가능 ✅ (모든 NFR 충족)
- **PGvector**: 규모 검증 부족 ⚠️

**결과 (Consequences)**:
- **장점**: 모든 NFR 충족, 완전한 제어, 물리적 격리
- **대가**: Kubernetes 운영 복잡성 증가, 전문 인력 1명 필요

#### ADR-002: Human-in-the-loop 승인 프로세스

**날짜**: 2024-11-11
**상태**: 결정됨

**맥락 (Context)**:
- Stage 1: L2-Q1 (100% 정확성 최우선)
- Stage 2: "할루시네이션 절대 불가" (품질 속성)
- Stage 1: L2-Q2 (B2B 10개사 - 수동 가능한 규모)

**결정 (Decision)**: AI 생성 초안은 반드시 법률팀 승인 필요

**근거 (Rationale)**:
- **100% 자동화**: L2-Q1 (정확성) 달성 불가능
- **Human 검증**: 10개사 규모라 수동 가능 (L2-Q2)
- **Hybrid**: AI 80% 자동화 + Human 20% 검증

**결과 (Consequences)**:
- **장점**: 100% 정확성 보장, 법적 리스크 제거
- **대가**: 완전 자동화 불가, 처리 시간 증가 (but 30초는 충족)

---

## 3. 사례 2: AI 외부 메모리 (확률론적 시스템)

### 3-1. 프로젝트 개요

**비즈니스 목표**: AI 에이전트를 위한 장기 기억 시스템 (개발자용 API)

**핵심 가치**:
- AI가 과거 대화, 사용자 선호, 지식을 "기억"
- 대화 연속성 보장 (1호 ↔ 2호 이어가기)
- 관련성 높은 정보를 빠르게 검색

**기술 스택**: RAG + 벡터 검색 + 멀티 테넌시

### 3-2. 핵심정의 (입력)

#### Layer 2 NFR 프로파일

**핵심정의 3-Layer 결과**:

```
Layer 1: 아키텍처 패밀리
- L1-Q1 (실패 파급력): C (점진적 - 관련성 낮은 결과)
- L1-Q2 (정보 형태): B (비구조화 - 텍스트, 문서)
- L1-Q3 (응답 시점): A (즉시 - 실시간)
→ 패밀리: 실시간 검색/추천

Layer 2: NFR 우선순위
- L2-Q1 (핵심 품질): B (가장 빠름 - p99 < 500ms)
- L2-Q2 (규모 특성): C (API - 수천 테넌트, QPS 1000+)
- L2-Q3 (데이터 노출): B (논리적 격리 - 암호화)
- L2-Q4 (데이터 최신성): A (즉시 반영 - 수 초 이내)

충돌 패턴: ✅ 있음! 💥
- L2-Q1 (속도) + L2-Q4 (즉시성) = 동기식 불가능
→ 트레이드오프: "최종일관성(수 초 지연) 수용"

Layer 3: 환경 제약
- L3-Q1 (레거시 연동): 없음 (신규 시스템)
- L3-Q2 (배포 제약): 클라우드 (AWS/GCP)
- L3-Q3 (배포 빈도): 주 1회 (CI/CD)
```

#### Trade-off 결정

```
핵심 Trade-off:
- 속도 > 정확성 (p99 < 500ms, 70% 관련성 OK)
- 확장성 > 보안 (논리적 격리, 물리적 불가능)
- 최종일관성 수용 (즉시 + 빠름 양립 위해)
```

#### 🔥 충돌 패턴 발견!

**충돌**: L2-Q1 (p99 < 500ms) + L2-Q4 (즉시 반영)

**문제**:
```
동기식 API:
1. 데이터 수신
2. 청킹 (수십 ms)
3. 임베딩 모델 호출 (외부 API, 수백 ms) 💥
4. 벡터 DB 저장 (수십 ms)
5. 200 OK 응답

총 시간: 500ms 초과! ❌
```

**트레이드오프 질문**: "최종일관성(수 초 지연)을 수용할 수 있나?"

**답변**: Yes! (AI 사용자는 수 초 지연 허용 가능)

### 3-3. 1단계: 기능 분해

#### 핵심정의 → 기능 매핑

| 문제 (핵심정의에서) | 필요한 기능 |
|-------------------|-----------|
| AI가 과거 대화를 잊어버림 | 대화 저장 및 검색 |
| 세션 간 연속성 부족 (1호 ↔ 2호) | 크로스 세션 메모리 |
| 관련 없는 정보 검색 | 하이브리드 검색 (키워드 + 의미) |
| 수천 테넌트 데이터 격리 | 멀티 테넌시 격리 |
| 실시간 대화 중 검색 | 저지연 API (< 500ms) |

#### 기능 목록

**기능 A: 실시간 데이터 수집**
- **설명**: 대화, 문서, 메모 등을 실시간으로 수집 및 인덱싱
- **해결하는 문제**: 즉시 반영 (L2-Q4)

**기능 B: 하이브리드 검색**
- **설명**: 키워드 필터 (시간, 타입) + 의미 유사도 검색
- **해결하는 문제**: 관련성 (L2-Q1 quality dimension)

**기능 C: 멀티 테넌트 격리**
- **설명**: tenant_id 기반 데이터 완전 격리
- **해결하는 문제**: 수천 테넌트 (L2-Q2)

**기능 D: 저지연 검색 API**
- **설명**: p99 < 500ms 보장
- **해결하는 문제**: 실시간 대화 (L2-Q1 speed)

### 3-4. 2단계: 속성 질문 ⭐

#### Layer 2 NFR → 속성 질문 매핑

```
┌────────────────────────────────────────┐
│ Layer 2 NFR 프로파일 + 충돌             │
├────────────────────────────────────────┤
│ L2-Q1: B (p99 < 500ms)                │
│ L2-Q2: C (API 수천 테넌트)              │
│ L2-Q3: B (논리적 격리)                  │
│ L2-Q4: A (즉시) + 충돌! 💥             │
└────────────────────────────────────────┘
              ↓ 매핑 + 충돌 해결
┌────────────────────────────────────────┐
│ 2단계: 속성 질문 + 충돌 해결책          │
├────────────────────────────────────────┤
│ 성능: "p99?" "QPS?" "지연 허용?"       │
│ 품질: "관련성?" "실패하면?"             │
│ 환경: "격리?" "암호화?"                 │
│ 충돌: 비동기 아키텍처 (Kafka)          │
└────────────────────────────────────────┘
```

#### 기능 A: 실시간 데이터 수집

**성능 속성**:

Q1: API 응답 시간은? (← L2-Q1: p99 < 500ms)
- A1: p99 < 100ms (수집 API는 매우 빨라야)

Q2: 데이터 최신성은? (← L2-Q4: 즉시)
- A2: **수 초 지연 허용** (최종일관성 수용)

Q3: 처리량은? (← L2-Q2: API)
- A3: QPS 1000+

**→ NFR**:
- API 응답: p99 < 100ms (202 Accepted)
- 데이터 반영: 최종일관성 (수 초)
- 처리량: QPS 1000+

**품질 속성**:

Q1: 수집 실패하면?
- A1: 재시도 가능 (치명적 아님)

Q2: 순서 보장?
- A2: 불필요 (타임스탬프로 정렬)

**→ NFR**:
- At-least-once 전달
- 순서 보장 불필요

**환경 속성**:

Q1: 어디서 실행?
- A1: 클라우드 (AWS/GCP)

Q2: 확장 전략?
- A2: 수평 확장 (Kafka + Workers)

**→ NFR**:
- 클라우드 네이티브
- Auto-scaling

#### 🔥 충돌 패턴 해결!

**충돌**: L2-Q1 (p99 < 100ms) + L2-Q4 (즉시 반영)

**구현방법 2단계 해결**:

```
속성 질문:
Q: "API 응답 시간은?" → A: "p99 < 100ms"
Q: "데이터 최신성은?" → A: "수 초 지연 허용"

→ NFR 도출:
- API 응답: < 100ms (동기 불가능)
- 데이터 반영: 최종일관성 (수 초)

→ 기술 옵션 (4단계):
  옵션 1: Kafka + 비동기 Workers ✅
  옵션 2: 동기식 처리 ❌ (100ms 초과)
  옵션 3: 배치 처리 ❌ (최신성 미충족)

→ 권장안: Kafka 기반 비동기 아키텍처
  - API: 즉시 202 Accepted 응답 (< 50ms)
  - Workers: 백그라운드 처리 (수 초)
  - 대가: 최종일관성 (Eventually Consistent)
```

**ADR 연결**:
```
ADR-002: Kafka 기반 비동기식 수집
Context: API 응답 < 100ms + 데이터 즉시 반영 충돌
Decision: Kafka + Workers 채택
Consequences: 최종일관성 수용 (수 초 지연)
```

#### 기능 B: 하이브리드 검색

**성능 속성**:

Q1: 검색 응답 시간은? (← L2-Q1)
- A1: p99 < 500ms

Q2: 동시 검색 요청? (← L2-Q2)
- A2: QPS 1000+

**→ NFR**:
- 검색 지연: p99 < 500ms
- 처리량: QPS 1000+

**품질 속성**:

Q1: 검색 관련성은? (← L2-Q1 quality)
- A1: 70% 정도면 충분 (완벽 불필요)

Q2: 검색 실패하면?
- A2: 괜찮음 (재검색 가능)

**→ NFR**:
- 관련성: 70%+ (유용성 우선)
- 실패: Graceful degradation

**환경 속성**:

Q1: 캐싱 전략?
- A1: 자주 검색되는 쿼리 캐싱

Q2: 인덱스 전략?
- A2: 인메모리 인덱스 (빠른 검색)

**→ NFR**:
- Redis 캐싱
- HNSW 인덱스

#### 기능 C: 멀티 테넌트 격리

**성능 속성**:

Q1: 동시 테넌트 수는? (← L2-Q2: 수천)
- A1: 초기 1000, 최대 10000

Q2: 테넌트당 데이터?
- A2: 평균 10만 청크

**→ NFR**:
- 테넌트 수: 10000
- 총 벡터: 10억 (10K × 100K)

**품질 속성**:

Q1: 격리 방식? (← L2-Q3: 논리적)
- A1: 논리적 격리 (tenant_id 필터)

Q2: 격리 실패하면?
- A2: 치명적 (데이터 유출)

**→ NFR**:
- tenant_id 필터 강제
- Row-level Security

**환경 속성**:

Q1: 물리적 격리 가능? (← L2-Q2: 수천)
- A1: 불가능 (비용/복잡도)

Q2: 보안 전략?
- A2: 논리적 격리 + 암호화

**→ NFR**:
- 공유 DB + RLS
- 암호화 (at-rest, in-transit)

### 3-5. 3단계: 제약조건 파악

#### 팀 역량
- **현재 역량**: Python, Kafka, Kubernetes
- **제약**: 벡터 DB 경험 부족
- **영향**: 관리형 서비스 고려 (Pinecone vs Milvus)

#### 비용/시간
- **예산**: 개발 3개월, 인프라 월 $5K
- **기간**: MVP 1개월 + 개발 2개월
- **인프라**: 클라우드 (AWS EKS)

#### 기존 시스템
- **연동 필요**: 없음 (신규)
- **제약**: 없음

#### 규제/정책
- **보안 정책**: 테넌트 간 격리 필수
- **규제**: GDPR (잊힐 권리)
- **영향**: tenant_id 기반 삭제 기능

### 3-6. 4단계: 기술 옵션 탐색

#### 🔥 충돌 해결: 데이터 수집 아키텍처

**속성 요구사항 (2단계 충돌 해결)**:
- API 응답: p99 < 100ms
- 데이터 반영: 최종일관성 (수 초)
- 처리량: QPS 1000+

**제약 조건**:
- Kafka 경험 있음
- 클라우드 배포

---

**옵션 1: Kafka + 비동기 Workers ✅**

**장점**:
- ✅ API 응답 < 50ms (202 Accepted)
- ✅ 버퍼 역할 (트래픽 폭증 대응)
- ✅ 수평 확장 (Workers 추가)
- ✅ 충돌 해결! (속도 + 즉시성 양립)

**단점**:
- ❌ 최종일관성 (수 초 지연)
- ❌ 아키텍처 복잡도 증가

**속성 충족도**:
- API 응답: ✅ (< 50ms)
- 데이터 반영: ✅ (수 초, 허용 범위)
- 처리량: ✅ (QPS 1000+)
- 충돌 해결: ✅

**비용**: 월 $1K (Kafka + Workers)

---

**옵션 2: 동기식 처리**

**장점**:
- ✅ 아키텍처 단순
- ✅ 즉시 일관성

**단점**:
- ❌ API 응답 > 500ms 💥
- ❌ L2-Q1 (p99 < 100ms) 위배
- ❌ 충돌 해결 불가

**결정**: **탈락** (NFR 미충족)

---

**옵션 3: 배치 처리**

**장점**:
- ✅ 아키텍처 단순

**단점**:
- ❌ L2-Q4 (즉시) 위배
- ❌ 최신성 부족

**결정**: **탈락** (NFR 미충족)

---

**권장안**: 옵션 1 (Kafka + 비동기 Workers)

**근거**:
1. **충돌 해결**: 속도 + 즉시성을 최종일관성으로 해결
2. **속성 충족**: 모든 NFR 충족
3. **확장성**: 수평 확장 가능
4. **검증됨**: 업계 표준 (Twitter, LinkedIn)

**대가**: 최종일관성 (비즈니스 승인 필요)

#### 기능 C: 벡터 데이터베이스 선택

**속성 요구사항**:
- 10억 벡터 (10K 테넌트 × 100K)
- tenant_id 필터링 (고효율)
- p99 < 500ms
- 클라우드 배포

---

**옵션 1: Pinecone (SaaS)**

**장점**:
- ✅ 완전 관리형
- ✅ 고효율 메타데이터 필터링
- ✅ 빠른 속도 (< 100ms)

**단점**:
- ❌ 벤더 종속성
- ❌ 비용 (10억 벡터 시 월 $3K+)

**속성 충족도**:
- 격리: ✅ (tenant_id 필터)
- 성능: ✅ (p99 < 100ms)
- 규모: ✅ (10억 벡터)

**비용**: 월 $3K

---

**옵션 2: Milvus (Self-hosted on EKS)**

**장점**:
- ✅ 오픈소스
- ✅ 비용 효율 (자체 호스팅)
- ✅ 메타데이터 필터링 지원

**단점**:
- ❌ Kubernetes 운영 복잡성
- ❌ 전문 인력 필요

**속성 충족도**:
- 격리: ✅ (tenant_id 필터)
- 성능: ✅ (< 500ms)
- 규모: ✅ (10억 벡터)

**비용**: 월 $1K (EKS) + 인력

---

**권장안**: 초기 Pinecone, 이후 Milvus 전환

**근거**:
1. **초기 (MVP)**: Pinecone으로 빠른 검증
2. **확장 (10K 테넌트)**: Milvus로 전환 (비용 절감)
3. **Trade-off**: 초기 속도 vs 장기 비용

### 3-7. 5단계: 통합 설계

#### 시스템 아키텍처

```
┌────────────────────────────────────────┐
│     AI Agents (1호, 2호) / Developers  │
│            API Consumers               │
└────────────┬───────────────────────────┘
             │ HTTPS (API Key)
             ↓
┌─────────────────────────────────────────┐
│   API Gateway (Kong + tenant_id Auth)   │
└────────────┬────────────────────────────┘
             │
    ┌────────┴─────────┐
    ↓                  ↓
┌──────────┐     ┌───────────┐
│ Ingest   │     │  Search   │
│ API      │     │   API     │
│(FastAPI) │     │ (FastAPI) │
└────┬─────┘     └─────┬─────┘
     │ 202              │
     ↓                  ↓
┌──────────┐     ┌─────────────┐
│  Kafka   │     │ Pinecone/   │
│ (Buffer) │     │  Milvus     │
└────┬─────┘     │ (tenant_id  │
     │           │  filtered)  │
     ↓           └─────────────┘
┌──────────────┐       ↑
│ Embedding    │───────┘
│  Workers     │
│ (Async)      │
└──────────────┘
     ↓
[PostgreSQL - Metadata]
```

**컴포넌트 설명**:
- **API Gateway**: tenant_id 인증, Rate limiting
- **Ingest API**: 202 Accepted → Kafka
- **Search API**: tenant_id 필터 강제 → Pinecone
- **Kafka**: 비동기 버퍼 (충돌 해결 핵심!)
- **Embedding Workers**: 청킹, 임베딩, 저장
- **Pinecone**: 벡터 검색 (tenant_id 메타데이터 필터)

#### 데이터 스키마 v1.0

```sql
-- Memories (PostgreSQL Metadata)
CREATE TABLE memories (
  id UUID PRIMARY KEY,
  tenant_id VARCHAR(50) NOT NULL,
  user_id VARCHAR(50),
  content TEXT NOT NULL,
  type VARCHAR(50),  -- 'conversation', 'document', 'note'
  project VARCHAR(100),
  speaker VARCHAR(50),
  created_at TIMESTAMP DEFAULT NOW(),
  INDEX idx_tenant_user (tenant_id, user_id),
  INDEX idx_tenant_type (tenant_id, type)
);

-- Pinecone Namespace per Tenant
Namespace: tenant_A
Vectors:
  - id: memory_uuid
  - vector: [1536 dims]  -- OpenAI Ada-002
  - metadata:
      tenant_id: "tenant_A"  -- MUST for filtering
      type: "conversation"
      project: "project_X"
      created_at: "2024-11-11T10:00:00Z"
```

**설계 원칙**:
- **논리적 격리**: tenant_id 메타데이터 필터 강제
- **Namespace**: Pinecone Namespace로 1차 격리
- **추적성**: memory_uuid로 원본 추적

#### API 설계 v1.0

```python
# Ingest API (비동기)
POST /api/ingest
Headers:
  X-Tenant-ID: tenant_A
  X-API-Key: xxx
Request:
{
  "content": "Jason과 memory 프로젝트 논의",
  "type": "conversation",
  "project": "memory",
  "metadata": {...}
}
Response:
202 Accepted
{
  "id": "mem_abc123",
  "status": "processing"  # 수 초 후 검색 가능
}

# Search API (동기)
POST /api/search
Headers:
  X-Tenant-ID: tenant_A
  X-API-Key: xxx
Request:
{
  "query": "어제 memory 프로젝트 뭐 했지?",
  "filters": {
    "type": "conversation",
    "project": "memory",
    "date_range": {"start": "2024-11-10"}
  },
  "top_k": 5
}
Response:
200 OK (< 500ms)
{
  "results": [
    {
      "id": "mem_abc123",
      "content": "...",
      "score": 0.89,
      "metadata": {
        "type": "conversation",
        "created_at": "2024-11-11T10:00:00Z"
      }
    }
  ],
  "latency_ms": 320
}
```

### 3-8. ADR 연결 예시

#### ADR-002: Kafka 기반 비동기식 수집 (충돌 해결!)

**날짜**: 2024-11-11
**상태**: 결정됨

**맥락 (Context)**:
- Stage 1: L2-Q1 (p99 < 500ms) + L2-Q4 (즉시 반영)
- Stage 2 충돌 발견: 동기식으로 양립 불가능 💥
- Stage 2 속성 질문: "최종일관성(수 초) 수용 가능?"
- Stage 2 답변: Yes (AI 사용자는 수 초 허용)

**결정 (Decision)**: Kafka + 비동기 Embedding Workers

**근거 (Rationale)**:
- **동기식**: 임베딩 API 호출 (수백 ms) → p99 초과 ❌
- **Kafka**: 즉시 202 반환 (< 50ms) → p99 충족 ✅
- **Workers**: 백그라운드 처리 (수 초) → 최종일관성

**결과 (Consequences)**:
- **장점**: 속도 + 즉시성 양립, 확장성, 안정성
- **대가**: 최종일관성 (수 초 지연) - 비즈니스 승인 필요

#### ADR-003: 멀티 테넌시 격리 전략

**날짜**: 2024-11-11
**상태**: 결정됨

**맥락 (Context)**:
- Stage 1: L2-Q2 (API 수천 테넌트) + L2-Q3 (논리적 격리)
- Stage 2: "tenant_id 필터링" 속성 도출
- Stage 2 충돌 분석: "물리적 격리는 확장성 위배"

**결정 (Decision)**: 공유 Pinecone + tenant_id 메타데이터 필터 강제

**근거 (Rationale)**:
- **물리적 격리**: 10K 테넌트 → 10K Namespace 불가능 ❌
- **논리적 격리**: tenant_id 필터 → 비용 효율 ✅
- **Trade-off**: 보안 위험을 애플리케이션 로직으로 이동

**결과 (Consequences)**:
- **장점**: 비용 효율, 수평 확장, 빠른 온보딩
- **대가**: 필터링 버그 → 데이터 유출 리스크
- **조치**: 엄격한 코드 리뷰, 다층 검증, QA 테스트

---

## 4. 사례 3: 채팅 애플리케이션 (실시간 시스템)

### 4-1. 프로젝트 개요

**비즈니스 목표**: 팀 협업을 위한 실시간 채팅 서비스

**핵심 가치**:
- 1:1 및 그룹 채팅
- 실시간 타이핑 인디케이터
- 메시지 전송 < 500ms
- 1000명 동시 접속 지원

**기술 스택**: WebSocket + Redis Pub/Sub

### 4-2. 핵심정의 (입력)

#### Layer 2 NFR 프로파일

```
Layer 1: 아키텍처 패밀리
- L1-Q1 (실패 파급력): B (업무 중단 - 재시도 필요)
- L1-Q2 (정보 형태): A (구조화 - JSON 메시지)
- L1-Q3 (응답 시점): A (즉시 - 실시간)
→ 패밀리: 실시간 스트리밍

Layer 2: NFR 우선순위
- L2-Q1 (핵심 품질): B (가장 빠름 - p99 < 500ms)
- L2-Q2 (규모 특성): B (B2C - 1000명 동시 접속)
- L2-Q3 (데이터 노출): C (공개 - 팀 내부)
- L2-Q4 (데이터 최신성): A (즉시 - 실시간)

충돌 패턴: ❌ 없음
- 실시간 + 속도 + 즉시성 → WebSocket으로 양립 가능

Layer 3: 환경 제약
- L3-Q1 (레거시 연동): OAuth 2.0, PostgreSQL
- L3-Q2 (배포 제약): 클라우드 (AWS ECS)
- L3-Q3 (배포 빈도): 주 2회 (CI/CD)
```

#### Trade-off 결정

```
핵심 Trade-off:
- 가용성 > 일관성 (일시적 중복 메시지 허용)
- 실시간성 > 순서 (타임스탬프로 정렬)
- 속도 > 완벽성 (99.9% 가용성)
```

### 4-3. 1단계: 기능 분해

| 문제 | 필요한 기능 |
|------|-----------|
| 실시간 소통 | WebSocket 양방향 통신 |
| 그룹 대화 | 채널(Channel) 관리 |
| 타이핑 인디케이터 | 실시간 상태 전파 |
| 많은 사용자 | 수평 확장 (Redis Pub/Sub) |

### 4-4. 2단계: 속성 질문 ⭐

#### Layer 2 NFR → 속성 질문 매핑

**기능 A: 1:1 및 그룹 채팅**

**성능 속성**:
- Q1: 메시지 전송 얼마나 빨라야? → A1: 99% 500ms 이내
- Q2: 동시에 몇 명? → A2: 최대 1,000명
- Q3: 메시지 양은? → A3: 하루 10만 건

**→ NFR**:
- 지연: p99 < 500ms
- 동시 접속: 1,000명
- 일일 처리량: 100K

**품질 속성**:
- Q1: 메시지 손실되면? → A1: 치명적 (At-least-once)
- Q2: 중복 메시지는? → A2: 허용 (UI 필터링)
- Q3: 가용성은? → A3: 99.9%

**→ NFR**:
- At-least-once 전달
- 중복 허용 (idempotent)
- 가용성 99.9%

**환경 속성**:
- Q1: 어디서 실행? → A1: 웹 브라우저 + 모바일
- Q2: 플랫폼 제약? → A2: 방화벽 환경 지원
- Q3: 보안? → A3: E2E 암호화

**→ NFR**:
- 브라우저 호환 (WebSocket)
- 방화벽 통과 (HTTP fallback)
- E2E 암호화

#### 충돌 패턴 분석

```
L2-Q1 (p99 < 500ms) + L2-Q4 (즉시) + L2-Q2 (1000명)
→ 충돌 없음! ✅

이유:
- WebSocket: 양방향, 저지연 (< 100ms)
- Redis Pub/Sub: 1000명 수평 확장 가능
- 실시간 + 속도 양립 (동기식 OK)
```

### 4-5. 3단계: 제약조건 파악

- **팀 역량**: Python/Flask, WebSocket 미경험 (2주 학습)
- **비용**: 개발 3개월, 인프라 월 $500
- **기존 시스템**: OAuth 2.0, PostgreSQL 12
- **규제**: 없음

### 4-6. 4단계: 기술 옵션 탐색

#### 실시간 메시지 전송 기술

| 옵션 | 장점 | 단점 | NFR 충족 | 비용 |
|------|------|------|---------|------|
| **WebSocket** | 양방향, < 100ms, 브라우저 표준 | 학습 2주, Stateful | ✅ 모두 충족 | $450/월 |
| Long Polling | HTTP 기반, 구현 단순 | 지연 1-2초, 부하 높음 | ❌ 지연 미충족 | $500/월 |
| gRPC Streaming | 고성능, < 100ms | 브라우저 제한, 학습 1개월+ | ⚠️ 브라우저 | $400/월 |

**권장안**: WebSocket (Socket.IO)
- 모든 NFR 충족
- 예산 내 ($450 < $500)
- Flask-SocketIO 라이브러리 성숙

### 4-7. 5단계: 통합 설계

#### 시스템 아키텍처

```
[Client (React)]
      ↓ WebSocket (Socket.IO)
      ↓ HTTPS (REST API)
[API Gateway (NGINX)]
      ↓
[Message Service (Flask)] ← [Auth Service]
      ↓                           ↓
[Redis Pub/Sub]              [PostgreSQL]
```

#### 데이터 스키마 v1.0

```sql
-- Users (기존 시스템)
users (id, username, created_at)

-- Channels
channels (id, name, type, created_at)

-- Messages
messages (
  id, channel_id, user_id,
  content, created_at,
  INDEX(channel_id, created_at)
)

-- User Status (Redis)
user_status:{user_id} = {
  "status": "online|offline",
  "last_seen": "timestamp"
}
```

#### API 설계 v1.0

```python
# REST API
GET /api/channels → 채널 목록
GET /api/channels/:id/messages → 메시지 히스토리

# WebSocket Events
## Client → Server
send_message(channel_id, content)
typing(channel_id, is_typing)

## Server → Client
new_message(message_data)
user_typing(user_data)
```

### 4-8. ADR 연결 예시

#### ADR-001: 실시간 메시지 전송 기술로 WebSocket 채택

**맥락**: p99 < 500ms + 1000명 + 브라우저 호환
**결정**: WebSocket (Socket.IO)
**근거**: 모든 NFR 충족, 검증된 기술
**결과**:
- 장점: 양방향, 저지연, 확장 가능
- 대가: Stateful 서버 (Redis 필요)

---

## 5. 사례 비교 분석

### 5-1. Layer 2 NFR 프로파일 비교

| 차원 | 문서 생성 | AI 메모리 | 채팅 앱 |
|------|----------|----------|--------|
| **L1 패밀리** | 비동기 CRUD | 실시간 검색 | 실시간 스트리밍 |
| **실패 모드** | 치명적 (0/1) | 점진적 (0-100%) | 중단 (재시도) |
| **핵심 품질** | 100% 정확성 | 관련성 + 속도 | 가용성 + 속도 |
| **규모** | B2B (10개사) | API (수천 테넌트) | B2C (1000명) |
| **보안** | 물리적 격리 | 논리적 격리 | 팀 내부 |
| **최신성** | 배치 (일 1회) | 즉시 (수 초) | 실시간 |
| **충돌** | ❌ 없음 | ✅ 속도+즉시성 | ❌ 없음 |

### 5-2. 충돌 패턴 해결 비교

**사례 1 (문서 생성)**: 충돌 없음 ✅
- 정확성 + B2B + 온프레미스 + 배치 → 모두 양립

**사례 2 (AI 메모리)**: 충돌 있음 💥
- 속도 (p99 < 500ms) + 즉시성 (수 초)
- **해결책**: Kafka 비동기 아키텍처
- **대가**: 최종일관성 (수 초 지연)

**사례 3 (채팅 앱)**: 충돌 없음 ✅
- 속도 + 즉시성 + 실시간 → WebSocket으로 양립

### 5-3. 기술 선택 비교

| 기술 영역 | 문서 생성 | AI 메모리 | 채팅 앱 |
|----------|----------|----------|--------|
| **데이터 수집** | 배치 (Airflow) | Kafka + Workers | WebSocket |
| **저장소** | Milvus (온프레미스) | Pinecone (SaaS) | PostgreSQL |
| **검색** | RAG (GPT-4) | 하이브리드 검색 | N/A |
| **실시간** | 불필요 | 비동기 | 동기 (WebSocket) |
| **확장** | 수직 (고객사별) | 수평 (Auto-scaling) | 수평 (Redis) |

### 5-4. 핵심 교훈

**교훈 1: 같은 기술, 다른 아키텍처**
- 3개 사례 모두 Python + Redis 사용
- 하지만 **아키텍처는 완전히 다름**
- 이유: **Layer 2 NFR 프로파일이 다르기 때문**

**교훈 2: 충돌 패턴이 아키텍처를 결정**
- 사례 1, 3: 충돌 없음 → 단순 아키텍처
- 사례 2: 충돌 있음 → 복잡한 비동기 아키텍처

**교훈 3: 속성 질문이 핵심**
- "얼마나 빨라야?" "몇 명?" "어디서?"
- 이 질문들이 **기술 선택을 주도**
- 기능만으로는 기술 선택 불가능

**교훈 4: Trade-off는 항상 존재**
- 문서 생성: 속도 희생 (정확성 우선)
- AI 메모리: 일관성 희생 (속도 우선)
- 채팅 앱: 순서 희생 (가용성 우선)

---

## 참고 문서

### 관련 가이드
- [01_CORE_DEFINITION_GUIDE.md](./01_CORE_DEFINITION_GUIDE.md) - 3-Layer 질문 구조
- [02_IMPLEMENTATION_APPROACH_GUIDE.md](./02_IMPLEMENTATION_APPROACH_GUIDE.md) - 5단계 프로세스
- [03_ADR_GUIDE.md](./03_ADR_GUIDE.md) - 기술 결정 기록

### 실전 사례 출처
- `Gemini_아키텍처_정의_질문_구조화_리서치.md` - 문서 생성 사례
- `Gemini_AI외부메모리_아키텍처_설계_프로세스.md` - AI 메모리 사례
- `memory_프로젝트_재구성_Level2_완료_20251110.md` - 실제 세션

---

**구현방법 실전 사례집 (Implementation Cases) v1.0** - 2025-11-11

가이드: [구현방법 작성 가이드](./02_IMPLEMENTATION_APPROACH_GUIDE.md)
이전: [핵심정의 작성 가이드](./01_CORE_DEFINITION_GUIDE.md)
다음: [ADR 가이드](./03_ADR_GUIDE.md)
