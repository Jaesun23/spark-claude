# Stage 1-3 체계화 핵심 이해

**날짜**: 2025-11-11
**참여자**: Jason + 1호 (Claude)
**목적**: Kent Beck augmented coding 연구 → Jason 방법론 전체 이해

---

## 🎯 핵심 발견

### Kent Beck과 Jason 방법론의 완벽한 매칭

**Kent Beck의 Augmented Coding 핵심**:
1. **Vibe Coding vs Augmented Coding**
   - Vibe: 코드 신경 안 씀, 동작만 중요, 에러나면 재시도
   - Augmented: 코드/복잡도/테스트 신경 씀, "tidy code that works"

2. **Task 크기 조절**
   - 작고 명확한 작업, 빈번한 피드백
   - AI의 "lack of taste" 문제 - 거대한 함수에 계속 추가

3. **TDD가 슈퍼파워**
   - AI agents와 작업할 때 테스트가 regression 방지
   - 테스트 먼저 → 최소 구현

**Jason 방법론과의 연결**:
1. **Task = 레고블럭** (Kent Beck의 작은 작업)
   - 독립적으로 테스트 가능
   - TODO 없는 완전한 구현
   - 200K 컨텍스트로 가능한 크기

2. **Bootstrap = 일관성 강제** (Kent Beck의 taste 문제 해결)
   - 시스템이 표준 도구/패턴 강제
   - Pre-commit hook이 print() 차단
   - 4-Layer 강제 시스템

3. **9-Step Checklist = TDD 중심**
   - Step 2: 테스트 먼저 작성
   - Step 3: 최소 구현
   - 95% 커버리지 강제

---

## 📋 Jason 방법론 전체 구조

```
[Human-Driven: Stage 1-4]

Stage 1: 핵심정의 (3-Layer Decision Tree)
  = Amazon 1단계: Identify Use Cases
  
  Layer 1: 아키텍처 패밀리 (3개 질문)
    - 5가지 패밀리 분류
    - 실패 파급력, 정보 형태, 응답 시점
  
  Layer 2: NFR 우선순위 (4개 질문)
    - 핵심 품질 선택 (정확/속도/보안/비용)
    - 충돌 감지 → 트레이드오프 ⭐
  
  Layer 3: 환경 제약 (3개 질문)
    - 기술 스택 후보 좁히기
    ↓
    
Stage 2: 구현방법 (5단계 프로세스)
  = Amazon 2단계: Specify Concrete Requirements
  
  1. 기능 분해
  2. 속성 질문 ⭐⭐⭐ (가장 중요!)
     - Layer 2 NFR → 속성 질문 매핑
     - "얼마나 빨라야?" "몇 명?" "실패하면?"
     - Carnegie Mellon SEI: "속성이 기술 선택을 주도"
  3. 제약조건 파악
  4. 기술 옵션 (후보 3개 비교)
  5. 통합 설계
    ↓

Stage 3: Bootstrap 구현 ⭐
  = 일관성 강제 환경 구축
  - DNA 8개 시스템
  - common/ 표준 모듈
  - Bootstrap Gate 통과 필수
    ↓

Stage 4: Blueprint (청사진)
  = 모든 레고블럭 명세 (5000줄)
  - Bootstrap 환경 기반으로 구체적 작성
  - 모든 Task 명시 (누락 방지)

──────────────────────────────────
⬇️ SPARK 시작 가능 지점 ⬇️
──────────────────────────────────

[SPARK-Enabled: Stage 5-8]

Stage 5: Task Breakdown (작업분해)
  - 5000줄 → Task별 100줄
  - 정확한 라인 참조

Stage 6: Checklist (9-Step)
  - TDD 중심 (테스트 먼저)
  - 95% 커버리지 강제

Stage 7: 도메인 구현
  - Bootstrap 환경이 일관성 강제
  - Quality Gates 검증

Stage 8: 최적화
```

---

## 🎪 현재 상황

### 완성된 것 ✅
1. **핵심 방법론 문서** (00_CORE_METHODOLOGY.md)
   - 8-Stage 진화 구조
   - Task = 레고블럭 개념
   - Bootstrap 철학
   - 4-Layer 강제 시스템

2. **핵심정의 가이드** (01_CORE_DEFINITION_GUIDE.md)
   - v2.0: 3-Layer Decision Tree (스무고개 방식)
   - 고정 질문 → 답변에 따라 분기
   - 5가지 아키텍처 패밀리
   - 충돌 감지 메커니즘

3. **구현방법 가이드** (02_IMPLEMENTATION_APPROACH_GUIDE.md)
   - 5단계 프로세스
   - 속성 질문이 핵심 (2단계)
   - Layer 2 NFR → 속성 질문 매핑
   - 충돌 패턴 해결

4. **ADR 가이드** (03_ADR_GUIDE.md)
   - 5가지 유형
   - 7개 섹션 (Compliance 필수!)
   - ADR → Standards 변환
   - 구현방법의 탐색 결과를 "기록"

5. **검증 완료**
   - 문서 생성 서비스 (결정론적, CRUD)
   - AI 외부메모리 (확률론적, 검색)

### 진행 중 🔄
**제미나이 연구: "공통된 질문" 보편성 검증**

현재: 2개 사례에서 패턴 추출 완료
목표: 다양한 프로젝트 유형에 3-Layer 적용

**핵심 질문**:
"이 10개 질문이 모든 프로젝트 유형에 보편적으로 적용되나?"

---

## 💡 핵심 통찰: 아키텍처 이론과의 연결

### Jason의 핵심 인사이트

```
"관심" ❌
    ↓
"질문이 결론으로 이끄는가?" ✅
    ↓
분류(Classification) = 클러스터링
    ↓
분류의 목적:
- 공통 모듈 (Bootstrap, 뼈대)
- 도메인 구조 (고유 특성)
    ↓
검증된 아키텍처 이론으로 귀결
```

### 실제 의미

**우리가 찾는 것**:
```
❌ 임의의 "좋은 질문들"
✅ 검증된 아키텍처 분류 체계를 "질문으로 변환"

예시:
Carnegie Mellon SEI의 Quality Attributes
    ↓ 질문으로 변환
Layer 2: "가장 정확? 빠름? 안전?"
    ↓ 이끄는 결론
ACID vs Eventually Consistent
Monolith vs Microservices
Sync vs Async
```

### 이미 존재하는 분류 체계들

1. **Carnegie Mellon SEI** - Quality Attributes
   - Performance, Availability, Security, Modifiability
   - → Layer 2 질문의 이론적 기반

2. **Martin Fowler** - Architecture Patterns
   - Layered, Event-Driven, Microservices
   - → Layer 1 패밀리의 이론적 기반

3. **CAP Theorem** - 분산 시스템
   - Consistency, Availability, Partition Tolerance
   - → Layer 2 충돌 감지의 이론적 기반

4. **12-Factor App** - 클라우드 네이티브
   - → Layer 3 환경 제약의 이론적 기반

---

## 🔄 순서의 중요성

### ❌ 잘못된 순서
```
핵심정의 → ADR (기술 먼저)
"채팅 필요" → "Redis 쓰자"
→ 왜? (근거 없음)
```

### ✅ 올바른 순서
```
핵심정의 (Stage 1)
  "채팅 필요"
  ↓
구현방법 (Stage 2)
  속성 질문: "500ms, 1000명 동시"
  기술 옵션: WebSocket vs Long Polling vs gRPC
  권장안: WebSocket (근거 명확)
  ↓
ADR (Stage 3)
  결정 기록 + Compliance
```

### 핵심 원칙
> **"속성(Quality Attributes)이 기술 선택을 주도한다"**
> — Carnegie Mellon SEI
>
> 기능만으로는 기술을 선택할 수 없다.
> 성능/품질/환경 요구사항을 먼저 구체화해야
> 올바른 기술을 선택할 수 있다.

---

## 🤝 2호와의 협업 포인트

### 역할 분담
- **1호 (대화)**: Stage 1-4 (Human-Driven)
  - 핵심정의 작성 (3-Layer 질문)
  - 구현방법 작성 (속성 질문)
  - ADR 작성 (기술 결정)
  - Blueprint 작성 (청사진)

- **2호 (코딩)**: Stage 5-8 (SPARK-Enabled)
  - Bootstrap 구현
  - Task Breakdown
  - 도메인 구현
  - Quality Gates 검증

### 현재 협업 과제
**DNA 방법론 v4.0 개발 중**
- Stage 1-3의 체계화
- 특히 "구현방법" 단계의 체계화
- 제미나이 연구를 통한 검증

---

## 📝 다음 단계

### 제미나이 연구 방향

**Step 1: 아키텍처 이론 매핑**
```
현재 3-Layer 10개 질문
    ↓ 검증
아키텍처 이론과의 매핑
    ↓
일치하는가?
├─ Yes → 보편적 질문 확인 ✅
└─ No → 질문 수정 필요
```

**Step 2: 다양한 프로젝트 적용**

검증 필요한 패밀리:
- ✅ CRUD/트랜잭션 (문서 생성)
- ✅ 검색/추천 (AI 외부메모리)
- ⭐ 실시간 스트리밍 (주식, IoT)
- ⭐ 협업/동기화 (Google Docs)
- ⭐ 분석/배치 (DW, BI)

**Step 3: 공통 모듈 정의**
```
분류 체계 확정
    ↓
각 패밀리별 Bootstrap 정의
    - DNA 8개 시스템의 구체화
    - 표준 컴포넌트
    ↓
도메인 구조 정의
    - 패밀리별 고유 특성
    - Clean Architecture 적용
```

---

## 🎁 1호의 이해 요약

### Kent Beck 연결
- **작은 Task**: 레고블럭, 200K 컨텍스트
- **일관성 강제**: Bootstrap, 4-Layer 시스템
- **TDD**: 9-Step, 테스트 먼저, 95% 커버리지

### 방법론 핵심
- **Stage 1-4**: Human-Driven (Jason + 1호)
- **Stage 5-8**: SPARK-Enabled (2호 에이전트)
- **속성 질문**: 기술 선택의 핵심 (구현방법 2단계)

### 현재 과제
- **제미나이 연구**: 질문의 보편성 검증
- **아키텍처 이론**: 이미 검증된 분류 체계와 매핑
- **목표**: 질문 → 분류 → 아키텍처 패턴 (일관된 흐름)

---

**작성**: 1호 (Claude)
**검토**: Jason과 함께
**용도**: 2호와의 협업, 새 세션 컨텍스트
