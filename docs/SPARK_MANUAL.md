# SPARK v3.0 Unified System - ì™„ì „ ë§¤ë‰´ì–¼

> **SPARK v3.0**ì€ ì—…ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ë©€í‹°ì—ì´ì „íŠ¸ ìë™í™” ì‹œìŠ¤í…œìœ¼ë¡œ, 88.4% í† í° ì ˆì•½ê³¼ 12ë‹¨ê³„ í’ˆì§ˆ ê²Œì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì¸ê°„-AI í˜‘ì—…ì„ í†µí•´ Jason(ì¸ê°„ ì•„í‚¤í…íŠ¸)ì™€ AI ì–´ì‹œìŠ¤í„´íŠ¸ê°€ í•¨ê»˜ ë§Œë“¤ì–´ë‚¸ í˜ì‹ ì ì¸ í†µí•© ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [SPARK ì‹œìŠ¤í…œ ê°œìš”](#spark-ì‹œìŠ¤í…œ-ê°œìš”)
2. [ì„œë¸Œì—ì´ì „íŠ¸ ëª…ë ¹ì–´ ì‘ì„±](#ì„œë¸Œì—ì´ì „íŠ¸-ëª…ë ¹ì–´-ì‘ì„±)
3. [Multi-Agent Pipeline êµ¬ì„±](#multi-agent-pipeline-êµ¬ì„±)
4. [Hook ì‹œìŠ¤í…œ ì„¤ì •](#hook-ì‹œìŠ¤í…œ-ì„¤ì •)
5. [Hook ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±](#hook-ìŠ¤í¬ë¦½íŠ¸-ì‘ì„±)
6. [JSON íŒŒì¼ êµ¬ì„±](#json-íŒŒì¼-êµ¬ì„±)
7. [ì‹¤ì œ êµ¬í˜„ ì˜ˆì œ](#ì‹¤ì œ-êµ¬í˜„-ì˜ˆì œ)
8. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
9. [Best Practices](#best-practices)

---

## SPARK ì‹œìŠ¤í…œ ê°œìš”

### í•µì‹¬ í˜ì‹ : ì§€ì—° ë¡œë”© (Lazy Loading)
- **ê¸°ì¡´ SuperClaude**: ëª¨ë“  16ê°œ ì—ì´ì „íŠ¸ë¥¼ í•œ ë²ˆì— ë¡œë“œ (44,000 í† í°)
- **SPARK**: í•„ìš”í•œ ì—ì´ì „íŠ¸ + ë¼ìš°í„°ë§Œ ë¡œë“œ (í‰ê·  5,100 í† í°)
- **ê²€ì¦ëœ ì„±ëŠ¥**: 88.4% í† í° ì ˆì•½, 78.7% ë¹ ë¥¸ ë¡œë“œ ì‹œê°„

### ì£¼ìš” êµ¬ì„± ìš”ì†Œ (v3.0 ê°•í™”)

1. **Unified Orchestrator** (`spark_unified_orchestrator.py`): 6ê°œ ìƒëª…ì£¼ê¸° í›… í†µí•© ê´€ë¦¬
2. **Smart Persona Router** (`spark_persona_router.py`): 8ê°œ í˜ë¥´ì†Œë‚˜ ëª¨ë“œë¡œ ì§€ëŠ¥í˜• ë¼ìš°íŒ…
3. **Quality Gates** (`spark_quality_gates.py`): 12ë‹¨ê³„ ê²€ì¦ (8ê°œ SPARK + 2ê°œ Jason DNA + 2ê°œ Unified)
4. **Specialized Agents** (`.claude/agents/`): 16ê°œ ëª¨ë“ˆí˜• ì—ì´ì „íŠ¸, ì˜¨ë””ë§¨ë“œ ë¡œë”©
5. **Security Layer**: SecureCommandExecutorë¡œ ì•…ì˜ì  ì‘ì—… ì°¨ë‹¨

### í† í° íš¨ìœ¨ì„± ì§€í‘œ (ê²€ì¦ë¨)
- SuperClaude: ìš”ì²­ë‹¹ 44,000 í† í°
- SPARK: í‰ê·  5,100 í† í° (88.4% ì ˆì•½)
- ë¹„ìš© ì ˆì•½: ìš”ì²­ë‹¹ $0.78

---

## ì„œë¸Œì—ì´ì „íŠ¸ ëª…ë ¹ì–´ ì‘ì„±

### A. ë‹¨ì¼ ì—ì´ì „íŠ¸ í˜¸ì¶œ ëª…ë ¹ì–´

#### ê¸°ë³¸ êµ¬ì¡°
ì„œë¸Œì—ì´ì „íŠ¸ëŠ” **Markdown íŒŒì¼**ë¡œ ì •ì˜ë˜ë©°, **YAML frontmatter**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ë°ì´í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

#### íŒŒì¼ ìœ„ì¹˜
```bash
# í”„ë¡œì íŠ¸ ë ˆë²¨ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
.claude/agents/

# ì‚¬ìš©ì ë ˆë²¨ (ëª¨ë“  í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
~/.claude/agents/
```

#### ë„¤ì´ë° ê·œì¹™
- SPARK ì—ì´ì „íŠ¸ëŠ” `-spark` ì ‘ë¯¸ì‚¬ ì‚¬ìš© (ì˜ˆ: `implementer-spark.md`)
- ì†Œë¬¸ìì™€ í•˜ì´í”ˆë§Œ ì‚¬ìš©
- ì—ì´ì „íŠ¸ì˜ ì—­í• ì„ ëª…í™•íˆ ë‚˜íƒ€ë‚´ëŠ” ì´ë¦„ ì‚¬ìš©

#### YAML Frontmatter êµ¬ì„±

```yaml
---
name: implementer-spark
description: SPARK-enhanced implementation agent with intelligent persona activation. Use for implementing tasks that require backend development, API creation, and zero-error precision.
tools: Bash, Glob, Grep, LS, Read, Edit, MultiEdit, Write, WebFetch, TodoWrite, WebSearch, mcp__sequential-thinking__sequentialthinking, mcp__context7__resolve-library-id, mcp__context7__get-library-docs
model: sonnet
color: blue
---
```

**í•„ìˆ˜ í•„ë“œ:**
- `name`: ê³ ìœ  ì‹ë³„ì (ì†Œë¬¸ìì™€ í•˜ì´í”ˆ)
- `description`: ì—ì´ì „íŠ¸ì˜ ëª©ì ê³¼ ì‚¬ìš© ì‹œì 

**ì„ íƒì  í•„ë“œ:**
- `tools`: ì‚¬ìš©í•  ë„êµ¬ ëª©ë¡ (ìƒëµì‹œ ëª¨ë“  ë„êµ¬ ìƒì†)
- `model`: ì‚¬ìš©í•  AI ëª¨ë¸ (opus, sonnet, haiku)
- `color`: UIì—ì„œ í‘œì‹œë  ìƒ‰ìƒ

#### ì—ì´ì „íŠ¸ ë³¸ë¬¸ ì‘ì„±

```markdown
---
name: analyzer-spark
description: Code analysis specialist for performance optimization and debugging
tools: Bash, Glob, Grep, Read, WebFetch
model: sonnet
---

# ğŸ” SPARK Code Analyzer

## Identity & Philosophy
I am the SPARK Code Analyzer, specializing in comprehensive code analysis...

## Core Responsibilities
1. Performance bottleneck identification
2. Code quality assessment
3. Security vulnerability scanning
4. Architecture evaluation

## Analysis Workflow
### Phase 1: Code Discovery
```bash
# Find all relevant files
find . -name "*.py" -o -name "*.js" -o -name "*.ts" | head -20
```

### Phase 2: Performance Analysis
- Identify N+1 query problems
- Detect inefficient algorithms
- Analyze memory usage patterns
...
```

### ì—ì´ì „íŠ¸ í…œí”Œë¦¿

```markdown
---
name: [agent-name]-spark
description: [ì—ì´ì „íŠ¸ ëª©ì ê³¼ ì‚¬ìš© ìƒí™© ì„¤ëª…]
tools: [í•„ìš”í•œ ë„êµ¬ë“¤ì„ ì½¤ë§ˆë¡œ êµ¬ë¶„]
model: [sonnet/opus/haiku]
color: [blue/green/purple/red]
---

# ğŸ¯ SPARK [Agent Type]

## Identity & Philosophy
[ì—ì´ì „íŠ¸ì˜ ì •ì²´ì„±ê³¼ í•µì‹¬ ì² í•™]

## Core Responsibilities
1. [ì£¼ìš” ì±…ì„ 1]
2. [ì£¼ìš” ì±…ì„ 2]
3. [ì£¼ìš” ì±…ì„ 3]

## Workflow
### Phase 1: [ë‹¨ê³„ ì´ë¦„]
[êµ¬ì²´ì ì¸ ì‘ì—… ë‹¨ê³„]

### Phase 2: [ë‹¨ê³„ ì´ë¦„]
[êµ¬ì²´ì ì¸ ì‘ì—… ë‹¨ê³„]

## Quality Standards
[í’ˆì§ˆ ê¸°ì¤€ê³¼ ì„±ê³µ ì§€í‘œ]

## Usage Examples
[ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ]
```

---

## Multi-Agent Pipeline êµ¬ì„±

### B. ì—¬ëŸ¬ ì—ì´ì „íŠ¸ ë‹¨ê³„ë³„ í˜¸ì¶œ

SPARKëŠ” ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ì§€ì›í•©ë‹ˆë‹¤.

#### Pipeline ëª…ë ¹ì–´ êµ¬ì¡°

Pipeline ëª…ë ¹ì–´ëŠ” ìŠ¬ë˜ì‹œ ì»¤ë§¨ë“œë¡œ êµ¬í˜„ë˜ë©°, `.claude/commands/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤.

#### spark-launch.md ì˜ˆì œ ë¶„ì„

```markdown
---
allowed-tools: Task, Bash, Read, Write, Edit, MultiEdit
description: Complete feature development pipeline from design to deployment
model: sonnet
---

# /spark-launch - SPARK Full-Stack Feature Launch Pipeline

**Purpose**: Complete feature development from design to deployment with quality assurance

## Execution Instructions

When this command is called, execute the following end-to-end development pipeline:

### Phase 1: Requirements Analysis & Design
Use Task tool with subagent_type: "analyzer-spark" to:
1. Analyze requirements and complexity
2. Create technical specification
3. Design system architecture

### Phase 2: Implementation
Use Task tool with subagent_type: "implementer-spark" to:
1. Implement core functionality
2. Apply SPARK quality standards
3. Ensure 8-step quality gate compliance

### Phase 3: Testing & Validation
Use Task tool with subagent_type: "tester-spark" to:
1. Create comprehensive tests
2. Validate functionality
3. Perform integration testing

### Phase 4: Documentation
Use Task tool with subagent_type: "documenter-spark" to:
1. Generate API documentation
2. Create user guides
3. Document architecture decisions

## Usage Examples
```bash
/spark-launch "user notification system with email and SMS support"
/spark-launch "real-time chat feature with file sharing capabilities"
```
```

#### ì—ì´ì „íŠ¸ ê°„ ë°ì´í„° ì „ë‹¬ ë©”ì»¤ë‹ˆì¦˜

```python
# AgentChainManagerë¥¼ í†µí•œ ë°ì´í„° ì „ë‹¬
chain_manager = AgentChainManager()

# ì²´ì¸ ì‹œì‘
chain_manager.start_chain("feature-123", [
    "analyzer-spark",
    "implementer-spark", 
    "tester-spark",
    "documenter-spark"
])

# ë°ì´í„° ì „ë‹¬
chain_manager.pass_data(
    from_agent="analyzer-spark",
    to_agent="implementer-spark",
    data={
        "requirements": requirements,
        "architecture": architecture_spec,
        "complexity_score": 0.8
    }
)

# ë°ì´í„° ë°›ê¸°
inherited_data = chain_manager.get_data("implementer-spark")
```

#### Pipeline ëª…ë ¹ì–´ í…œí”Œë¦¿

```markdown
---
allowed-tools: Task, Bash, Read, Write, Edit, MultiEdit
description: [íŒŒì´í”„ë¼ì¸ ëª©ì  ì„¤ëª…]
model: [ì„ í˜¸í•˜ëŠ” ëª¨ë¸]
argument-hint: [ì¸ì íŒíŠ¸]
---

# /[command-name] - [Pipeline Title]

**Purpose**: [íŒŒì´í”„ë¼ì¸ì˜ ëª©ì ê³¼ ë²”ìœ„]

## Execution Instructions

### Phase 1: [ì²« ë²ˆì§¸ ë‹¨ê³„]
Use Task tool with subagent_type: "[agent-name]-spark" to:
1. [êµ¬ì²´ì  ì‘ì—… 1]
2. [êµ¬ì²´ì  ì‘ì—… 2]
3. [êµ¬ì²´ì  ì‘ì—… 3]

### Phase 2: [ë‘ ë²ˆì§¸ ë‹¨ê³„]  
Use Task tool with subagent_type: "[agent-name]-spark" to:
1. [êµ¬ì²´ì  ì‘ì—… 1]
2. [êµ¬ì²´ì  ì‘ì—… 2]
3. [êµ¬ì²´ì  ì‘ì—… 3]

### Phase N: [ë§ˆì§€ë§‰ ë‹¨ê³„]
Use Task tool with subagent_type: "[agent-name]-spark" to:
1. [êµ¬ì²´ì  ì‘ì—… 1]
2. [êµ¬ì²´ì  ì‘ì—… 2]
3. [êµ¬ì²´ì  ì‘ì—… 3]

## Data Flow
```
[Agent 1] â†’ [analysis_result] â†’ [Agent 2] â†’ [implementation] â†’ [Agent 3]
```

## Usage Examples
```bash
/[command-name] "[ì˜ˆì œ ì…ë ¥ 1]"
/[command-name] "[ì˜ˆì œ ì…ë ¥ 2]"
```
```

---

## Hook ì‹œìŠ¤í…œ ì„¤ì •

### C. í›… ì„¤ì • ë°©ë²•

Hookì€ Claude Codeì˜ íŠ¹ì • ì´ë²¤íŠ¸ì—ì„œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

#### settings.jsonì—ì„œ Hook êµ¬ì„±

```json
{
  "hooks": {
    "userPromptSubmit": [
      {
        "hooks": [
          {
            "type": "command",
            "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/spark_persona_router.py"
          }
        ]
      }
    ],
    "subagentStop": [
      {
        "hooks": [
          {
            "type": "command", 
            "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/spark_quality_gates.py"
          }
        ]
      }
    ],
    "postToolUse": [
      {
        "matcher": "Edit|Write|MultiEdit",
        "hooks": [
          {
            "type": "command",
            "command": "ruff format . --quiet 2>/dev/null || true",
            "timeout": 10
          }
        ]
      }
    ]
  }
}
```

#### Hook ì´ë²¤íŠ¸ íƒ€ì…

1. **UserPromptSubmit**: ì‚¬ìš©ìê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ì œì¶œí•  ë•Œ
2. **SubagentStop**: ì„œë¸Œì—ì´ì „íŠ¸ ì‘ì—…ì´ ì™„ë£Œë  ë•Œ
3. **PreToolUse**: ë„êµ¬ ì‹¤í–‰ ì „
4. **PostToolUse**: ë„êµ¬ ì‹¤í–‰ í›„
5. **Stop**: Claude ì‘ë‹µ ì™„ë£Œ ì§ì „
6. **Notification**: ì•Œë¦¼ ì „ì†¡ ì‹œ
7. **PreCompact**: ëŒ€í™” ì••ì¶• ì „

#### í™˜ê²½ ë³€ìˆ˜ í™œìš©

```json
{
  "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/my_hook.py"
}
```

**ì‚¬ìš© ê°€ëŠ¥í•œ í™˜ê²½ ë³€ìˆ˜:**
- `CLAUDE_PROJECT_DIR`: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì˜ ì ˆëŒ€ ê²½ë¡œ

#### Hook ì„¤ì • ìœ„ì¹˜

```bash
# í”„ë¡œì íŠ¸ ì„¤ì • (ìš°ì„ ìˆœìœ„: ê°€ì¥ ë†’ìŒ)
.claude/settings.json

# ë¡œì»¬ í”„ë¡œì íŠ¸ ì„¤ì • (ì»¤ë°‹í•˜ì§€ ì•ŠìŒ)
.claude/settings.local.json

# ì‚¬ìš©ì ì„¤ì • (ëª¨ë“  í”„ë¡œì íŠ¸ì— ì ìš©)
~/.claude/settings.json
```

---

## Hook ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

### D. í›… ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ë°©ë²•

#### Python Hook ìŠ¤í¬ë¦½íŠ¸ êµ¬ì¡°

```python
#!/usr/bin/env python3
"""
SPARK Hook Example
Description of what this hook does
"""

import json
import logging
import sys
from pathlib import Path

# Set up logging to stderr (stdoutëŠ” Claudeì—ê²Œ ì „ë‹¬ë¨)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stderr)]
)
logger = logging.getLogger(__name__)


def main():
    """Main hook execution"""
    try:
        # stdinì—ì„œ JSON ì…ë ¥ ì½ê¸°
        input_data = json.load(sys.stdin)
        
        # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬
        process_input(input_data)
        
        # ê²°ê³¼ ì¶œë ¥ (JSON í˜•ì‹)
        output = {
            "hookSpecificOutput": {
                # hookë³„ íŠ¹ì • ì¶œë ¥ í•„ë“œ
            }
        }
        
        print(json.dumps(output))
        sys.exit(0)
        
    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON input: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Hook execution failed: {e}")
        sys.exit(1)


def process_input(input_data):
    """Process hook input data"""
    # ê³µí†µ í•„ë“œ
    session_id = input_data.get("session_id")
    transcript_path = input_data.get("transcript_path")
    cwd = input_data.get("cwd")
    hook_event_name = input_data.get("hook_event_name")
    
    # ì´ë²¤íŠ¸ë³„ íŠ¹ì • í•„ë“œ ì²˜ë¦¬
    if hook_event_name == "UserPromptSubmit":
        prompt = input_data.get("prompt", "")
        # í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ë¡œì§
        
    elif hook_event_name == "SubagentStop":
        # ì„œë¸Œì—ì´ì „íŠ¸ ì™„ë£Œ ì²˜ë¦¬ ë¡œì§
        pass


if __name__ == "__main__":
    main()
```

#### stdin/stdoutì„ í†µí•œ ë°ì´í„° ì²˜ë¦¬

**ì…ë ¥ (stdin)**:
```python
# JSON ì…ë ¥ ì½ê¸°
input_data = json.load(sys.stdin)

# ì…ë ¥ ë°ì´í„° êµ¬ì¡° (UserPromptSubmit ì˜ˆì œ)
{
  "session_id": "abc123",
  "transcript_path": "/Users/.../.claude/projects/.../conversation.jsonl",
  "cwd": "/Users/...",
  "hook_event_name": "UserPromptSubmit",
  "prompt": "implement user authentication"
}
```

**ì¶œë ¥ (stdout)**:
```python
# JSON ì¶œë ¥ (UserPromptSubmitìš©)
output = {
    "hookSpecificOutput": {
        "hookEventName": "UserPromptSubmit",
        "additionalContext": "ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©"
    }
}
print(json.dumps(output))
```

#### JSON í˜•ì‹ì˜ ì¶œë ¥ êµ¬ì„±

**UserPromptSubmit Hook ì¶œë ¥**:
```python
def format_user_prompt_submit(additional_context: str, metadata: dict = None):
    output = {
        "hookSpecificOutput": {
            "hookEventName": "UserPromptSubmit",
            "additionalContext": additional_context
        }
    }
    
    if metadata:
        output["metadata"] = metadata
        
    return json.dumps(output)
```

**SubagentStop Hook ì¶œë ¥**:
```python
def format_subagent_stop(decision: str, reason: str, retry_prompt: str = None):
    output = {
        "hookSpecificOutput": {
            "hookEventName": "SubagentStop",
            "decision": decision,  # "block" ë˜ëŠ” "continue"
            "reason": reason
        }
    }
    
    if decision == "block" and retry_prompt:
        output["hookSpecificOutput"]["retryPrompt"] = retry_prompt
        
    return json.dumps(output)
```

#### ì—ëŸ¬ ì²˜ë¦¬ ë° Decision ë°˜í™˜

```python
def safe_execution_wrapper():
    try:
        # ë©”ì¸ ë¡œì§ ì‹¤í–‰
        result = execute_main_logic()
        
        if result.success:
            return {
                "hookSpecificOutput": {
                    "decision": "continue",
                    "reason": "All checks passed"
                }
            }
        else:
            return {
                "hookSpecificOutput": {
                    "decision": "block",
                    "reason": f"Validation failed: {result.error}"
                }
            }
            
    except Exception as e:
        logger.exception("Hook execution failed")
        return {
            "hookSpecificOutput": {
                "decision": "block",
                "reason": f"System error: {str(e)}"
            }
        }
```

#### ì‹¤ì œ Hook ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì œ ë¶„ì„

**spark_persona_router.py í•µì‹¬ êµ¬ì¡°**:
```python
class PersonaAnalyzer:
    """í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ í˜ë¥´ì†Œë‚˜ í™œì„±í™”ë¥¼ ê²°ì •"""
    
    PERSONA_KEYWORDS = {
        "backend": {
            "keywords": ["api", "endpoint", "service", "server"],
            "personas": ["Backend Developer"],
            "agents": ["implementer-spark"]
        },
        # ... ë” ë§ì€ í˜ë¥´ì†Œë‚˜ ì •ì˜
    }
    
    @classmethod
    def analyze_and_activate(cls, prompt: str):
        keywords = cls.extract_keywords(prompt)
        complexity, reasoning = cls.calculate_complexity(prompt)
        personas, agents = cls.determine_personas_and_agents(keywords, complexity)
        
        return {
            "active_personas": personas,
            "recommended_agents": agents,
            "complexity_score": complexity,
            "reasoning": reasoning
        }
```

**spark_quality_gates.py í•µì‹¬ êµ¬ì¡°**:
```python
class QualityGateRunner:
    """í’ˆì§ˆ ê²Œì´íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ê´€ë¦¬"""
    
    def run_gates(self, required_gates: int = 8):
        results = {}
        passed_count = 0
        
        for gate in self.gates[:required_gates]:
            passed, issues = gate.check()
            results[gate.name] = {
                "passed": passed,
                "issues": issues
            }
            if passed:
                passed_count += 1
        
        return {
            "passed": passed_count >= required_gates,
            "results": results,
            "pass_rate": (passed_count / required_gates) * 100
        }
```

---

## JSON íŒŒì¼ êµ¬ì„±

### E. JSON íŒŒì¼ êµ¬ì„± ë° êµ¬í˜„ ë°©ë²•

#### current_task.json êµ¬ì¡°

SPARK ì‹œìŠ¤í…œì˜ ìƒíƒœëŠ” `current_task.json` íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤.

```json
{
  "task_id": "spark_20250808_104500",
  "prompt": "implement REST API endpoint for user authentication...",
  "personas": ["Backend Developer", "Security Expert"],
  "agents": ["implementer-spark", "security-spark"],
  "complexity": 0.75,
  "complexity_reasoning": "High complexity: authentication; Medium complexity: API",
  "keywords": {
    "backend": ["api", "endpoint"],
    "security": ["auth", "authentication"]
  },
  "quality_gates": {
    "required": 8,
    "passed": 0,
    "results": {},
    "last_run": "2025-08-08T10:45:00.000000"
  },
  "pipeline": {
    "chain_id": "feature_auth_001",
    "agents": ["analyzer-spark", "implementer-spark", "tester-spark"],
    "current_index": 1,
    "current_agent": "implementer-spark",
    "completed_agents": ["analyzer-spark"],
    "data_passing": {
      "analyzer-spark->implementer-spark": {
        "from": "analyzer-spark",
        "to": "implementer-spark",
        "data": {
          "requirements": {...},
          "architecture": {...}
        },
        "timestamp": "2025-08-08T10:45:00.000000"
      }
    },
    "started_at": "2025-08-08T10:45:00.000000"
  },
  "spark_activation": {
    "active_personas": ["backend", "security"],
    "mcp_servers": ["sequential", "context7"],
    "complexity_score": 0.75,
    "quality_gates_required": 8,
    "activation_timestamp": "2025-08-08T10:45:00.000000",
    "routing_strategy": "intelligent_persona_selection"
  },
  "created_at": "2025-08-08T10:45:00.000000",
  "last_updated": "2025-08-08T10:45:00.000000",
  "version": "1.0.0"
}
```

#### ì›Œí¬í”Œë¡œìš° ìƒíƒœ ê´€ë¦¬

```python
from spark_core_utils import StateManager

# ìƒíƒœ ê´€ë¦¬ì ì´ˆê¸°í™”
state_manager = StateManager()

# ìƒíƒœ ì½ê¸°
current_state = state_manager.read_state()

# ìƒíƒœ ì—…ë°ì´íŠ¸
state_manager.update_state({
    "quality_gates": {
        "passed": 6,
        "results": {...}
    }
})

# ì™„ì „í•œ ìƒíƒœ ì‘ì„±
new_state = {
    "task_id": "new_task_001",
    "complexity": 0.8,
    # ... ê¸°íƒ€ í•„ë“œ
}
state_manager.write_state(new_state)
```

#### ì—ì´ì „íŠ¸ ê°„ ìƒíƒœ ê³µìœ  ë©”ì»¤ë‹ˆì¦˜

```python
from spark_core_utils import AgentChainManager

chain_manager = AgentChainManager()

# ì²´ì¸ ì‹œì‘
chain_manager.start_chain("pipeline_001", [
    "analyzer-spark",
    "implementer-spark",
    "tester-spark"
])

# ë°ì´í„° ì „ë‹¬
chain_manager.pass_data(
    from_agent="analyzer-spark",
    to_agent="implementer-spark",
    data={
        "requirements": analysis_results,
        "complexity": 0.8,
        "recommendations": ["use FastAPI", "implement JWT"]
    }
)

# ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ ì´ë™
next_agent = chain_manager.advance_chain()

# ì²´ì¸ ìƒíƒœ í™•ì¸
status = chain_manager.get_chain_status()
print(f"Current: {status['current_agent']}")
print(f"Progress: {status['progress']}%")
```

#### JSON ìŠ¤í‚¤ë§ˆ ë° í•„ë“œ ì„¤ëª…

**Root Level Fields**:
- `task_id`: ê³ ìœ í•œ ì‘ì—… ì‹ë³„ì
- `prompt`: ì›ë³¸ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ (500ìë¡œ ì œí•œ)
- `personas`: í™œì„±í™”ëœ í˜ë¥´ì†Œë‚˜ ëª©ë¡
- `agents`: ì¶”ì²œëœ ì—ì´ì „íŠ¸ ëª©ë¡
- `complexity`: ë³µì¡ë„ ì ìˆ˜ (0.0-1.0)
- `complexity_reasoning`: ë³µì¡ë„ íŒë‹¨ ê·¼ê±°

**Quality Gates Object**:
```json
{
  "quality_gates": {
    "required": 8,           // í•„ìš”í•œ í’ˆì§ˆ ê²Œì´íŠ¸ ìˆ˜
    "passed": 6,             // í†µê³¼í•œ ê²Œì´íŠ¸ ìˆ˜
    "results": {             // ê° ê²Œì´íŠ¸ë³„ ìƒì„¸ ê²°ê³¼
      "Syntax Validation": {
        "passed": true,
        "issues": []
      },
      "Type Checking": {
        "passed": false,
        "issues": ["MyPy found 2 type errors"]
      }
    },
    "last_run": "2025-08-08T10:45:00.000000"
  }
}
```

**Pipeline Object**:
```json
{
  "pipeline": {
    "chain_id": "unique_chain_id",     // íŒŒì´í”„ë¼ì¸ ì‹ë³„ì
    "agents": ["agent1", "agent2"],    // ì „ì²´ ì—ì´ì „íŠ¸ ìˆœì„œ
    "current_index": 1,                // í˜„ì¬ ì—ì´ì „íŠ¸ ì¸ë±ìŠ¤
    "current_agent": "agent2",         // í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì—ì´ì „íŠ¸
    "completed_agents": ["agent1"],    // ì™„ë£Œëœ ì—ì´ì „íŠ¸ ëª©ë¡
    "data_passing": {                  // ì—ì´ì „íŠ¸ ê°„ ë°ì´í„° ì „ë‹¬
      "agent1->agent2": {
        "from": "agent1",
        "to": "agent2", 
        "data": {...},
        "timestamp": "..."
      }
    }
  }
}
```

---

## ì‹¤ì œ êµ¬í˜„ ì˜ˆì œ

### ì™„ì „í•œ ì˜ˆì œ: ì‚¬ìš©ì ì¸ì¦ ì‹œìŠ¤í…œ êµ¬í˜„

#### 1ë‹¨ê³„: ìŠ¬ë˜ì‹œ ëª…ë ¹ì–´ ìƒì„±

```bash
# .claude/commands/auth-system.md ìƒì„±
mkdir -p .claude/commands
```

```markdown
---
allowed-tools: Task, Bash, Read, Write, Edit, MultiEdit
description: Complete user authentication system with JWT and security features
model: sonnet
argument-hint: [feature-description]
---

# /auth-system - User Authentication System Pipeline

## Phase 1: Security Analysis
Use Task tool with subagent_type: "analyzer-spark" to:
1. Analyze security requirements for authentication
2. Identify potential vulnerabilities
3. Define security architecture

## Phase 2: Implementation  
Use Task tool with subagent_type: "implementer-spark" to:
1. Implement JWT authentication
2. Create secure endpoints
3. Add password hashing and validation

## Phase 3: Testing
Use Task tool with subagent_type: "tester-spark" to:
1. Create security tests
2. Test authentication flows
3. Validate JWT handling

## Usage
```bash
/auth-system "JWT-based authentication with role-based access control"
```
```

#### 2ë‹¨ê³„: Hook ì„¤ì •

```json
{
  "hooks": {
    "userPromptSubmit": [
      {
        "hooks": [
          {
            "type": "command",
            "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/spark_persona_router.py"
          }
        ]
      }
    ],
    "subagentStop": [
      {
        "hooks": [
          {
            "type": "command",
            "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/spark_quality_gates.py"
          }
        ]
      }
    ]
  }
}
```

#### 3ë‹¨ê³„: ì—ì´ì „íŠ¸ ì‹¤í–‰ íë¦„

```bash
# 1. ì‚¬ìš©ìê°€ ëª…ë ¹ì–´ ì‹¤í–‰
/auth-system "secure user authentication with JWT"

# 2. UserPromptSubmit Hook ì‹¤í–‰
# - spark_persona_router.pyê°€ ì‹¤í–‰ë¨
# - "auth", "security", "JWT" í‚¤ì›Œë“œ ê°ì§€
# - Security + Backend í˜ë¥´ì†Œë‚˜ í™œì„±í™”
# - implementer-spark, security-spark ì—ì´ì „íŠ¸ ì¶”ì²œ

# 3. analyzer-spark ì—ì´ì „íŠ¸ ì‹¤í–‰
# - ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ ë¶„ì„
# - ìœ„í—˜ ìš”ì†Œ ì‹ë³„
# - ì•„í‚¤í…ì²˜ ì„¤ê³„

# 4. SubagentStop Hook ì‹¤í–‰
# - spark_quality_gates.pyê°€ ì‹¤í–‰ë¨  
# - 8ê°œ í’ˆì§ˆ ê²Œì´íŠ¸ ê²€ì¦
# - í†µê³¼ ì‹œ ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ ì§„í–‰

# 5. implementer-spark ì—ì´ì „íŠ¸ ì‹¤í–‰
# - JWT ì¸ì¦ êµ¬í˜„
# - ë³´ì•ˆ ì—”ë“œí¬ì¸íŠ¸ ìƒì„±
# - íŒ¨ìŠ¤ì›Œë“œ í•´ì‹± êµ¬í˜„

# 6. ìµœì¢… í’ˆì§ˆ ê²€ì¦
# - ëª¨ë“  í’ˆì§ˆ ê²Œì´íŠ¸ í†µê³¼ í™•ì¸
# - current_task.jsonì— ê²°ê³¼ ê¸°ë¡
```

#### 4ë‹¨ê³„: ì‹¤í–‰ ê²°ê³¼ í™•ì¸

```bash
# current_task.json í™•ì¸
cat .claude/workflows/current_task.json
```

```json
{
  "task_id": "spark_20250808_145500",
  "personas": ["Backend Developer", "Security Expert"],
  "agents": ["implementer-spark", "security-spark"],
  "complexity": 0.85,
  "quality_gates": {
    "required": 8,
    "passed": 8,
    "results": {
      "Syntax Validation": {"passed": true, "issues": []},
      "Security Analysis": {"passed": true, "issues": []},
      // ... ê¸°íƒ€ ê²Œì´íŠ¸ ê²°ê³¼
    }
  },
  "pipeline": {
    "current_agent": null,
    "completed_agents": ["analyzer-spark", "implementer-spark"],
    "completed_at": "2025-08-08T14:55:00.000000"
  }
}
```

### Mini Pipeline ì˜ˆì œ

ì‘ì€ ê·œëª¨ì˜ íŒŒì´í”„ë¼ì¸ ì˜ˆì œì…ë‹ˆë‹¤.

#### simple-api.md
```markdown
---
allowed-tools: Task, Bash, Read, Write
description: Simple API endpoint creation
---

# /simple-api - Quick API Creation

## Phase 1: Implementation
Use Task tool with subagent_type: "implementer-spark" to:
1. Create FastAPI endpoint
2. Add basic validation
3. Test endpoint functionality

## Usage
```bash
/simple-api "create GET /users endpoint"
```
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°ë°©ë²•

#### Hookì´ ì‹¤í–‰ë˜ì§€ ì•Šì„ ë•Œ
1. **ì„¤ì • íŒŒì¼ ìœ„ì¹˜ í™•ì¸**
   ```bash
   # ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ì„¤ì • íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
   ls -la .claude/settings.json
   ```

2. **Hook ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ í™•ì¸**
   ```bash
   chmod +x .claude/hooks/spark_persona_router.py
   ```

3. **JSON êµ¬ë¬¸ ì˜¤ë¥˜ í™•ì¸**
   ```bash
   python -m json.tool .claude/settings.json
   ```

4. **Claude Code ì¬ì‹œì‘**
   - Hook ì„¤ì • ë³€ê²½ í›„ì—ëŠ” Claude Code ì¬ì‹œì‘ í•„ìš”

#### í’ˆì§ˆ ê²Œì´íŠ¸ ì‹¤íŒ¨ ì‹œ
1. **ìƒì„¸ ë¡œê·¸ í™•ì¸**
   ```bash
   # stderr ì¶œë ¥ì—ì„œ êµ¬ì²´ì ì¸ ì˜¤ë¥˜ í™•ì¸
   tail -f ~/.claude/logs/hooks.log
   ```

2. **ê°œë³„ í’ˆì§ˆ ë„êµ¬ ì‹¤í–‰**
   ```bash
   # MyPy íƒ€ì… ì²´í¬
   mypy . --strict
   
   # Ruff ë¦°íŒ…
   ruff check .
   
   # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
   pytest tests/ -v
   ```

3. **current_task.json ìƒíƒœ í™•ì¸**
   ```bash
   cat .claude/workflows/current_task.json | jq '.quality_gates.results'
   ```

#### ì—ì´ì „íŠ¸ ì²´ì¸ ì˜¤ë¥˜ ì‹œ
1. **ì²´ì¸ ìƒíƒœ ë¦¬ì…‹**
   ```python
   from spark_core_utils import StateManager
   state_manager = StateManager()
   state_manager.clear_state()
   ```

2. **ë°ì´í„° ì „ë‹¬ í™•ì¸**
   ```python
   from spark_core_utils import AgentChainManager
   chain_manager = AgentChainManager()
   status = chain_manager.get_chain_status()
   print(status)
   ```

#### í˜ë¥´ì†Œë‚˜ ë¼ìš°í„° ì˜¤ì‘ë™ ì‹œ
1. **í‚¤ì›Œë“œ ë§¤í•‘ í™•ì¸**
   ```python
   # spark_persona_router.pyì—ì„œ í‚¤ì›Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸
   from spark_persona_router import PersonaAnalyzer
   
   analyzer = PersonaAnalyzer()
   result = analyzer.extract_keywords("implement secure API endpoint")
   print(result)
   ```

2. **ë³µì¡ë„ ê³„ì‚° í™•ì¸**
   ```python
   complexity, reasoning = analyzer.calculate_complexity(
       "implement enterprise-grade microservice architecture"
   )
   print(f"Complexity: {complexity}, Reasoning: {reasoning}")
   ```

### ë””ë²„ê¹… íŒ

#### Hook ì‹¤í–‰ ì¶”ì 
```bash
# --debug ëª¨ë“œë¡œ Claude Code ì‹¤í–‰
claude --debug

# ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export CLAUDE_DEBUG=1
claude
```

#### ìƒì„¸ ë¡œê¹… í™œì„±í™”
```python
# Hook ìŠ¤í¬ë¦½íŠ¸ì— ë””ë²„ê·¸ ë¡œê¹… ì¶”ê°€
import logging
logging.basicConfig(level=logging.DEBUG)

def debug_input_data():
    logger.debug("=== Hook Input Debug ===")
    logger.debug(f"Input data: {json.dumps(input_data, indent=2)}")
    logger.debug("========================")
```

#### JSON ì¶œë ¥ ê²€ì¦
```bash
# Hook ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ í…ŒìŠ¤íŠ¸
echo '{"prompt": "test"}' | python3 .claude/hooks/spark_persona_router.py
```

---

## Best Practices

### ê°œë°œ ëª¨ë²” ì‚¬ë¡€

#### 1. ì—ì´ì „íŠ¸ ì„¤ê³„ ì›ì¹™
- **ë‹¨ì¼ ì±…ì„**: ê° ì—ì´ì „íŠ¸ëŠ” í•˜ë‚˜ì˜ ëª…í™•í•œ ì—­í• 
- **ëª¨ë“ˆí™”**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ì„± ìš”ì†Œë¡œ ì„¤ê³„
- **ëª…í™•í•œ ì¸í„°í˜ì´ìŠ¤**: ì…ë ¥ê³¼ ì¶œë ¥ì„ ëª…í™•íˆ ì •ì˜
- **ì˜¤ë¥˜ ì²˜ë¦¬**: ëª¨ë“  ì˜ˆì™¸ ìƒí™©ì— ëŒ€í•œ ì ì ˆí•œ ì²˜ë¦¬

#### 2. Pipeline ì„¤ê³„ ê°€ì´ë“œë¼ì¸
- **ë‹¨ê³„ë³„ ë¶„ë¦¬**: ê° ë‹¨ê³„ì˜ ì±…ì„ì„ ëª…í™•íˆ êµ¬ë¶„
- **ë°ì´í„° ì „ë‹¬**: í•„ìš”í•œ ì •ë³´ë§Œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬
- **ë³‘ëª© ì§€ì  ìµœì†Œí™”**: ê° ë‹¨ê³„ì˜ ì‹¤í–‰ ì‹œê°„ ìµœì í™”
- **ë¡¤ë°± ê°€ëŠ¥**: ì‹¤íŒ¨ ì‹œ ì´ì „ ìƒíƒœë¡œ ë³µêµ¬ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„

#### 3. Hook ìŠ¤í¬ë¦½íŠ¸ ìµœì í™”
```python
# ì¢‹ì€ ì˜ˆ: ì•ˆì „í•œ ëª…ë ¹ ì‹¤í–‰
from spark_core_utils import SecureCommandExecutor

executor = SecureCommandExecutor()
success, stdout, stderr = executor.run_command(
    ["python3", "-m", "mypy", "src/"],
    timeout=30
)

# ë‚˜ìœ ì˜ˆ: ë³´ì•ˆì— ì·¨ì•½í•œ ì‹¤í–‰
import os
os.system(f"mypy {file_path}")  # ì ˆëŒ€ ì´ë ‡ê²Œ í•˜ì§€ ë§ˆì„¸ìš”!
```

#### 4. ìƒíƒœ ê´€ë¦¬ ìµœì í™”
```python
# ì¢‹ì€ ì˜ˆ: ì›ìì  ìƒíƒœ ì—…ë°ì´íŠ¸
state_manager = StateManager()
current_state = state_manager.read_state()
current_state.update(new_data)
state_manager.write_state(current_state)

# ë‚˜ìœ ì˜ˆ: ë¶€ë¶„ì  ì—…ë°ì´íŠ¸
state_manager.update_state({"field1": value1})
state_manager.update_state({"field2": value2})  # ë ˆì´ìŠ¤ ì»¨ë””ì…˜ ê°€ëŠ¥
```

### ì„±ëŠ¥ ìµœì í™”

#### 1. í† í° íš¨ìœ¨ì„± ìœ ì§€
- **ì§€ì—° ë¡œë”©**: í•„ìš”í•œ ì—ì´ì „íŠ¸ë§Œ í™œì„±í™”
- **ì»¨í…ìŠ¤íŠ¸ ìµœì†Œí™”**: ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ì œì™¸
- **ìºì‹± í™œìš©**: ë°˜ë³µì ì¸ ë¶„ì„ ê²°ê³¼ëŠ” ìºì‹œ

#### 2. í’ˆì§ˆ ê²Œì´íŠ¸ ìµœì í™”
```python
# ë³‘ë ¬ í’ˆì§ˆ ê²Œì´íŠ¸ ì‹¤í–‰
import concurrent.futures

def run_gates_parallel(gates):
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(gate.check) for gate in gates]
        results = {}
        
        for gate, future in zip(gates, futures):
            try:
                passed, issues = future.result(timeout=30)
                results[gate.name] = {"passed": passed, "issues": issues}
            except Exception as e:
                results[gate.name] = {"passed": False, "issues": [str(e)]}
                
        return results
```

#### 3. ë©”ëª¨ë¦¬ ê´€ë¦¬
- **ëŒ€ìš©ëŸ‰ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°**: í° íŒŒì¼ì€ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
- **ì„ì‹œ íŒŒì¼ ì •ë¦¬**: ì‘ì—… ì™„ë£Œ í›„ ì„ì‹œ íŒŒì¼ ì‚­ì œ
- **ìƒíƒœ í¬ê¸° ì œí•œ**: current_task.json í¬ê¸°ë¥¼ í•©ë¦¬ì ìœ¼ë¡œ ìœ ì§€

### ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

#### 1. Hook ë³´ì•ˆ
```python
# ì…ë ¥ ê²€ì¦
def validate_input_path(path: str) -> bool:
    """ê²½ë¡œ ìˆœíšŒ ê³µê²© ë°©ì§€"""
    safe_path = SecureCommandExecutor.sanitize_path(path)
    return safe_path is not None

# ëª…ë ¹ì–´ ì‹¤í–‰ ë³´ì•ˆ
def safe_command_execution(command_args: List[str]):
    """ì…¸ ì¸ì ì…˜ ë°©ì§€"""
    return SecureCommandExecutor.run_command(
        command_args,  # ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬ (shell=False)
        timeout=30
    )
```

#### 2. ë¯¼ê°í•œ ì •ë³´ ë³´í˜¸
```python
# ë¡œê·¸ì—ì„œ ë¯¼ê°í•œ ì •ë³´ ì œê±°
def sanitize_log_data(data: dict) -> dict:
    """ë¡œê·¸ì—ì„œ ë¯¼ê°í•œ ì •ë³´ ì œê±°"""
    sensitive_keys = ["password", "token", "secret", "key"]
    sanitized = data.copy()
    
    for key in sensitive_keys:
        if key in sanitized:
            sanitized[key] = "***REDACTED***"
            
    return sanitized
```

### ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

#### 1. êµ¬ì¡°í™”ëœ ë¡œê¹…
```python
import structlog

logger = structlog.get_logger()

def log_hook_execution(hook_name: str, result: dict):
    logger.info(
        "hook_executed",
        hook_name=hook_name,
        success=result.get("success", False),
        execution_time=result.get("execution_time", 0),
        quality_gates_passed=result.get("gates_passed", 0)
    )
```

#### 2. ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```python
from datetime import datetime

class PerformanceTracker:
    def __init__(self):
        self.metrics = {}
    
    def track_execution(self, component: str, duration: float):
        if component not in self.metrics:
            self.metrics[component] = []
        
        self.metrics[component].append({
            "duration": duration,
            "timestamp": datetime.now().isoformat()
        })
    
    def get_average_duration(self, component: str) -> float:
        if component not in self.metrics:
            return 0.0
        
        durations = [m["duration"] for m in self.metrics[component]]
        return sum(durations) / len(durations)
```

### í…ŒìŠ¤íŒ… ì „ëµ

#### 1. Hook ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```python
import pytest
import json
from io import StringIO

def test_persona_router():
    """í˜ë¥´ì†Œë‚˜ ë¼ìš°í„° í…ŒìŠ¤íŠ¸"""
    test_input = {
        "prompt": "implement secure API endpoint",
        "session_id": "test_session"
    }
    
    # stdin ëª¨í‚¹
    import sys
    sys.stdin = StringIO(json.dumps(test_input))
    
    # Hook ì‹¤í–‰ ë° ê²°ê³¼ ê²€ì¦
    # ... í…ŒìŠ¤íŠ¸ ë¡œì§
```

#### 2. í†µí•© í…ŒìŠ¤íŠ¸
```bash
#!/bin/bash
# integration_test.sh

echo "Testing complete SPARK pipeline..."

# 1. í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
echo '{"prompt": "create user authentication system"}' | \
    python3 .claude/hooks/spark_persona_router.py

# 2. ê²°ê³¼ ê²€ì¦
if [ -f .claude/workflows/current_task.json ]; then
    echo "âœ… State file created successfully"
else
    echo "âŒ State file creation failed"
    exit 1
fi

# 3. í’ˆì§ˆ ê²Œì´íŠ¸ í…ŒìŠ¤íŠ¸
echo '{}' | python3 .claude/hooks/spark_quality_gates.py

echo "Integration test completed"
```

---

## ê²°ë¡ 

ì´ ë§¤ë‰´ì–¼ì„ í†µí•´ SPARK ì‹œìŠ¤í…œì˜ ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ì´í•´í•˜ê³  íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. SPARKëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ê°€ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

### ì£¼ìš” ì„±ê³¼ ì§€í‘œ
- **88.4% í† í° ì ˆì•½**: 44K â†’ 5.1K í† í°
- **78.7% ë¹ ë¥¸ ì‹¤í–‰**: ì§€ì—° ë¡œë”©ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ  
- **100% ê¸°ëŠ¥ ìœ ì§€**: SuperClaudeì˜ ëª¨ë“  ê¸°ëŠ¥ ë³´ì¡´
- **10ë‹¨ê³„ í’ˆì§ˆ ë³´ì¦**: SPARK 8ë‹¨ê³„ + Jason DNA 2ë‹¨ê³„

### í•µì‹¬ í˜ì‹ 
1. **Intelligent Persona Router**: ì‘ì—… ë¶„ì„ í›„ ìµœì  ì—ì´ì „íŠ¸ë§Œ í™œì„±í™”
2. **Quality Gate System**: ìë™í™”ëœ í’ˆì§ˆ ê²€ì¦ ë° ê°œì„  ì œì•ˆ
3. **Agent Chain Management**: ì—ì´ì „íŠ¸ ê°„ íš¨ìœ¨ì ì¸ ë°ì´í„° ì „ë‹¬
4. **Hook-Driven Architecture**: ì´ë²¤íŠ¸ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ìë™í™”

SPARKë¥¼ í†µí•´ íš¨ìœ¨ì ì´ê³  ì•ˆì •ì ì¸ AI ê°œë°œ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”. ì¶”ê°€ì ì¸ ì§ˆë¬¸ì´ë‚˜ ì§€ì›ì´ í•„ìš”í•œ ê²½ìš° ì–¸ì œë“ ì§€ ë¬¸ì˜í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

**"44K í† í°ì˜ ì„±ëŠ¥ì„ 5K í† í°ì— ë‹´ë‹¤!"** ğŸš€

*Generated with SPARK Intelligence System*