uhjy`   JHUKYI`           JHU ,â‚©  # CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

SPARK v4.3 (Subagent Performance Architecture with Reduced toKens) is a traits-based multi-agent orchestration system achieving 95.5% token reduction by loading only the required agent on-demand from a pool of 21 specialized agents (6 core + 15 team agents).

**Constitution**: All agents follow SPARK Constitution v1.1 (`.claude/SPARK_CONSTITUTION.md`)

## Core Commands

### Development & Testing
```bash
# Setup environment (recommended: uv for 10x faster installation)
uv venv && source .venv/bin/activate
uv pip install -e ".[full,dev,benchmark]"

# Run quality checks
ruff check .                    # Linting
mypy .                          # Type checking
black . --check                 # Format checking
pytest tests/                   # Run all tests
pytest tests/test_specific.py  # Run single test file

# Benchmark agent performance
python benchmarks/run_benchmarks.py
```

### SPARK-Specific Commands

#### Single Agent Commands
- `/spark-implement <feature>` - Feature implementation with quality gates
- `/spark-test <target>` - Create comprehensive tests (95% coverage target)
- `/spark-analyze <scope>` - Multi-dimensional system analysis
- `/spark-design <system>` - Architecture and API design
- `/spark-clean` - Remove technical debt and dead code
- `/spark-fix <issue>` - Troubleshoot and fix issues
- `/spark-improve <code>` - Performance optimization and modernization
- `Task("qc-spark", "fix ruff violations")` - Quality violations cleanup (use direct Task calls)

#### Pipeline Commands (Sequential Phases)
- `/spark <complex-task>` - Full pipeline: analyze â†’ implement â†’ test â†’ document
- `/spark-refactor <module>` - Refactor pipeline: analyze â†’ improve â†’ test
- `/spark-audit <system>` - Audit: analyze â†’ troubleshoot â†’ document
- `/spark-migrate <legacy>` - Migration: analyze â†’ design â†’ implement â†’ test
- `/spark-optimize <scope>` - Optimize: analyze â†’ improve â†’ test
- `/spark-launch <feature>` - Complete: design â†’ implement â†’ test â†’ document â†’ git

#### Parallel Execution
- `/multi-implement task1,task2,task3,task4,task5` - Execute up to 5 tasks in parallel using team agents

## Architecture & Execution Flow

### Three-Layer System
1. **Router Layer** (`.claude/hooks/spark_persona_router.py`) - Analyzes task and selects optimal agent
2. **Quality Gates** (`.claude/hooks/spark_quality_gates.py`) - Verifies agent claims vs actual results
3. **Agent Layer** (`.claude/agents/`) - 21 specialized agents (6 core + 15 team)

**Core Agents (6)**:
- `analyzer-spark` - Multi-dimensional system analysis (v1.1: 500 lines, evidence-based)
- `implementer-spark` - Feature implementation with 95% test coverage
- `tester-spark` - Comprehensive testing (95% unit, 85% integration)
- `designer-spark` - System architecture and API design
- `documenter-spark` - API docs, user guides, architecture documents
- `qc-spark` - Quality violations cleanup with 5-phase inspection

**Team Agents (15)**: 5 teams Ã— 3 roles (implementer, tester, documenter) for parallel execution

### Critical Execution Protocol for Claude Code

When receiving SPARK commands, you MUST follow this exact pattern:

```python
# For single agent:
1. IMMEDIATELY CALL:
   Task("agent-name-spark", user_request)
2. WAIT for completion
3. CHECK ~/.claude/workflows/current_task.json:
   - quality.violations_total == 0
   - quality.can_proceed == true
   - state.status == "completed"
4. DECISION:
   âœ… ALL MET â†’ Report success
   âŒ ANY FAILED â†’ Retry with feedback (max 3 attempts)

# For parallel execution (MUST be in ONE message):
Task("team1-implementer-spark", task1)
Task("team2-implementer-spark", task2)
Task("team3-implementer-spark", task3)
Task("team4-implementer-spark", task4)
Task("team5-implementer-spark", task5)
# WAIT for ALL to complete
```

### Phase Structure (All Agents)

**Constitution v1.1**: Phase count is flexible, workflow is adaptive and iterative.

Typical structure:
- **Phase 0**: Task Understanding (read 2å·'s specific instructions)
- **Phase 1-N**: Domain work (agent-specific, iterative)
- **Phase N+1**: Quality verification (MANDATORY)
  - Phase 5A: Quality metrics recording
  - Phase 5B: Quality gates execution
  - Must check for "Quality gates PASSED" or "Quality gates FAILED"
  - All violations must be 0 to proceed
  - Maximum 3 retry attempts

**Key Principles**:
- Agents use professional judgment, not mechanical checklists
- Iteration between phases is expected (e.g., Phase 2 â†” Phase 3)
- 2å· provides task-specific instructions (scope, depth, priorities)
- Agents provide common protocols that adapt to any task

### Quality Gates Verification

```bash
# Agent self-validation
echo '{"subagent": "implementer-spark", "self_check": true}' | \
python3 ~/.claude/hooks/spark_quality_gates.py

# Returns English messages:
# âœ… "Quality gates PASSED"
# ğŸš« "Quality gates FAILED"
```

## Token Management

### Safety Limits
- **Hard limit**: 200K tokens per agent context
- **Practical limit**: 90K tokens (safety margin)
- **Write operations**: Double token consumption
- **Agent sizes**: ~1K (team agents) to ~3.9K (implementer-spark)

### Critical Rules
- Agents CANNOT call other agents (only Claude Code uses Task tool)
- Write operations consume 2x tokens (memory + output)
- All agents include 90K token safety protocol
- Compression available for 30-50% reduction

## JSON State Management

Agents communicate via JSON state files:
```
~/.claude/workflows/current_task.json         # Main task state
~/.claude/workflows/team[1-5]_current_task.json  # Team-specific states
```

Structure:
```json
{
  "id": "spark_YYYYMMDD_HHMMSS",
  "version": "4.3",
  "state": {"status": "pending|running|completed|failed"},
  "quality": {
    "violations_total": 0,
    "can_proceed": true
  }
}
```

## Common Pitfalls to Avoid

- Loading multiple agents unnecessarily (use one at a time)
- Sequential Task calls for parallel execution (must be simultaneous)
- Skipping quality gate validation
- Not checking JSON state after agent completion
- Forgetting Write operations double token consumption

## Agent Specialization & Role Separation

### Primary Agents (17 total)
- **improver-spark**: Performance optimization, architecture modernization, context7 research
- **qc-spark**: Quality violations cleanup (ruff, mypy, pytest failures) with 5-phase inspection
- **analyzer-spark**: Multi-dimensional system analysis with multi-session support
- **implementer-spark**: Feature implementation with 95% test coverage requirement
- **tester-spark**: Comprehensive testing (95% unit, 85% integration) 
- **designer-spark**: System architecture and API design
- **documenter-spark**: API docs, user guides, architecture documentation
- **troubleshooter-spark**: Systematic debugging and issue resolution
- **cleaner-spark**: Dead code removal and dependency updates
- **explainer-spark**: Concept and pattern explanation
- **builder-spark**: Build process and CI/CD optimization
- **estimater-spark**: Evidence-based time and resource estimation
- **gitter-spark**: Git strategy, branching, automation
- **loader-spark**: Project context analysis and loading
- **indexer-spark**: SuperClaude command navigation
- **tasker-spark**: Multi-session project management
- **spawner-spark**: Complex multi-agent coordination

### Team Agents (15 total - 5 teams Ã— 3 roles)
- **team[1-5]-implementer-spark**: Parallel implementation specialists
- **team[1-5]-tester-spark**: Parallel testing specialists  
- **team[1-5]-documenter-spark**: Parallel documentation specialists

### Critical Role Separation
- **Quality Control**: Use `qc-spark` for fixing violations (ruff, mypy, test failures)
- **Enhancement**: Use `improver-spark` for modernization and optimization
- **Multi-Session**: `analyzer-spark`, `improver-spark`, and `qc-spark` support progressive work

## Key File Locations

- **Agent definitions**: `.claude/agents/*-spark.md`
- **Command definitions**: `.claude/commands/spark-*.md`
- **Quality gates script**: `.claude/hooks/spark_quality_gates.py`
- **Router script**: `.claude/hooks/spark_persona_router.py`
- **JSON states**: `.claude/workflows/*.json`
- **Documentation**: `docs/SPARK_COMPLETE_GUIDE.md`

## âš ï¸ IMPORTANT: Missing Documentation Files (2025-11-10)

**Status**: The following guide documents are on Jason's home computer and NOT yet committed to the repository.

**Missing Files** (created during home session, need to be retrieved):
1. **ì²´í¬ë¦¬ìŠ¤íŠ¸ ì‘ì„± ê°€ì´ë“œ** (Checklist Creation Guide)
2. **ì‘ì—…ë¶„í•´ ê°€ì´ë“œ** (Task Breakdown Guide)
3. **ì²­ì‚¬ì§„ ê°€ì´ë“œ** (Blueprint Guide)

**Committed Files** (currently in repository):
- `docs/CORE_METHODOLOGY.md` - AI collaboration methodology v4.0 âœ…
- `docs/ADR_GUIDE.md` - Architecture Decision Records guide âœ…
- `docs/PROJECT_STANDARDS_GUIDE.md` - Project standards guide âœ…

**Work Sequence**:
1. Created CORE_METHODOLOGY.md first
2. Created checklist guide (bottom-up approach)
3. Created task breakdown guide
4. Created blueprint guide
5. Created ADR_GUIDE.md and PROJECT_STANDARDS_GUIDE.md (only these were committed)

**Action Required**:
- DO NOT overwrite files in `docs/` from office computer
- Retrieve missing guides from home computer first
- All old documentation has been moved to `docs/docs-backup/`
- Only new guides (6 total) should remain in `docs/` root

**Current docs/ Structure**:
```
docs/
â”œâ”€â”€ ADR_GUIDE.md
â”œâ”€â”€ CORE_METHODOLOGY.md
â”œâ”€â”€ PROJECT_STANDARDS_GUIDE.md
â”œâ”€â”€ Stage1-2_ê°€ì´ë“œ_ì‘ì„±_í”„ë¡œì íŠ¸.md âœ… (NEW: 2025-11-10)
â”œâ”€â”€ ENTERPRISE_INITIATION_PROCESS.md
â”œâ”€â”€ [MISSING] ì²´í¬ë¦¬ìŠ¤íŠ¸_ì‘ì„±_ê°€ì´ë“œ.md
â”œâ”€â”€ [MISSING] ì‘ì—…ë¶„í•´_ê°€ì´ë“œ.md
â”œâ”€â”€ [MISSING] ì²­ì‚¬ì§„_ê°€ì´ë“œ.md
â”œâ”€â”€ docs-backup/  (all previous documentation)
â””â”€â”€ references/  (backup and reference materials)
```

## ğŸ“‹ Current Project Status (2025-11-12)

### âœ… Completed: Stage 1-3 Validation (2025-11-12)

**Phase 2**: ì£¼ì‹ ê±°ë˜ í”Œë«í¼ ì‚¬ë¡€ ì—°êµ¬ ì™„ë£Œ

**ì™„ë£Œëœ ì‘ì—…**:
- âœ… **Stage 1**: ì•„í‚¤í…ì²˜ íŒ¨ë°€ë¦¬ ê²°ì • (A-C-A íŒ¨í„´ ë°œê²¬!)
  - Part 0: í•µì‹¬ ê¸°ëŠ¥ íŒŒì•… (ê±°ë˜ = 1ê°œ ê¸°ëŠ¥, ìˆ˜ë™/ìë™ = êµ¬í˜„ ë°©ì‹)
  - Part 1-2: Layer 1-2 ë¶„ì„ (A-C-A + NFR A-B-B-A)

- âœ… **Stage 2**: í™˜ê²½ ì œì•½ ë° êµ¬í˜„ ê²°ì •
  - Part 1: Layer 3 ì™¸ë¶€ ì œì•½ (ì¦ê¶Œì‚¬ API 6ê°œ ë¹„êµ, ê¸ˆìœµ ê·œì œ ì¡°ì‚¬)
  - Part 2: ì¶©ëŒ íŒ¨í„´ ë°œê²¬ (3ê°œ ì¶©ëŒ ì‹ë³„)
  - Part 3: 5ë‹¨ê³„ êµ¬í˜„ë°©ë²• (ê¸°ëŠ¥â†’ì†ì„±â†’ì œì•½â†’ê¸°ìˆ â†’ì„¤ê³„)

- âœ… **Stage 3**: ADR ì‘ì„±
  - ì „ì²´ ëª©ë¡: 18ê°œ (ì™¸ë¶€ ì œì•½, ì¶©ëŒ í•´ê²°, ê¸°ìˆ  ìŠ¤íƒ, ë°ì´í„°/API ì„¤ê³„, í’ˆì§ˆ/ë³´ì•ˆ)
  - ì˜ˆì‹œ ì‘ì„±: 3ê°œ (í•œêµ­íˆ¬ìì¦ê¶Œ ì„ íƒ, í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜, FastAPI ì„ íƒ)

**í•µì‹¬ ë°œê²¬**:
1. **í•µì‹¬ ê¸°ëŠ¥ íŒë‹¨ ê¸°ì¤€**: êµ¬í˜„ ë°©ì‹ì´ ì•„ë‹Œ ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ìœ¼ë¡œ êµ¬ë¶„
2. **A-C-A íŒ¨ë°€ë¦¬**: ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ íŒ¨ë°€ë¦¬ íŒ¨í„´ ë°œê²¬ (ì‹¤ì‹œê°„ íŠ¸ëœì­ì…˜)
3. **ADR ì›ì¹™**: "ì œì•½ì— ì˜í•œ ê²°ì •ë„ ADRì´ë‹¤" (ì„ íƒì§€ê°€ 1ê°œì—¬ë„ ê¸°ë¡)
4. **ì†ì„± ì§ˆë¬¸**: NFR í”„ë¡œíŒŒì¼ â†’ êµ¬ì²´ì  ìˆ˜ì¹˜ â†’ ê¸°ìˆ  ì„ íƒ (SEI ADD ì›ì¹™)

**ìƒì„±ëœ ë¬¸ì„œ**:
- `docs/session-summaries/20251112_Phase2_ë³µí•©ì‹œìŠ¤í…œ_ë„ì „.md` (1,770ì¤„)
- `docs/session-summaries/20251112_í•µì‹¬ê¸°ëŠ¥_íŒë‹¨ê¸°ì¤€.md` (240ì¤„)
- `docs/session-summaries/20251112_Layer3_ì™¸ë¶€ì œì•½ì¡°ì‚¬.md` (680ì¤„)

**ë‹¤ìŒ ì‘ì—… (Next Session)**:
- ê°€ì´ë“œ ì¬êµ¬ì„±: ì§€ì¹¨/í…œí”Œë¦¿ vs í•´ì„¤ì„œ vs ì‚¬ë¡€ì§‘ ë¶„ë¦¬
  - `02_ARCHITECTURE_DECISION_GUIDE.md`: ê°„ê²°í•œ ì²´í¬ë¦¬ìŠ¤íŠ¸ + í…œí”Œë¦¿
  - `02-1_ARCHITECTURE_DECISION_MANUAL.md`: ìƒì„¸ í•´ì„¤
  - `02-2_IMPLEMENTATION_CASES.md`: Case 4 ì¶”ê°€ (A-C-A)

---

### Previous Work: Stage 1-2 Guide Development (2025-11-10)

**Goal**: Create methodology guide for "Idea â†’ Blueprint" stage (upward consolidation process)

**Background**:
- âœ… Downward process validated (Blueprint â†’ Breakdown â†’ Checklist â†’ Implementation)
- âŒ Upward process undefined (Idea â†’ ??? â†’ Blueprint)
- ğŸ¯ Solution approach: Hierarchical Templates (Level 1/2/3)

**Key Discovery**: "Context" is the Core Challenge
- 7 project analysis completed (6 failures + 1 success)
- AI collaboration limitations identified:
  - Context loss when projects scale
  - Hallucination ("possible" â†’ "impossible")
  - Stubbornness (ignoring user requirements)
  - Information overload (long documents â†’ arbitrary actions)
  - Repeated mistakes (if-fi, } omissions)

**Method**: Real reconstruction of memory project
- Selected project: memory system (actual need + repeated failures + appropriate scale)
- Approach: Reconstruct from scratch with 2í˜¸, document needed template items
- Expected output: Stage 1-2 guide with hierarchical templates

**Related Documents**:
- `docs/Stage1-2_ê°€ì´ë“œ_ì‘ì„±_í”„ë¡œì íŠ¸.md` - Project definition and plan
- `ì‹¤íŒ¨ í”„ë¡œì íŠ¸ ë¶„ì„ ë³´ê³ ì„œ/00_ì¢…í•©ë¶„ì„_7ê°œí”„ë¡œì íŠ¸_íŒ¨í„´ë¶„ì„_20251110.md` - 7-project analysis
- `ì‹¤íŒ¨ í”„ë¡œì íŠ¸ ë¶„ì„ ë³´ê³ ì„œ/Jasonì˜ ë³€ëª….md` - Real experiences from failures

---

## ğŸ“‹ DNA Methodology File Naming Convention (2025-11-12)

### **Purpose**
Instant identification of Stage and document role from filename alone.

### **Pattern**: `{Stage}{Type}-{Seq}_{descriptive_name}.md`

```
01F-01_core_functions.md
â”‚â”‚â”‚ â”‚â”‚ â””â”€ Descriptive name (snake_case)
â”‚â”‚â”‚ â”‚â””â”€ Sequence (01-99)
â”‚â”‚â”‚ â””â”€ Separator (hyphen)
â”‚â”‚â””â”€ Document Type (single letter)
â”‚â””â”€ Stage (01-09)
â””â”€ 2-digit number
```

### **Type Codes**

**Project Outputs** (files created per project):
- **F** = Function (ê¸°ëŠ¥ ì •ì˜)
- **C** = Classification (ë¶„ë¥˜/ë¶„ì„)
- **D** = Decision (ê²°ì • ì‚¬í•­)
- **S** = Schema (ìŠ¤í‚¤ë§ˆ/ì„¤ê³„)
- **A** = ADR (Architecture Decision Record)
- **B** = Blueprint (ì²­ì‚¬ì§„)
- **T** = Task (ì‘ì—… ë¶„í•´)
- **L** = List/Checklist (ì²´í¬ë¦¬ìŠ¤íŠ¸)

**Methodology Docs** (DNA methodology guides):
- **G** = Guide (ê°„ê²°í•œ ê°€ì´ë“œ)
- **M** = Manual (ìƒì„¸ í•´ì„¤ì„œ)
- **E** = Example/Case (ì‚¬ë¡€ì§‘)

**Special**:
- **00** = Meta documents (methodology itself)

### **Examples**

```bash
# Stage 1: Family Classification
01F-01_core_functions.md          # Function definition
01C-01_family_classification.md   # Family: A-C-A
01D-01_tech_candidates.md         # Tech candidates

# Stage 2: Structure Design
02C-01_layer3_constraints.md      # Layer 3 constraints
02C-02_conflicts_analysis.md      # Conflict patterns
02D-01_tech_stack_decision.md     # Tech stack decision

# Stage 3: ADR
03A-001_logging.md                # Bootstrap ADR (001-099)
03A-101_kis_api_selection.md      # Domain ADR (100-999)

# Guides
01G-00_core_definition_guide.md   # Stage 1 guide
02M-01_layer3_manual.md           # Stage 2 manual
02E-01_stock_trading_case.md      # Stage 2 example
```

### **Quick Reference**
```
01F-01 = Stage 1, Function doc, #1
02C-02 = Stage 2, Classification doc, #2
03A-101 = Stage 3, ADR, Domain #101
01M-01 = Stage 1, Manual, #1
```

### **Benefits**
- âœ… Instant Stage/role identification
- âœ… Auto-sorting by Stage â†’ Type â†’ Seq
- âœ… AI-friendly (clear rules)
- âœ… Searchable (`find . -name "03A-*"` = all ADRs)

**Full specification**: `docs/completed-guide/00_FILE_NAMING_CONVENTION.md`